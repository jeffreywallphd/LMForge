{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üï∑Ô∏è LMForge MCP Agent System - Standalone Colab\n",
        "\n",
        "**Crawler + Parser Agents - Complete Standalone Implementation**\n",
        "\n",
        "This notebook contains everything needed - no external files required!\n",
        "\n",
        "‚úÖ **Features:**\n",
        "- **Crawler Agent** - Fetches HTML from URLs\n",
        "- **Parser Agent** - Extracts clean text\n",
        "- **Fallback Support** - Works even if MCP agents fail\n",
        "- **Complete Standalone** - All code embedded\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Step 1: Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q fastapi uvicorn mcp-use langchain-openai python-dotenv nest-asyncio requests beautifulsoup4 pydantic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Step 2: Enable Async Support\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "print(\"‚úÖ Async support enabled\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Step 3: Create All Agent Files (Embedded in Notebook)\n",
        "\n",
        "All agent code is embedded here - no external files needed!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Create agents directory\n",
        "os.makedirs('agents', exist_ok=True)\n",
        "\n",
        "# Crawler Agent\n",
        "crawler_code = '''#!/usr/bin/env python3\n",
        "import asyncio\n",
        "import json\n",
        "from typing import Any\n",
        "import requests\n",
        "from mcp.server import Server\n",
        "from mcp.server.stdio import stdio_server\n",
        "from mcp.types import Tool, TextContent\n",
        "\n",
        "app = Server(\"crawler-server\")\n",
        "\n",
        "@app.list_tools()\n",
        "async def list_tools() -> list[Tool]:\n",
        "    return [\n",
        "        Tool(\n",
        "            name=\"fetch_url\",\n",
        "            description=\"Fetches the raw HTML content from a given URL\",\n",
        "            inputSchema={\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\"url\": {\"type\": \"string\", \"description\": \"The URL to fetch\"}},\n",
        "                \"required\": [\"url\"],\n",
        "            },\n",
        "        )\n",
        "    ]\n",
        "\n",
        "@app.call_tool()\n",
        "async def call_tool(name: str, arguments: Any) -> list[TextContent]:\n",
        "    if name != \"fetch_url\":\n",
        "        raise ValueError(f\"Unknown tool: {name}\")\n",
        "    \n",
        "    url = arguments.get(\"url\")\n",
        "    if not url:\n",
        "        raise ValueError(\"URL is required\")\n",
        "\n",
        "    try:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        result = {\"url\": url, \"status_code\": response.status_code, \"html\": response.text}\n",
        "        return [TextContent(type=\"text\", text=json.dumps(result))]\n",
        "    except Exception as e:\n",
        "        return [TextContent(type=\"text\", text=json.dumps({\"url\": url, \"error\": str(e), \"html\": \"\"}))]\n",
        "\n",
        "async def main():\n",
        "    async with stdio_server() as (read_stream, write_stream):\n",
        "        await app.run(read_stream, write_stream, app.create_initialization_options())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n",
        "'''\n",
        "\n",
        "# Parser Agent\n",
        "parser_code = '''#!/usr/bin/env python3\n",
        "import asyncio\n",
        "import json\n",
        "from typing import Any\n",
        "from bs4 import BeautifulSoup\n",
        "from mcp.server import Server\n",
        "from mcp.server.stdio import stdio_server\n",
        "from mcp.types import Tool, TextContent\n",
        "\n",
        "app = Server(\"parser-server\")\n",
        "\n",
        "@app.list_tools()\n",
        "async def list_tools() -> list[Tool]:\n",
        "    return [\n",
        "        Tool(\n",
        "            name=\"parse_html\",\n",
        "            description=\"Extracts text content from HTML\",\n",
        "            inputSchema={\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\"html\": {\"type\": \"string\", \"description\": \"The HTML content to parse\"}},\n",
        "                \"required\": [\"html\"],\n",
        "            },\n",
        "        )\n",
        "    ]\n",
        "\n",
        "@app.call_tool()\n",
        "async def call_tool(name: str, arguments: Any) -> list[TextContent]:\n",
        "    if name != \"parse_html\":\n",
        "        raise ValueError(f\"Unknown tool: {name}\")\n",
        "\n",
        "    html = arguments.get(\"html\")\n",
        "    if html is None:\n",
        "        raise ValueError(\"HTML content is required\")\n",
        "\n",
        "    try:\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        for script_or_style in soup(['script', 'style', 'meta', 'noscript']):\n",
        "            script_or_style.decompose()\n",
        "        \n",
        "        text = soup.get_text()\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        text = '\\\\n'.join(chunk for chunk in chunks if chunk)\n",
        "        \n",
        "        result = {\"text\": text, \"success\": True}\n",
        "        return [TextContent(type=\"text\", text=json.dumps(result))]\n",
        "    except Exception as e:\n",
        "        return [TextContent(type=\"text\", text=json.dumps({\"text\": \"\", \"error\": str(e), \"success\": False}))]\n",
        "\n",
        "async def main():\n",
        "    async with stdio_server() as (read_stream, write_stream):\n",
        "        await app.run(read_stream, write_stream, app.create_initialization_options())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n",
        "'''\n",
        "\n",
        "# Write agent files\n",
        "with open('agents/crawler_server.py', 'w') as f:\n",
        "    f.write(crawler_code)\n",
        "\n",
        "with open('agents/parser_server.py', 'w') as f:\n",
        "    f.write(parser_code)\n",
        "\n",
        "print(\"‚úÖ Agent files created:\")\n",
        "print(\"   - agents/crawler_server.py\")\n",
        "print(\"   - agents/parser_server.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Step 4: Create Orchestrator (main.py)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "main_code = '''import os\n",
        "import json\n",
        "import asyncio\n",
        "import sys\n",
        "from typing import Dict, Any\n",
        "from mcp_use import MCPClient\n",
        "\n",
        "class LMForgeOrchestrator:\n",
        "    def __init__(self):\n",
        "        self.clients: Dict[str, MCPClient] = {}\n",
        "        self.agents: Dict[str, Any] = {}\n",
        "        \n",
        "    async def initialize(self):\n",
        "        mcp_servers = {\n",
        "            \"crawler\": {\n",
        "                \"command\": sys.executable,\n",
        "                \"args\": [\"agents/crawler_server.py\"],\n",
        "                \"env\": os.environ.copy()\n",
        "            },\n",
        "            \"parser\": {\n",
        "                \"command\": sys.executable,\n",
        "                \"args\": [\"agents/parser_server.py\"],\n",
        "                \"env\": os.environ.copy()\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        for name, config in mcp_servers.items():\n",
        "            try:\n",
        "                client = MCPClient()\n",
        "                client.add_server(name, config)\n",
        "                await client.create_all_sessions()\n",
        "                session = client.get_session(name)\n",
        "                if not session:\n",
        "                    raise RuntimeError(f\"Failed to create session for {name}\")\n",
        "                self.clients[name] = client\n",
        "                self.agents[name] = session\n",
        "                print(f\"‚úì Initialized {name} agent\")\n",
        "            except Exception as e:\n",
        "                error_msg = str(e)\n",
        "                if \"fileno\" in error_msg.lower():\n",
        "                    print(f\"‚úó Failed to initialize {name} agent: stdio issue\")\n",
        "                    print(f\"  Using fallback implementation (direct function calls)\")\n",
        "                else:\n",
        "                    print(f\"‚úó Failed to initialize {name} agent: {str(e)}\")\n",
        "                # Continue with fallback\n",
        "    \n",
        "    async def crawl_url(self, url: str) -> Dict[str, Any]:\n",
        "        # Try MCP agent first\n",
        "        if \"crawler\" in self.agents:\n",
        "            try:\n",
        "                session = self.agents[\"crawler\"]\n",
        "                result = await session.call_tool(\"fetch_url\", {\"url\": url})\n",
        "                \n",
        "                if result and hasattr(result, 'content') and result.content:\n",
        "                    content_item = result.content[0]\n",
        "                    result_text = content_item.text if hasattr(content_item, 'text') else str(content_item)\n",
        "                    return json.loads(result_text)\n",
        "                raise RuntimeError(\"No result from crawler\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö† MCP crawler failed, using fallback: {e}\")\n",
        "        \n",
        "        # Fallback: Direct implementation\n",
        "        import requests\n",
        "        try:\n",
        "            headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            return {\n",
        "                \"url\": url,\n",
        "                \"status_code\": response.status_code,\n",
        "                \"html\": response.text,\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"url\": url,\n",
        "                \"error\": str(e),\n",
        "                \"html\": \"\",\n",
        "            }\n",
        "    \n",
        "    async def parse_html(self, html: str) -> Dict[str, Any]:\n",
        "        # Try MCP agent first\n",
        "        if \"parser\" in self.agents:\n",
        "            try:\n",
        "                session = self.agents[\"parser\"]\n",
        "                result = await session.call_tool(\"parse_html\", {\"html\": html})\n",
        "                \n",
        "                if result and hasattr(result, 'content') and result.content:\n",
        "                    content_item = result.content[0]\n",
        "                    result_text = content_item.text if hasattr(content_item, 'text') else str(content_item)\n",
        "                    return json.loads(result_text)\n",
        "                raise RuntimeError(\"No result from parser\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö† MCP parser failed, using fallback: {e}\")\n",
        "        \n",
        "        # Fallback: Direct implementation\n",
        "        from bs4 import BeautifulSoup\n",
        "        try:\n",
        "            soup = BeautifulSoup(html, 'html.parser')\n",
        "            for script_or_style in soup(['script', 'style', 'meta', 'noscript']):\n",
        "                script_or_style.decompose()\n",
        "            \n",
        "            text = soup.get_text()\n",
        "            lines = (line.strip() for line in text.splitlines())\n",
        "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "            text = '\\\\n'.join(chunk for chunk in chunks if chunk)\n",
        "            \n",
        "            return {\"text\": text, \"success\": True}\n",
        "        except Exception as e:\n",
        "            return {\"text\": \"\", \"error\": str(e), \"success\": False}\n",
        "    \n",
        "    def clean_text_simple(self, text: str) -> str:\n",
        "        import re\n",
        "        text = re.sub(r'\\\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "    \n",
        "    async def run_full_flow(self, url: str) -> Dict[str, Any]:\n",
        "        crawl_result = await self.crawl_url(url)\n",
        "        if \"error\" in crawl_result:\n",
        "            return {\"url\": url, \"error\": crawl_result[\"error\"], \"stage\": \"crawl\"}\n",
        "        \n",
        "        html_content = crawl_result.get(\"html\", \"\")\n",
        "        if not html_content:\n",
        "            return {\"url\": url, \"error\": \"No HTML content received\", \"stage\": \"crawl\"}\n",
        "        \n",
        "        parse_result = await self.parse_html(html_content)\n",
        "        if not parse_result.get(\"success\", False):\n",
        "            return {\"url\": url, \"error\": parse_result.get(\"error\", \"Parse failed\"), \"stage\": \"parse\"}\n",
        "        \n",
        "        parsed_text = parse_result.get(\"text\", \"\")\n",
        "        cleaned_text = self.clean_text_simple(parsed_text)\n",
        "        \n",
        "        return {\n",
        "            \"url\": url,\n",
        "            \"raw_html\": html_content[:500] + (\"...\" if len(html_content) > 500 else \"\"),\n",
        "            \"parsed_text\": parsed_text[:500] + (\"...\" if len(parsed_text) > 500 else \"\"),\n",
        "            \"cleaned_text\": cleaned_text,\n",
        "            \"success\": True\n",
        "        }\n",
        "    \n",
        "    async def close(self):\n",
        "        for name, client in self.clients.items():\n",
        "            try:\n",
        "                await client.close_all_sessions()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "orchestrator = LMForgeOrchestrator()\n",
        "\n",
        "async def get_orchestrator():\n",
        "    if not orchestrator.clients:\n",
        "        await orchestrator.initialize()\n",
        "    return orchestrator\n",
        "'''\n",
        "\n",
        "with open('main.py', 'w') as f:\n",
        "    f.write(main_code)\n",
        "\n",
        "print(\"‚úÖ main.py created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "app_code = '''import os\n",
        "import sys\n",
        "from typing import Dict, Any\n",
        "from contextlib import asynccontextmanager\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel, Field\n",
        "from main import get_orchestrator\n",
        "\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    print(\"üöÄ Starting LMForge MCP-Use Backend...\")\n",
        "    try:\n",
        "        orchestrator = await get_orchestrator()\n",
        "        print(\"‚úì All agents initialized successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Failed to initialize agents: {str(e)}\")\n",
        "        sys.exit(1)\n",
        "    yield\n",
        "    print(\"üõë Shutting down...\")\n",
        "    try:\n",
        "        orchestrator = await get_orchestrator()\n",
        "        await orchestrator.close()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "app = FastAPI(title=\"LMForge MCP-Use Backend\", version=\"1.0.0\")\n",
        "app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_credentials=True, allow_methods=[\"*\"], allow_headers=[\"*\"])\n",
        "\n",
        "class RunFlowRequest(BaseModel):\n",
        "    url: str = Field(..., description=\"The URL to process\")\n",
        "\n",
        "class RunFlowResponse(BaseModel):\n",
        "    url: str\n",
        "    raw_html: str\n",
        "    parsed_text: str\n",
        "    cleaned_text: str\n",
        "    success: bool = True\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"name\": \"LMForge Backend\", \"status\": \"running\"}\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    return {\"status\": \"healthy\", \"service\": \"LMForge MCP-Use Backend\"}\n",
        "\n",
        "@app.post(\"/run-flow\", response_model=RunFlowResponse)\n",
        "async def run_flow(request: RunFlowRequest) -> Dict[str, Any]:\n",
        "    try:\n",
        "        orchestrator = await get_orchestrator()\n",
        "        result = await orchestrator.run_full_flow(request.url)\n",
        "        \n",
        "        if \"error\" in result:\n",
        "            raise HTTPException(status_code=400, detail={\"error\": result[\"error\"], \"stage\": result.get(\"stage\", \"unknown\")})\n",
        "        \n",
        "        return result\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Internal server error: {str(e)}\")\n",
        "'''\n",
        "\n",
        "with open('app.py', 'w') as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "print(\"‚úÖ app.py created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üåê Step 6: Start the Server\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import threading\n",
        "import time\n",
        "import asyncio\n",
        "from pathlib import Path\n",
        "from uvicorn import Config, Server\n",
        "\n",
        "# Verify app.py exists (should be created in previous cells)\n",
        "if not Path('app.py').exists():\n",
        "    print(\"‚ö†Ô∏è app.py not found!\")\n",
        "    print(\"Please run the previous cells first:\")\n",
        "    print(\"  - Cell 6: Create agent files\")\n",
        "    print(\"  - Cell 8: Create main.py\")\n",
        "    print(\"  - Cell 10: Create app.py\")\n",
        "    print(\"\\nThen run this cell again.\")\n",
        "else:\n",
        "    print(\"‚úÖ app.py found - starting server...\")\n",
        "    \n",
        "    def run_server():\n",
        "        config = Config(\"app:app\", host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "        server = Server(config)\n",
        "        loop = asyncio.new_event_loop()\n",
        "        asyncio.set_event_loop(loop)\n",
        "        loop.run_until_complete(server.serve())\n",
        "    \n",
        "    server_thread = threading.Thread(target=run_server, daemon=True)\n",
        "    server_thread.start()\n",
        "    \n",
        "    print(\"üöÄ Starting server...\")\n",
        "    time.sleep(8)\n",
        "    print(\"‚úÖ Server started at http://localhost:8000\")\n",
        "    print(\"üìö API docs: http://localhost:8000/docs\")\n",
        "    print(\"\\nüí° To test: Run the next cell and enter a URL!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Step 7: Test the Agents - Extract Text from URL\n",
        "\n",
        "Enter any URL below and see the agents extract text!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Test URL\n",
        "print(\"=\"*70)\n",
        "print(\"üß™ Test the Backend - Extract Text from URL\")\n",
        "print(\"=\"*70)\n",
        "print()\n",
        "\n",
        "test_url = input(\"üìù Enter URL to extract text from (or press Enter for default): \").strip()\n",
        "\n",
        "if not test_url:\n",
        "    test_url = \"https://example.com\"  # Default\n",
        "    print(f\"   Using default URL: {test_url}\")\n",
        "else:\n",
        "    print(f\"   Testing: {test_url}\")\n",
        "\n",
        "print(f\"\\nüåê Processing: {test_url}\")\n",
        "print(\"‚è≥ Please wait (this may take a few seconds)...\\n\")\n",
        "\n",
        "# Wait a bit for server to be ready\n",
        "time.sleep(2)\n",
        "\n",
        "try:\n",
        "    response = requests.post(\n",
        "        \"http://localhost:8000/run-flow\",\n",
        "        json={\"url\": test_url},\n",
        "        timeout=60\n",
        "    )\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        \n",
        "        print(\"=\"*70)\n",
        "        print(\"‚úÖ SUCCESS - Content Extracted!\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"\\nüìã URL: {result['url']}\")\n",
        "        print(f\"\\nüìä Statistics:\")\n",
        "        print(f\"   ‚Ä¢ Raw HTML preview: {len(result.get('raw_html', ''))} characters\")\n",
        "        print(f\"   ‚Ä¢ Parsed Text preview: {len(result.get('parsed_text', ''))} characters\")\n",
        "        print(f\"   ‚Ä¢ Cleaned Text: {len(result.get('cleaned_text', ''))} characters\")\n",
        "        \n",
        "        print(f\"\\nüìÑ Extracted Text (First 500 chars):\")\n",
        "        print(\"-\"*70)\n",
        "        cleaned = result.get('cleaned_text', '')\n",
        "        if cleaned:\n",
        "            print(cleaned[:500])\n",
        "            if len(cleaned) > 500:\n",
        "                print(f\"\\n... (truncated - showing first 500 of {len(cleaned)} chars)\")\n",
        "        else:\n",
        "            print(\"(No text extracted)\")\n",
        "        print(\"-\"*70)\n",
        "        \n",
        "        print(f\"\\n‚ú® Full cleaned text ({len(cleaned)} chars) is available!\")\n",
        "        print(f\"\\nüí° Tip: You can access the full result in the 'result' variable\")\n",
        "        \n",
        "        # Store result for next cell\n",
        "        globals()['result'] = result\n",
        "        \n",
        "    else:\n",
        "        print(f\"‚ùå Error: HTTP {response.status_code}\")\n",
        "        try:\n",
        "            error_detail = response.json()\n",
        "            print(f\"\\nüìã Error Details:\")\n",
        "            print(json.dumps(error_detail, indent=2))\n",
        "        except:\n",
        "            print(f\"\\nüìã Response: {response.text}\")\n",
        "        \n",
        "except requests.exceptions.ConnectionError:\n",
        "    print(\"‚ùå Cannot connect to backend!\")\n",
        "    print(\"   Make sure the server is running.\")\n",
        "    print(\"   Check the previous cell - did you see 'Server started'?\")\n",
        "    print(\"\\n   Try running the server cell again if needed.\")\n",
        "    \n",
        "except requests.exceptions.Timeout:\n",
        "    print(\"‚ùå Request timed out!\")\n",
        "    print(\"   The server may be processing. Try again or check if the URL is accessible.\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "    print(\"\\nüí° Troubleshooting:\")\n",
        "    print(\"   1. Check if server started (previous cell)\")\n",
        "    print(\"   2. Wait a few seconds after starting server\")\n",
        "    print(\"   3. Try a simpler URL like https://example.com\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 8: View Full Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display full result as JSON\n",
        "try:\n",
        "    # Check if result exists\n",
        "    if 'result' in globals() or 'result' in locals():\n",
        "        result_val = globals().get('result') or locals().get('result')\n",
        "        \n",
        "        if result_val and result_val.get('success'):\n",
        "            print(\"\\nüìã Complete Result (JSON):\")\n",
        "            print(\"=\"*70)\n",
        "            print(json.dumps(result_val, indent=2, ensure_ascii=False))\n",
        "            print(\"=\"*70)\n",
        "            print(\"\\nüí° Access individual fields:\")\n",
        "            print(f\"   - result['cleaned_text'] - Full extracted text\")\n",
        "            print(f\"   - result['parsed_text'] - Parsed text preview\")\n",
        "            print(f\"   - result['raw_html'] - Raw HTML preview\")\n",
        "        else:\n",
        "            print(\"‚ö† Result exists but wasn't successful\")\n",
        "            print(\"   Run the previous test cell again\")\n",
        "    else:\n",
        "        print(\"üìù Run the previous cell first to extract content from a URL\")\n",
        "        print(\"   Then run this cell to see the full JSON result\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö† Error displaying results: {e}\")\n",
        "    print(\"   Make sure you ran the test cell first\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üåê Step 9: Access API Documentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import IFrame\n",
        "\n",
        "print(\"üìö FastAPI Interactive Documentation\")\n",
        "print(\"Open in new tab: http://localhost:8000/docs\")\n",
        "print(\"\\nOr view below:\")\n",
        "\n",
        "IFrame(src=\"http://localhost:8000/docs\", width=900, height=600)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ What's Working\n",
        "\n",
        "- ‚úÖ **Crawler Agent** - Fetches HTML from URLs (with fallback)\n",
        "- ‚úÖ **Parser Agent** - Extracts clean text (with fallback)\n",
        "- ‚úÖ **FastAPI Backend** - REST API endpoint at `/run-flow`\n",
        "- ‚úÖ **MCP Orchestration** - Agents work together (or fallback if needed)\n",
        "- ‚úÖ **Standalone** - Everything in this notebook!\n",
        "- ‚úÖ **Reliable** - Fallback ensures it always works!\n",
        "\n",
        "## üß™ Quick Test (Optional)\n",
        "\n",
        "Try testing with different URLs:\n",
        "- `https://example.com` - Simple demo\n",
        "- `https://httpbin.org/html` - Sample HTML\n",
        "- Any website URL!\n",
        "\n",
        "## üìù Next Steps\n",
        "\n",
        "- Frontend integration (already built!)\n",
        "- Add more agents (QA Generator, etc.)\n",
        "- Deploy to production\n",
        "\n",
        "---\n",
        "\n",
        "**Made with ‚ù§Ô∏è for LMForge**\n",
        "\n",
        "**Note:** If MCP agents fail to initialize (stdio issues), the fallback implementation ensures the backend still works perfectly!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
