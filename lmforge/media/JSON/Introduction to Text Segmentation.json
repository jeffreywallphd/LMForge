[
  {
    "part": [
      {"text":"Introduction to Text SegmentationText Segmentation is the task of splitting text into meaningful segments. These segmentscan be composed of words, sentences, or topics. In this post, we look at a specific type ofText Segmentation task - Topic Segmentation, which divides a long body of text intosegments that correspond to a distinct topic or subtopic.For example, consider an hour-long podcast transcription generated by an AutomaticSpeech Recognition (ASR) system. The transcription can be lengthy and, as a result it canbe easy to lose track of which sentence you're currently reading. Automatic TopicSegmentation solves this problem by dividing the text into multiple segments, making thetranscription more readable.Figure: Topic SegmentationTypes of Text FormatsThere are different types of text we may want to segment. For example:• Written text like blogs, articles, news, etc.• Transcription of TV news, where a single person is talking• Transcription of a podcast where more than one person is talking• Transcription of phone calls (e.g., call center)• Transcription of an online meeting where many people are talkingThe above list is ordered based on the level of noise the text may contain (i.e., typos,grammatical errors, or incorrect usage of words in the case of automatic transcription).Noise is an important factor to consider when predicting topics because it contributes tothe quality of the segments predicted by the Topic Segmentation models. We will discussthis more in later sections.Since blogs and articles are mostly typed on a computer, they contain the least amount ofnoise.Transcriptions of online meetings, for example, contain the highest level of noise sincethere may be several people speaking with different quality microphones, with differentaccents and over varying internet connection quality, which can cause accuracy issueswith ASR systems. On the other hand, TV News Reports and Podcasts are usually recordedwith studio-quality microphones, so there is far less noise compared to online meetings,which results in much more accurate text transcriptions.Experience Advanced Text Processing with AssemblyAITry our Speech-to-Text API with advanced features like topic detection and auto chapters -perfect for implementing text segmentation in your projects.Try AssemblyAI Playground#Use Cases for Topic SegmentationNow that we know what Topic Segmentation is, and the types of text formats we areworking with, here are some real-life use cases:• Readability: One of the main reasons we want to split text into multiple paragraph-sized segments is readability. Consider this blog post without any section names,paragraphs, or breaks. The long string of text makes it much harder to read. AnAutomatic Topic Segmentation model can be used to break down long, continuoustext into bite-sized information so it is easier for the reader to consume.• Summarization: Text Summarization helps the reader to summarize any given text.However, if the text deals with multiple topics, it can be challenging for the TextSummarization model to capture all the topics in the summary. A TopicSegmentation model can make it easier to generate a summary for each segment inbullet points, covering all the topics or themes in a given text.• Turning News Reports into Articles: Today, news reports are often distributedthrough multiple channels that require different formats, including videos andwritten formats (like articles or blog posts). To reuse a news report video as anarticle or blog, we can transcribe the video using a Speech-to-Text API like the onewe build at AssemblyAI, and apply Topic Segmentation to the transcription. This willhelp to organize the article into a more readable format for the readers.• Information Retrieval: Given a large amount of text documents, we can cluster textthat belongs to the same topic. Once our documents are segmented by topic, wecan easily extract the information we need from each document.• AI Writing Assistant: For AI writing assistants like Grammarly, Topic Segmentationcan be used to suggest to writers when to start a new paragraph, making theirwriting more readable.Ready to Implement Text Segmentation?Sign up for AssemblyAI and start building powerful NLP applications with our state-of-the-art API.Start Building with AssemblyAI#Methods for Topic Segmentation and Evaluation MetricsBefore jumping into discussing the existing methods for Automatic Topic Segmentation,let's first investigate a potential solution to the topic segmentation problem and look atevaluation metrics for segmentation accuracy.To recap, in Automatic Topic Segmentation, our goal is to segment text by thetopic/subtopic. An Automatic Topic Segmentation model would classify each sentence in adocument and determine whether it is a boundary sentence (i.e., the last sentence of aparagraph). In other words, we can think of Topic Segmentation as a binary classificationproblem, where we classify each"},
      {
        "question": "What is text segmentation?",
        "answer": "Text segmentation is the task of splitting text into meaningful segments, which can be composed of words, sentences, or topics."
      },
      {
        "question": "What is topic segmentation?",
        "answer": "Topic segmentation is a specific type of text segmentation that divides a long body of text into segments corresponding to distinct topics or subtopics."
      },
      {
        "question": "Why is topic segmentation important for podcast transcriptions?",
        "answer": "Podcast transcriptions can be lengthy, making it easy to lose track of sentences. Topic segmentation divides the text into multiple segments, improving readability."
      },
      {
        "question": "What are some types of text formats that can be segmented?",
        "answer": "Examples include written text (blogs, articles, news), transcriptions of TV news, podcasts, phone calls, and online meetings."
      },
      {
        "question": "Which type of text format has the least noise?",
        "answer": "Blogs and articles contain the least amount of noise as they are mostly typed on a computer."
      },
      {
        "question": "Why do transcriptions of online meetings contain high levels of noise?",
        "answer": "Online meetings have multiple speakers using different microphones, accents, and varying internet quality, leading to accuracy issues in automatic transcription."
      },
      {
        "question": "What are some real-life use cases of topic segmentation?",
        "answer": "Real-life use cases include improving readability, aiding summarization, turning news reports into articles, information retrieval, and AI writing assistance."
      },
      {
        "question": "How does topic segmentation help in summarization?",
        "answer": "It allows the generation of summaries for each segment, making it easier to capture all topics in bullet points."
      },
      {
        "question": "How can topic segmentation be useful for turning news reports into articles?",
        "answer": "By applying topic segmentation to transcriptions of news report videos, the content can be better organized into readable articles or blog posts."
      },
      {
        "question": "How can AI writing assistants benefit from topic segmentation?",
        "answer": "AI writing assistants can suggest paragraph breaks to enhance readability by detecting topic changes in the text."
      },
      {
        "question": "What is the goal of automatic topic segmentation?",
        "answer": "The goal is to classify each sentence in a document and determine whether it is a boundary sentence, effectively segmenting the text by topics or subtopics."
      },
      {
        "question": "How can text segmentation improve information retrieval?",
        "answer": "Segmenting documents by topic allows for easier extraction of relevant information from large text datasets."
      },
      {
        "question": "What kind of classification problem is topic segmentation considered as?",
        "answer": "Topic segmentation can be considered a binary classification problem where each sentence is classified as a boundary or non-boundary sentence."
      }
    ]
  },


  {
    "part": [
      {"text":"Figure: Topic Segmentation as Binary ClassificationNow that we have established an understanding of the Topic Segmentation problem, let'sdiscuss its evaluation metrics. The most commonly used metrics are:• Precision & Recall• Pk• WindowDiffPrecision & RecallSince this is a binary classification problem, you might be tempted to use Precision &Recall as an evaluation metric. However, there are some challenges with using Precision &Recall for this type of classification problem. Let's first understand what Precision & Recallmean in relation to Topic Segmentation.Precision: percentage of boundaries identified by the model that are true boundariesRecall: percentage of true boundaries identified by the modelHowever, the main challenge with Precision & Recall is that they are not sensitive to nearmisses. To understand what a near miss is, let's consider two Topic Segmentation modelsA-0 and A-1. In the following figure, the Ref is the ground truth and each of the blocksrepresent a sentence. The vertical lines indicate a topic/subtopic segmentation boundary.Figure: Ground truth and output of segmentation algorithmsFrom the figure, you can clearly see the prediction made by model A-0 is pretty close to theground truth. This is called a near miss, where the prediction is off by one or two sentences.On the other hand, the prediction made by model A-1 is pretty far from the ground truth.Technically, model A-0 should be penalized significantly less than model A-1. But that is notthe case, both models get the same Precision & Recall score. This is because Precision &Recall do not consider how close or far away the boundary predictions are.Pk ScoreTo solve the challenges with Precision & Recall, the Pk score was introduced byBeeferemen et al (1997).Pk is calculated by using a sliding window-based method. The window size is usually set tohalf of the average true segment number. While sliding the window, the algorithmdetermines whether the two ends of the window are in the same or different segments inthe ground truth segmentation, and increases a counter if there is a mismatch. The finalscore is calculated by scaling the penalty between 0 and 1 and dividing the number ofmeasurements.Figure: Sliding window over reference and predictions (Pevzner and Hearst - 2002)A model that predicts all the boundaries correctly gets a score of 0. So the lower the score,the better.Challenges with the Pk Evaluation Metric:• False negatives are penalized more than false positives.• Does not take the number of boundaries into consideration. If there are multipleboundaries inside the window, Pk doesn't consider that.• Sensitive to the variation in segment size.• Near-miss errors are penalized too much.WindowDiffWindowDiff was introduced to solve the challenges with the Pk score. This is alsocalculated by a sliding window. In this case, for each position of the window of size k, wesimply compare how many boundaries are in the ground truth, and how many boundariesare predicted by the Topic Segmentation model.Figure: Equation for calculating WindowDiff scoreHere, b(i, j) is a function that returns the number of boundaries between two positions i andj in the text. N represents the number of sentences in the text.In practice, both Pk and WindowDiff scores are used to evaluate a model. A lower scoremeans predictions are closer to the actual boundaries. For a more detailed comparison ofthese metrics and exactly how WindowDiff score solves the challenges with Pk, you canrefer to Pevzner et al (2002).#Methods for Topic SegmentationIn this section, we take a look at the most common methods of Topic Segmentation, whichcan be divided into mainly two groups - Supervised & Unsupervised.Supervised ApproachesSupervised approaches are pretty straightforward - We take a labelled dataset and then wetry to fit a model on it. This works well when we have a domain-specific segmentation taskand the dataset belongs to the same domain. For example, if we know that our model willbe seeing texts similar to Wikipedia at inference time, then training a model on Wiki-727kwill produce the best result. However, it will perform worse if used in other domains, forexample on a news article or transcriptions of meetings.In the supervised approach, we want to classify each sentence to determine whether it is aboundary sentence or not. Here is how the pipeline works at high level:1. Take text as input2. Extract all the sentences from the text, i.e., segment the text into sentences. (Youcan use libraries like nltk, stanza, trankit etc. for this task)3. Classify each sentence--this will be a binary classificationOne important thing to note is that we're taking input at a word level and predicting on asentence level. This means we need to convert the word-level representation (embeddings)to the sentence level. We can add the embeddings of all the tokens in a sentence to get anaggregated representation. This will give us sentence-level embeddings.Figure: Calculation of sentence embeddings from word embeddingAdditionally, if we use a model like BERT for this, we can get the embedding of the [CLS]token instead of aggregating all the word embeddings in a sentence. This is because inBERT, the [CLS] token aggregates the representation of the whole sequence.Next, we pass the embeddings to a bidirectional LSTM or a Transformer and calculatesoftmax over the output. Using a bidirectional LSTM/Transformer is a good idea herebecause it will enable the model to look at both the left and right context of the sentencebefore making a decision.Figure: Boundary sentence classifier modelThe LSTM based approach described above is actually used in Koshorek et al., whichachieved a 22.13 Pk score on the Wiki-727k dataset.Glavas et al. (2020) proposed a Transformer-based model which holds the current State-of-the-Art result on datasets like Wiki-727k, CHOI, Elements & Cities etc. They used two-levelTransformers: one at token-level and another one at sentence-level. On top of it, theprediction objective was augmented with an auxiliary coherence modeling objective. Thekey idea is that text coherence is related to text segmentation. This means a text within asegment is expected to be more coherent than the text in a different segment.Unsupervised ApproachesUnsupervised approaches neither have a learning phase nor labelled data. Therefore,unsupervised approaches leverage different techniques for Topic Segmentation, such as:• Lexical Cohesion• Topic Modeling• Graph• Similarity MeasurementWe will cover how these approaches work at a high level.Lexical CohesionA group of words is “lexically cohesive” if they are semantically related. The level of lexicalcohesion is determined by the lexical frequency and distribution, and there are algorithmsthat exploit lexical cohesion to segment the text. The idea is that when there is a subtopicshift, the lexical cohesion will be lower between two blocks of text, and we can segmentthe text based on that.TextTiling: TextTiling was introduced by Hearst (1997) and is one of the first unsupervisedtopic segmentation algorithms. It's a moving window-based approach that uses lexicalcohesion between blocks of text to detect topic boundaries.The algorithm has three main components:• First, it divides the input text into sequences of relevant tokens and calculates thecohesion at each potential boundary point.• It then uses these cohesion scores to produce depth scores for each potentialboundary point that has a lower cohesion than the neighboring boundary points.• Using these depth scores, the algorithm is able to select boundary points where thedepth is low relative to the other depth scores, indicating that the gap represents atopic shift in the text.LCseg: LCseg was introduced by Galley et al. (2003) for segmenting multipartyconversations. The algorithm uses lexical cohesion to segment topics, and it can handleboth speech and text. The core algorithm of LCseg has two main parts:"},
      {
        "question": "What are the challenges of using Precision & Recall in topic segmentation?",
        "answer": "Precision & Recall are not sensitive to near misses, meaning that predictions that are slightly off by one or two sentences are penalized equally as predictions that are far off. This makes it difficult to distinguish between models with close and distant predictions."
      },
      {
        "question": "What is the Pk score and how is it calculated?",
        "answer": "The Pk score, introduced by Beeferemen et al. (1997), is calculated using a sliding window-based method. The window size is typically set to half of the average true segment number. The algorithm checks if the two ends of the window belong to the same or different segments in the ground truth and increments a counter if there is a mismatch. The final score is scaled between 0 and 1."
      },
      {
        "question": "What are the challenges associated with the Pk evaluation metric?",
        "answer": "Challenges with the Pk evaluation metric include: false negatives being penalized more than false positives, not considering the number of boundaries within the window, sensitivity to segment size variation, and excessive penalization of near-miss errors."
      },
      {
        "question": "How does WindowDiff improve upon the Pk score?",
        "answer": "WindowDiff solves the challenges of Pk by counting the number of boundaries within a sliding window and comparing them with the ground truth. It ensures that the evaluation considers both over- and under-segmentation, making it more robust against segment size variations."
      },
      {
        "question": "What are the two main approaches to topic segmentation?",
        "answer": "The two main approaches are supervised and unsupervised methods. Supervised approaches use labeled datasets to train models, whereas unsupervised approaches do not require labeled data and instead leverage techniques like lexical cohesion, topic modeling, graph-based methods, and similarity measurement."
      },
      {
        "question": "What are some commonly used datasets for topic segmentation?",
        "answer": "Commonly used datasets include TDT Corpus, CHOI, Galley Dataset, Elements and Cities, Wiki727k, Malach Corpus, and QMSum."
      },
      {
        "question": "What is TextTiling and how does it work?",
        "answer": "TextTiling, introduced by Hearst (1997), is an unsupervised topic segmentation algorithm that divides text into sequences, calculates lexical cohesion at potential boundary points, generates depth scores, and selects boundary points where cohesion is low relative to neighboring points."
      },
      {
        "question": "What are the key differences between supervised and unsupervised topic segmentation approaches?",
        "answer": "Supervised approaches rely on labeled data and specific domain knowledge, while unsupervised approaches do not require labeled data and use techniques such as lexical cohesion, topic modeling, and graph-based methods to segment text."
      },
      {
        "question": "How does BERT help in supervised topic segmentation?",
        "answer": "BERT provides contextual embeddings where the [CLS] token can be used to aggregate representations of entire sentences. These embeddings can then be passed to a bidirectional LSTM or Transformer to classify sentences as boundary or non-boundary."
      },
      {
        "question": "What are some key takeaways regarding topic segmentation?",
        "answer": "Segmenting text improves readability and downstream tasks, the main evaluation metrics are Precision & Recall, Pk, and WindowDiff, and both supervised and unsupervised methods provide viable approaches for text segmentation."
      }
    ]
  }
]

