FROM ollama/ollama:latest
RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*
RUN mkdir -p /app /root/.ollama
EXPOSE 11434

RUN echo '#!/bin/bash\n\
echo "Starting Unified Ollama service with GPU/CPU auto-detection..."\n\
# Detect GPU availability and configure accordingly\n\
if nvidia-smi >/dev/null 2>&1; then\n\
  echo "GPU detected, enabling GPU acceleration"\n\
  export OLLAMA_GPU_ENABLED=1\n\
  # Use environment variable or default\n\
  export OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-6}\n\
  echo "GPU mode: $OLLAMA_NUM_PARALLEL parallel requests for optimal throughput"\n\
else\n\
  echo "No GPU detected, using optimized CPU mode"\n\
  export OLLAMA_GPU_ENABLED=0\n\
  # Use environment variable or default\n\
  export OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-4}\n\
  echo "CPU mode: $OLLAMA_NUM_PARALLEL parallel requests"\n\
fi\n\
# Use environment variables from docker-compose with fallbacks\n\
export OLLAMA_MAX_LOADED_MODELS=${OLLAMA_MAX_LOADED_MODELS:-1}\n\
export OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-5m}\n\
export OLLAMA_MAX_QUEUE=${OLLAMA_MAX_QUEUE:-32}\n\
export OLLAMA_FLASH_ATTENTION=${OLLAMA_FLASH_ATTENTION:-false}\n\
export OLLAMA_KV_CACHE_TYPE=${OLLAMA_KV_CACHE_TYPE:-f16}\n\
export OLLAMA_GPU_OVERHEAD=${OLLAMA_GPU_OVERHEAD:-0}\n\
export OLLAMA_CONTEXT_LENGTH=${OLLAMA_CONTEXT_LENGTH:-1024}\n\
export OLLAMA_MULTIUSER_CACHE=${OLLAMA_MULTIUSER_CACHE:-false}\n\
export OLLAMA_SCHED_SPREAD=${OLLAMA_SCHED_SPREAD:-true}\n\
export OLLAMA_LOAD_TIMEOUT=${OLLAMA_LOAD_TIMEOUT:-5m}\n\
# Memory and stability settings\n\
export OLLAMA_HOST=0.0.0.0\n\
export OLLAMA_ORIGINS="*"\n\
export OLLAMA_MAX_VRAM=${OLLAMA_MAX_VRAM:-2048}\n\
export OLLAMA_CPU_THREADS=${OLLAMA_CPU_THREADS:-2}\n\
echo "Configuration:"\n\
echo "  Max Loaded Models: $OLLAMA_MAX_LOADED_MODELS"\n\
echo "  Parallel Requests: $OLLAMA_NUM_PARALLEL"\n\
echo "  GPU Overhead: $OLLAMA_GPU_OVERHEAD"\n\
echo "  Context Length: $OLLAMA_CONTEXT_LENGTH"\n\
echo "  Keep Alive: $OLLAMA_KEEP_ALIVE"\n\
# Start Ollama server\n\
ollama serve &\n\
OLLAMA_PID=$!\n\
echo "Waiting for Ollama service to start..."\n\
sleep 15\n\
# Wait for API to be ready\n\
while ! curl -f http://localhost:11434/api/tags >/dev/null 2>&1; do\n\
  echo "Waiting for Ollama API..."\n\
  sleep 5\n\
done\n\
echo "Ollama API ready, pulling and optimizing models..."\n\
# Pull both models in parallel for faster startup\n\
(\n\
  echo "Pulling all-minilm:33m (33MB embedding model)..."\n\
  if ollama pull all-minilm:33m; then\n\
    echo "Embedding model ready - testing with minimal load"\n\
    # Test embedding model with very simple request\n\
    curl -s -X POST http://localhost:11434/api/embeddings \\\n\
      -H "Content-Type: application/json" \\\n\
      -d '"'"'{"model":"all-minilm:33m","prompt":"test"}'"'"' > /dev/null || echo "Embedding warmup failed"\n\
    echo "Embedding model tested"\n\
  else\n\
    echo "Failed to pull embedding model"\n\
  fi\n\
) &\n\
(\n\
  echo "Pulling qwen2.5:0.5b-instruct (395MB)..."\n\
  if ollama pull qwen2.5:0.5b-instruct; then\n\
    echo "Chat model ready - testing with minimal settings"\n\
    # Test chat model with ultra-conservative settings\n\
    curl -s -X POST http://localhost:11434/api/generate \\\n\
      -H "Content-Type: application/json" \\\n\
      -d '"'"'{"model":"qwen2.5:0.5b-instruct","prompt":"Hi","stream":false,"options":{"num_ctx":512,"num_predict":10,"temperature":0.1,"num_gpu":0,"num_thread":1}}'"'"' > /dev/null || echo "Chat warmup failed"\n\
    echo "Chat model tested and ready"\n\
  else\n\
    echo "Failed to pull chat model"\n\
  fi\n\
) &\n\
# Wait for both downloads to complete\n\
wait\n\
echo "Unified Ollama service ready with stable models"\n\
echo "Embedding: all-minilm:33m (tested and stable)"\n\
echo "Chat: qwen2.5:0.5b-instruct (CPU mode for stability)"\n\
echo "Parallel capacity: $OLLAMA_NUM_PARALLEL requests (conservative)"\n\
wait $OLLAMA_PID\n\
' > /app/start.sh && chmod +x /app/start.sh

HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 CMD curl -f http://localhost:11434/api/tags || exit 1
ENTRYPOINT ["/app/start.sh"]