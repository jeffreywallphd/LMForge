{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a903082",
   "metadata": {},
   "source": [
    "#### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f725fd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.29.2\n",
      "<class 'transformers.training_args.TrainingArguments'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "import logging\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    # tokenizers\n",
    "    AutoTokenizer,\n",
    "    DebertaV2Tokenizer,\n",
    "    DistilBertTokenizer,\n",
    "    BertTokenizer,\n",
    "    RobertaTokenizer,\n",
    "    ElectraTokenizer,\n",
    "    AlbertTokenizer,\n",
    "    XLNetTokenizer,\n",
    "    MobileBertTokenizer,\n",
    "    # models\n",
    "    DebertaV2ForSequenceClassification,\n",
    "    DistilBertForSequenceClassification,\n",
    "    BertForSequenceClassification,\n",
    "    RobertaForSequenceClassification,\n",
    "    ElectraForSequenceClassification,\n",
    "    AlbertForSequenceClassification,\n",
    "    XLNetForSequenceClassification,\n",
    "    MobileBertForSequenceClassification,\n",
    ")\n",
    "from torch.nn import CrossEntropyLoss\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "print(transformers.TrainingArguments)\n",
    "\n",
    "# Cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eadf70b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='classification.log', level=logging.INFO)\n",
    "logging.info(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3f82b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: D:/huggingface_cache\n",
      "TRANSFORMERS_CACHE: D:/huggingface_cache\n",
      "HUGGINGFACE_HUB_CACHE: D:/huggingface_cache\n"
     ]
    }
   ],
   "source": [
    "# setting huggingface token\n",
    "login(token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"D:/huggingface_cache\" \n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"D:/huggingface_cache\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"D:/huggingface_cache\"\n",
    "\n",
    "print(\"HF_HOME:\", os.getenv(\"HF_HOME\"))\n",
    "print(\"TRANSFORMERS_CACHE:\", os.getenv(\"TRANSFORMERS_CACHE\"))\n",
    "print(\"HUGGINGFACE_HUB_CACHE:\", os.getenv(\"HUGGINGFACE_HUB_CACHE\"))\n",
    "\n",
    "logging.info(f\"HF_HOME: {os.getenv('HF_HOME')}\")\n",
    "logging.info(f\"TRANSFORMERS_CACHE: {os.getenv('TRANSFORMERS_CACHE')}\")\n",
    "logging.info(f\"HUGGINGFACE_HUB_CACHE: {os.getenv('HUGGINGFACE_HUB_CACHE')}\")\n",
    "\n",
    "transformers.utils.hub.TRANSFORMERS_CACHE = \"D:/huggingface_cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e30cc",
   "metadata": {},
   "source": [
    "### LOADING SQLITE DB WITH RECORDS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04c002d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to JSONL file.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "DB_FILE = \"chunks.db\"\n",
    "OUTPUT_FILE = \"exported_chunks.jsonl\"\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Query all data from chunks table\n",
    "cur.execute(\"SELECT text, label FROM chunks\")\n",
    "rows = cur.fetchall()\n",
    "\n",
    "# Write to JSONL\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for text, label in rows:\n",
    "        obj = {\"text\": text}\n",
    "        if label is not None:\n",
    "            obj[\"label\"] = label\n",
    "        f.write(json.dumps(obj) + \"\\n\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"Data exported to JSONL file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47d48089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled chunks after removing label 11: {1: 8199, 0: 800}\n",
      "Final labeled chunks: {1: 5384, 0: 800}\n"
     ]
    }
   ],
   "source": [
    "# Load the labeled chunks\n",
    "with open(\"exported_chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    labeled_chunks = [json.loads(line) for line in f]\n",
    "\n",
    "data = pd.DataFrame(labeled_chunks)\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "\n",
    "# Get the first 9000 rows\n",
    "data = data.head(9000)\n",
    "\n",
    "# Remove rows with label == 11\n",
    "data = data[data['label'] != 11]\n",
    "\n",
    "# Print labeled count after removing label 11\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "print(f\"Labeled chunks after removing label 11: {labeled_count}\")\n",
    "\n",
    "# Remove rows where label == 1 and text length < 100\n",
    "data = data[~((data['label'] == 1) & (data['text'].str.len() < 100))]\n",
    "\n",
    "# Print final labeled count\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "print(f\"Final labeled chunks: {labeled_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ca564",
   "metadata": {},
   "source": [
    "##### Spliting data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5f9aa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([3.8648, 0.5743], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "data['label'] = data['label'].astype(int)\n",
    "\n",
    "# Train-Test Split using stratified sampling\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, stratify=data['label'], random_state=42)\n",
    "\n",
    "# since there is a class imbalance, we will compute class weights\n",
    "# to handle this in the loss function\n",
    "labels = train_df[\"label\"].values\n",
    "# Compute class weights\n",
    "classes = np.unique(labels)\n",
    "weights = compute_class_weight(class_weight=\"balanced\",\n",
    "                            classes=classes,\n",
    "                            y=labels)\n",
    "class_weights = torch.tensor(weights, dtype=torch.float, device=device)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# Convert ing the DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a629272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Tuning parameters ----\n",
    "CONFIG = {\n",
    "    \"epochs\": 2,\n",
    "    \"batch_size\": 16,\n",
    "    \"batch_size_larger\": 8, # Batch size for larger models\n",
    "    \"max_length\": [128, 256, 512], # Max length of input sequences\n",
    "    \"learning_rate\": 5e-5, # Learning rate for the optimizer\n",
    "    \"learning_rate_larger\": 3e-5, # Learning rate for larger models\n",
    "    \"weight_decay\": 0.01, # Weight decay for regularization\n",
    "    \"output_dir\": \"D:/huggingface_cache/classification_models\"\n",
    "}\n",
    "\n",
    "# ---- Model configurations ----\n",
    "MODEL_CONFIGS = {\n",
    "    \"deberta\": {\n",
    "        \"tokenizer_class\": DebertaV2Tokenizer,\n",
    "        \"pretrained_model_name\": \"microsoft/deberta-v3-small\", # params 55M\n",
    "        \"model_class\": DebertaV2ForSequenceClassification\n",
    "    },\n",
    "    \"distilbert\": {\n",
    "        \"tokenizer_class\": DistilBertTokenizer,\n",
    "        \"pretrained_model_name\": \"distilbert-base-uncased\", # params 66M\n",
    "        \"model_class\": DistilBertForSequenceClassification\n",
    "    },\n",
    "    \"bert\": {\n",
    "        \"tokenizer_class\": BertTokenizer,\n",
    "        \"pretrained_model_name\": \"bert-base-uncased\", # params 110M\n",
    "        \"model_class\": BertForSequenceClassification\n",
    "    },\n",
    "    \"roberta\": {\n",
    "        \"tokenizer_class\": RobertaTokenizer,\n",
    "        \"pretrained_model_name\": \"roberta-base\", # params 125M\n",
    "        \"model_class\": RobertaForSequenceClassification\n",
    "    },\n",
    "    \"electra\": {\n",
    "        \"tokenizer_class\": ElectraTokenizer,\n",
    "        \"pretrained_model_name\": \"google/electra-small-discriminator\", # params 14M\n",
    "        \"model_class\": ElectraForSequenceClassification\n",
    "    },\n",
    "    \"albert\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-base-v2\", # params 11M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"xlnet\": {\n",
    "        \"tokenizer_class\": XLNetTokenizer,\n",
    "        \"pretrained_model_name\": \"xlnet-base-cased\", # params 110M\n",
    "        \"model_class\": XLNetForSequenceClassification\n",
    "    },\n",
    "    \"mobilebert\": {\n",
    "        \"tokenizer_class\": AutoTokenizer,\n",
    "        \"pretrained_model_name\": \"google/mobilebert-uncased\", # params 25M\n",
    "        \"model_class\": MobileBertForSequenceClassification\n",
    "    },\n",
    "    \"albert-base-v1\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-base-v1\", # params 12M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"albert-large-v2\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-large-v2\", # params 18M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"albert-xlarge-v2\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-xlarge-v2\", # params 60M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"albert-xxlarge-v2\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-xxlarge-v2\", # params 235M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"bert-large-uncased\": {\n",
    "        \"tokenizer_class\": BertTokenizer,\n",
    "        \"pretrained_model_name\": \"bert-large-uncased\", # params 340M\n",
    "        \"model_class\": BertForSequenceClassification\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a3199a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Metric function ----\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba905e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Weighted Trainer ----\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc761c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Function to train and evaluate ----\n",
    "def train_and_evaluate(model_name, train_dataset, test_dataset, data, max_length):\n",
    "    print(f\"\\n===== Training {model_name} =====\")\n",
    "\n",
    "    # Model + tokenizer\n",
    "    cfg = MODEL_CONFIGS[model_name]\n",
    "    tokenizer = cfg[\"tokenizer_class\"].from_pretrained(cfg[\"pretrained_model_name\"])\n",
    "    model = cfg[\"model_class\"].from_pretrained(\n",
    "        cfg[\"pretrained_model_name\"],\n",
    "        num_labels=len(data['label'].unique())\n",
    "    )\n",
    "\n",
    "    # Tokenization\n",
    "    def tokenize_fn(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "    train_enc = train_dataset.map(tokenize_fn, batched=True)\n",
    "    test_enc = test_dataset.map(tokenize_fn, batched=True)\n",
    "    train_enc = train_enc.rename_column(\"label\", \"labels\")\n",
    "    test_enc = test_enc.rename_column(\"label\", \"labels\")\n",
    "    train_enc.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    test_enc.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    # Training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{CONFIG['output_dir']}/{model_name}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "        per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "        num_train_epochs=CONFIG[\"epochs\"],\n",
    "        weight_decay=CONFIG[\"weight_decay\"],\n",
    "        logging_dir=f\"./logs_{model_name}\",\n",
    "        report_to=\"none\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = WeightedLossTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_enc,\n",
    "        eval_dataset=test_enc,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "\n",
    "    # Train + evaluate\n",
    "    # time to train\n",
    "    t_train_start = torch.cuda.Event(enable_timing=True)\n",
    "    t_train_end = torch.cuda.Event(enable_timing=True)\n",
    "    t_train_start.record()\n",
    "    trainer.train()\n",
    "    t_train_end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    t_train_time = t_train_start.elapsed_time(t_train_end) / 1000  # convert to seconds\n",
    "    print(f\"Training time for {model_name}: {t_train_time:.2f} seconds\")\n",
    "\n",
    "    # Predictions\n",
    "    t_pred_start = torch.cuda.Event(enable_timing=True)\n",
    "    t_pred_end = torch.cuda.Event(enable_timing=True)\n",
    "    t_pred_start.record()\n",
    "    preds = trainer.predict(test_enc)\n",
    "    t_pred_end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    t_pred_time = t_pred_start.elapsed_time(t_pred_end) / 1000  # convert to seconds\n",
    "    print(f\"Prediction time for {model_name}: {t_pred_time:.2f} seconds\")\n",
    "    y_true = preds.label_ids\n",
    "    y_pred = np.argmax(preds.predictions, axis=1)\n",
    "\n",
    "    print(f\"\\nClassification Report for {model_name}:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "    performance = {\n",
    "        \"model\": model_name,\n",
    "        \"max_length\": max_length,\n",
    "        \"train_time_sec\": t_train_time,\n",
    "        \"pred_time_sec\": t_pred_time,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1\": precision_recall_fscore_support(y_true, y_pred, average='weighted')[2],\n",
    "        \"precision\": precision_recall_fscore_support(y_true, y_pred, average='weighted')[0],\n",
    "        \"recall\": precision_recall_fscore_support(y_true, y_pred, average='weighted')[1],\n",
    "    }\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95403fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping deberta with max_length=128 (already in CSV).\n",
      "Skipping deberta with max_length=256 (already in CSV).\n",
      "Skipping deberta with max_length=512 (already in CSV).\n",
      "Skipping distilbert with max_length=128 (already in CSV).\n",
      "Skipping distilbert with max_length=256 (already in CSV).\n",
      "Skipping distilbert with max_length=512 (already in CSV).\n",
      "Skipping bert with max_length=128 (already in CSV).\n",
      "Skipping bert with max_length=256 (already in CSV).\n",
      "Skipping bert with max_length=512 (already in CSV).\n",
      "Skipping roberta with max_length=128 (already in CSV).\n",
      "Skipping roberta with max_length=256 (already in CSV).\n",
      "Skipping roberta with max_length=512 (already in CSV).\n",
      "Skipping electra with max_length=128 (already in CSV).\n",
      "Skipping electra with max_length=256 (already in CSV).\n",
      "Skipping electra with max_length=512 (already in CSV).\n",
      "Skipping albert with max_length=128 (already in CSV).\n",
      "Skipping albert with max_length=256 (already in CSV).\n",
      "Skipping albert with max_length=512 (already in CSV).\n",
      "Skipping xlnet with max_length=128 (already in CSV).\n",
      "Skipping xlnet with max_length=256 (already in CSV).\n",
      "Skipping xlnet with max_length=512 (already in CSV).\n",
      "Skipping mobilebert with max_length=128 (already in CSV).\n",
      "Skipping mobilebert with max_length=256 (already in CSV).\n",
      "Skipping mobilebert with max_length=512 (already in CSV).\n",
      "Skipping albert-base-v1 with max_length=128 (already in CSV).\n",
      "Skipping albert-base-v1 with max_length=256 (already in CSV).\n",
      "Skipping albert-base-v1 with max_length=512 (already in CSV).\n",
      "Skipping albert-large-v2 with max_length=128 (already in CSV).\n",
      "Skipping albert-large-v2 with max_length=256 (already in CSV).\n",
      "Skipping albert-large-v2 with max_length=512 (already in CSV).\n",
      "Skipping albert-xlarge-v2 with max_length=128 (already in CSV).\n",
      "Skipping albert-xlarge-v2 with max_length=256 (already in CSV).\n",
      "Skipping albert-xlarge-v2 with max_length=512 (already in CSV).\n",
      "Skipping albert-xxlarge-v2 with max_length=128 (already in CSV).\n",
      "\n",
      "--- Training albert-xxlarge-v2 with max_length=256 ---\n",
      "\n",
      "===== Training albert-xxlarge-v2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at albert-xxlarge-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-xxlarge-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408422112875423682738dcbef87d871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b60e4ee1e7549eaaabed0320a5cd3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc395e64da4442bfaf4bc26ec70076d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcbd120c06694ecd96516087c43954fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42707252502441406, 'eval_accuracy': 0.9652384801940178, 'eval_f1': 0.9636207000414232, 'eval_precision': 0.9649097316821669, 'eval_recall': 0.9652384801940178, 'eval_runtime': 83.0967, 'eval_samples_per_second': 14.886, 'eval_steps_per_second': 0.939, 'epoch': 1.0}\n",
      "{'loss': 0.4781, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c376c05d48d4492ca0784c43f402127f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3642871677875519, 'eval_accuracy': 0.9660468876313663, 'eval_f1': 0.9656744092881774, 'eval_precision': 0.9654725909095633, 'eval_recall': 0.9660468876313663, 'eval_runtime': 83.7629, 'eval_samples_per_second': 14.768, 'eval_steps_per_second': 0.931, 'epoch': 2.0}\n",
      "{'train_runtime': 1842.9475, 'train_samples_per_second': 5.369, 'train_steps_per_second': 0.336, 'train_loss': 0.44406432490194997, 'epoch': 2.0}\n",
      "Training time for albert-xxlarge-v2: 1842.99 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad4c9f5ec704a87988fd6086c76463e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-xxlarge-v2: 81.95 seconds\n",
      "\n",
      "Classification Report for albert-xxlarge-v2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8882    0.8438    0.8654       160\n",
      "           1     0.9770    0.9842    0.9806      1077\n",
      "\n",
      "    accuracy                         0.9660      1237\n",
      "   macro avg     0.9326    0.9140    0.9230      1237\n",
      "weighted avg     0.9655    0.9660    0.9657      1237\n",
      "\n",
      "{'model': 'albert-xxlarge-v2', 'max_length': 256, 'train_time_sec': 1842.992125, 'pred_time_sec': 81.9478671875, 'accuracy': 0.9660468876313663, 'f1': 0.9656744092881774, 'precision': 0.9654725909095633, 'recall': 0.9660468876313663}\n",
      "\n",
      "--- Training albert-xxlarge-v2 with max_length=512 ---\n",
      "\n",
      "===== Training albert-xxlarge-v2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at albert-xxlarge-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-xxlarge-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac3663eaf384e3bb97450e2053b765c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f15b04cc25440694d13809d6d2286a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f20001e3b654d0d8f8def65d6bcfa59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a24ed87c124af685ead1f105d1bb24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4386589229106903, 'eval_accuracy': 0.957962813257882, 'eval_f1': 0.9545473094953939, 'eval_precision': 0.9594244937215558, 'eval_recall': 0.957962813257882, 'eval_runtime': 4470.4325, 'eval_samples_per_second': 0.277, 'eval_steps_per_second': 0.017, 'epoch': 1.0}\n",
      "{'loss': 0.5749, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4a40214c704cd58aae90f8261c484b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5354265570640564, 'eval_accuracy': 0.9644300727566694, 'eval_f1': 0.962952886634131, 'eval_precision': 0.9637956729972517, 'eval_recall': 0.9644300727566694, 'eval_runtime': 5277.0158, 'eval_samples_per_second': 0.234, 'eval_steps_per_second': 0.015, 'epoch': 2.0}\n",
      "{'train_runtime': 57596.1703, 'train_samples_per_second': 0.172, 'train_steps_per_second': 0.011, 'train_loss': 0.5363737721596995, 'epoch': 2.0}\n",
      "Training time for albert-xxlarge-v2: 57596.08 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342d77f1c1224c82b94562c1292287ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-xxlarge-v2: 5336.47 seconds\n",
      "\n",
      "Classification Report for albert-xxlarge-v2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9394    0.7750    0.8493       160\n",
      "           1     0.9674    0.9926    0.9798      1077\n",
      "\n",
      "    accuracy                         0.9644      1237\n",
      "   macro avg     0.9534    0.8838    0.9146      1237\n",
      "weighted avg     0.9638    0.9644    0.9630      1237\n",
      "\n",
      "{'model': 'albert-xxlarge-v2', 'max_length': 512, 'train_time_sec': 57596.084, 'pred_time_sec': 5336.474, 'accuracy': 0.9644300727566694, 'f1': 0.962952886634131, 'precision': 0.9637956729972517, 'recall': 0.9644300727566694}\n",
      "\n",
      "--- Training bert-large-uncased with max_length=128 ---\n",
      "\n",
      "===== Training bert-large-uncased =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f3a591f8c04a47a0187dc88e43183a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17c6bd9122c4d8c9664de33d468a3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5048a00a244ecabbb17e45f60c61f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf40a46fcc943648649543b2e4ab9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26782798767089844, 'eval_accuracy': 0.9636216653193209, 'eval_f1': 0.9634748259573163, 'eval_precision': 0.9633511663551466, 'eval_recall': 0.9636216653193209, 'eval_runtime': 7.1254, 'eval_samples_per_second': 173.603, 'eval_steps_per_second': 10.947, 'epoch': 1.0}\n",
      "{'loss': 0.3518, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6b17241f0d45c59be3384dbe6c05ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2301100343465805, 'eval_accuracy': 0.9725141471301536, 'eval_f1': 0.9722126170428104, 'eval_precision': 0.9720861990164603, 'eval_recall': 0.9725141471301536, 'eval_runtime': 6.966, 'eval_samples_per_second': 177.576, 'eval_steps_per_second': 11.197, 'epoch': 2.0}\n",
      "{'train_runtime': 973.0466, 'train_samples_per_second': 10.168, 'train_steps_per_second': 0.637, 'train_loss': 0.31489719575451264, 'epoch': 2.0}\n",
      "Training time for bert-large-uncased: 973.10 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e081101d3c14a18ae84a25e94691627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for bert-large-uncased: 6.84 seconds\n",
      "\n",
      "Classification Report for bert-large-uncased:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9145    0.8688    0.8910       160\n",
      "           1     0.9806    0.9879    0.9843      1077\n",
      "\n",
      "    accuracy                         0.9725      1237\n",
      "   macro avg     0.9476    0.9283    0.9376      1237\n",
      "weighted avg     0.9721    0.9725    0.9722      1237\n",
      "\n",
      "{'model': 'bert-large-uncased', 'max_length': 128, 'train_time_sec': 973.096375, 'pred_time_sec': 6.839703125, 'accuracy': 0.9725141471301536, 'f1': 0.9722126170428104, 'precision': 0.9720861990164603, 'recall': 0.9725141471301536}\n",
      "\n",
      "--- Training bert-large-uncased with max_length=256 ---\n",
      "\n",
      "===== Training bert-large-uncased =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322187d4f1e14830a863b8dde7fde15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a1f0873bcb4604b41550faad044015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f720c36e8aa04c83a64ea9ecdbae75c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b48cbea416844cc91be5d7682872e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.292150616645813, 'eval_accuracy': 0.973322554567502, 'eval_f1': 0.9728377426864748, 'eval_precision': 0.9728293922272504, 'eval_recall': 0.973322554567502, 'eval_runtime': 12.9768, 'eval_samples_per_second': 95.324, 'eval_steps_per_second': 6.011, 'epoch': 1.0}\n",
      "{'loss': 0.2815, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bf9100710b4745809297bdcc5e0f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24471412599086761, 'eval_accuracy': 0.9757477768795473, 'eval_f1': 0.9754126476261835, 'eval_precision': 0.975357579606813, 'eval_recall': 0.9757477768795473, 'eval_runtime': 12.9859, 'eval_samples_per_second': 95.257, 'eval_steps_per_second': 6.006, 'epoch': 2.0}\n",
      "{'train_runtime': 1936.5742, 'train_samples_per_second': 5.109, 'train_steps_per_second': 0.32, 'train_loss': 0.2562798253951534, 'epoch': 2.0}\n",
      "Training time for bert-large-uncased: 1936.58 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea92ec89bd84c67a9070d57eda9c2d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for bert-large-uncased: 12.72 seconds\n",
      "\n",
      "Classification Report for bert-large-uncased:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9333    0.8750    0.9032       160\n",
      "           1     0.9816    0.9907    0.9861      1077\n",
      "\n",
      "    accuracy                         0.9757      1237\n",
      "   macro avg     0.9575    0.9329    0.9447      1237\n",
      "weighted avg     0.9754    0.9757    0.9754      1237\n",
      "\n",
      "{'model': 'bert-large-uncased', 'max_length': 256, 'train_time_sec': 1936.581625, 'pred_time_sec': 12.7241767578125, 'accuracy': 0.9757477768795473, 'f1': 0.9754126476261835, 'precision': 0.975357579606813, 'recall': 0.9757477768795473}\n",
      "\n",
      "--- Training bert-large-uncased with max_length=512 ---\n",
      "\n",
      "===== Training bert-large-uncased =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a60c5ed7c9483eaeb73dcf88255ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d278f23ca324e3cb2ca47419cfcdf02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01fa065b8fe4f2bb3c8b27a63f6320a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff67def41c54e9589b864343dff37d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34209224581718445, 'eval_accuracy': 0.9717057396928052, 'eval_f1': 0.9709364293477776, 'eval_precision': 0.9712126527963085, 'eval_recall': 0.9717057396928052, 'eval_runtime': 334.0841, 'eval_samples_per_second': 3.703, 'eval_steps_per_second': 0.233, 'epoch': 1.0}\n",
      "{'loss': 0.2866, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b957a31b734fe2b45cb9ab13dc90c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20411457121372223, 'eval_accuracy': 0.9781729991915926, 'eval_f1': 0.9777117433869209, 'eval_precision': 0.9779044235111473, 'eval_recall': 0.9781729991915926, 'eval_runtime': 334.4073, 'eval_samples_per_second': 3.699, 'eval_steps_per_second': 0.233, 'epoch': 2.0}\n",
      "{'train_runtime': 9865.2664, 'train_samples_per_second': 1.003, 'train_steps_per_second': 0.063, 'train_loss': 0.25964279790078443, 'epoch': 2.0}\n",
      "Training time for bert-large-uncased: 9865.26 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ed33f748d145f89255ec97eae257eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for bert-large-uncased: 334.98 seconds\n",
      "\n",
      "Classification Report for bert-large-uncased:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9586    0.8688    0.9115       160\n",
      "           1     0.9808    0.9944    0.9876      1077\n",
      "\n",
      "    accuracy                         0.9782      1237\n",
      "   macro avg     0.9697    0.9316    0.9495      1237\n",
      "weighted avg     0.9779    0.9782    0.9777      1237\n",
      "\n",
      "{'model': 'bert-large-uncased', 'max_length': 512, 'train_time_sec': 9865.265, 'pred_time_sec': 334.9819375, 'accuracy': 0.9781729991915926, 'f1': 0.9777117433869209, 'precision': 0.9779044235111473, 'recall': 0.9781729991915926}\n"
     ]
    }
   ],
   "source": [
    "for model_name in MODEL_CONFIGS.keys():\n",
    "    for max_len in CONFIG[\"max_length\"]:\n",
    "        # skip if this (model, max_len) already exists in CSV\n",
    "        if os.path.exists(\"model_performance.csv\") and f\"{model_name},{max_len},\" in open(\"model_performance.csv\").read():\n",
    "            print(f\"Skipping {model_name} with max_length={max_len} (already in CSV).\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Training {model_name} with max_length={max_len} ---\")\n",
    "        performance = train_and_evaluate(model_name, train_dataset, test_dataset, data, max_len)\n",
    "        print(performance)\n",
    "        # save performance to a csv file\n",
    "        with open(\"model_performance.csv\", \"a\") as f:\n",
    "            #headers if file is empty\n",
    "            if os.stat(\"model_performance.csv\").st_size == 0:\n",
    "                f.write(\"model,max_length,train_time_sec,pred_time_sec,accuracy,f1,precision,recall\\n\")\n",
    "            f.write(\",\".join([str(performance[k]) for k in performance]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f468f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba7b36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-forge-Copy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
