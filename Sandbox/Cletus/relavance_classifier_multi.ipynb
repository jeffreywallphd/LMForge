{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a903082",
   "metadata": {},
   "source": [
    "#### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f725fd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.29.2\n",
      "<class 'transformers.training_args.TrainingArguments'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "import logging\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    # tokenizers\n",
    "    AutoTokenizer,\n",
    "    DebertaV2Tokenizer,\n",
    "    DistilBertTokenizer,\n",
    "    BertTokenizer,\n",
    "    RobertaTokenizer,\n",
    "    ElectraTokenizer,\n",
    "    AlbertTokenizer,\n",
    "    XLNetTokenizer,\n",
    "    MobileBertTokenizer,\n",
    "    # models\n",
    "    DebertaV2ForSequenceClassification,\n",
    "    DistilBertForSequenceClassification,\n",
    "    BertForSequenceClassification,\n",
    "    RobertaForSequenceClassification,\n",
    "    ElectraForSequenceClassification,\n",
    "    AlbertForSequenceClassification,\n",
    "    XLNetForSequenceClassification,\n",
    "    MobileBertForSequenceClassification,\n",
    ")\n",
    "from torch.nn import CrossEntropyLoss\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "print(transformers.TrainingArguments)\n",
    "\n",
    "# Cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eadf70b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='classification.log', level=logging.INFO)\n",
    "logging.info(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3f82b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: D:/huggingface_cache\n",
      "TRANSFORMERS_CACHE: D:/huggingface_cache\n",
      "HUGGINGFACE_HUB_CACHE: D:/huggingface_cache\n"
     ]
    }
   ],
   "source": [
    "# setting huggingface token\n",
    "login(token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"D:/huggingface_cache\" \n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"D:/huggingface_cache\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"D:/huggingface_cache\"\n",
    "\n",
    "print(\"HF_HOME:\", os.getenv(\"HF_HOME\"))\n",
    "print(\"TRANSFORMERS_CACHE:\", os.getenv(\"TRANSFORMERS_CACHE\"))\n",
    "print(\"HUGGINGFACE_HUB_CACHE:\", os.getenv(\"HUGGINGFACE_HUB_CACHE\"))\n",
    "\n",
    "logging.info(f\"HF_HOME: {os.getenv('HF_HOME')}\")\n",
    "logging.info(f\"TRANSFORMERS_CACHE: {os.getenv('TRANSFORMERS_CACHE')}\")\n",
    "logging.info(f\"HUGGINGFACE_HUB_CACHE: {os.getenv('HUGGINGFACE_HUB_CACHE')}\")\n",
    "\n",
    "transformers.utils.hub.TRANSFORMERS_CACHE = \"D:/huggingface_cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e30cc",
   "metadata": {},
   "source": [
    "### LOADING SQLITE DB WITH RECORDS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04c002d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to JSONL file.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "DB_FILE = \"chunks.db\"\n",
    "OUTPUT_FILE = \"exported_chunks.jsonl\"\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Query all data from chunks table\n",
    "cur.execute(\"SELECT text, label FROM chunks\")\n",
    "rows = cur.fetchall()\n",
    "\n",
    "# Write to JSONL\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for text, label in rows:\n",
    "        obj = {\"text\": text}\n",
    "        if label is not None:\n",
    "            obj[\"label\"] = label\n",
    "        f.write(json.dumps(obj) + \"\\n\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"Data exported to JSONL file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47d48089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled chunks after removing label 11: {1: 8199, 0: 800}\n",
      "Final labeled chunks: {1: 5384, 0: 800}\n"
     ]
    }
   ],
   "source": [
    "# Load the labeled chunks\n",
    "with open(\"exported_chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    labeled_chunks = [json.loads(line) for line in f]\n",
    "\n",
    "data = pd.DataFrame(labeled_chunks)\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "\n",
    "# Get the first 9000 rows\n",
    "data = data.head(9000)\n",
    "\n",
    "# Remove rows with label == 11\n",
    "data = data[data['label'] != 11]\n",
    "\n",
    "# Print labeled count after removing label 11\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "print(f\"Labeled chunks after removing label 11: {labeled_count}\")\n",
    "\n",
    "# Remove rows where label == 1 and text length < 100\n",
    "data = data[~((data['label'] == 1) & (data['text'].str.len() < 100))]\n",
    "\n",
    "# Print final labeled count\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "print(f\"Final labeled chunks: {labeled_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ca564",
   "metadata": {},
   "source": [
    "##### Spliting data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5f9aa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([3.8648, 0.5743], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "data['label'] = data['label'].astype(int)\n",
    "\n",
    "# Train-Test Split using stratified sampling\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, stratify=data['label'], random_state=42)\n",
    "\n",
    "# since there is a class imbalance, we will compute class weights\n",
    "# to handle this in the loss function\n",
    "labels = train_df[\"label\"].values\n",
    "# Compute class weights\n",
    "classes = np.unique(labels)\n",
    "weights = compute_class_weight(class_weight=\"balanced\",\n",
    "                            classes=classes,\n",
    "                            y=labels)\n",
    "class_weights = torch.tensor(weights, dtype=torch.float, device=device)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# Convert ing the DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a629272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Tuning parameters ----\n",
    "CONFIG = {\n",
    "    \"epochs\": 2,\n",
    "    \"batch_size\": 8,\n",
    "    \"max_length\": [128, 256, 512], # Max length of input sequences\n",
    "    \"learning_rate\": 3e-5, # Learning rate for larger models\n",
    "    \"weight_decay\": 0.01, # Weight decay for regularization\n",
    "    \"output_dir\": \"D:/huggingface_cache/classification_models\"\n",
    "}\n",
    "\n",
    "# ---- Model configurations ----\n",
    "MODEL_CONFIGS = {\n",
    "    \"deberta\": {\n",
    "        \"tokenizer_class\": DebertaV2Tokenizer,\n",
    "        \"pretrained_model_name\": \"microsoft/deberta-v3-small\", # params 55M\n",
    "        \"model_class\": DebertaV2ForSequenceClassification\n",
    "    },\n",
    "    \"distilbert\": {\n",
    "        \"tokenizer_class\": DistilBertTokenizer,\n",
    "        \"pretrained_model_name\": \"distilbert-base-uncased\", # params 66M\n",
    "        \"model_class\": DistilBertForSequenceClassification\n",
    "    },\n",
    "    \"bert\": {\n",
    "        \"tokenizer_class\": BertTokenizer,\n",
    "        \"pretrained_model_name\": \"bert-base-uncased\", # params 110M\n",
    "        \"model_class\": BertForSequenceClassification\n",
    "    },\n",
    "    \"roberta\": {\n",
    "        \"tokenizer_class\": RobertaTokenizer,\n",
    "        \"pretrained_model_name\": \"roberta-base\", # params 125M\n",
    "        \"model_class\": RobertaForSequenceClassification\n",
    "    },\n",
    "    \"electra\": {\n",
    "        \"tokenizer_class\": ElectraTokenizer,\n",
    "        \"pretrained_model_name\": \"google/electra-small-discriminator\", # params 14M\n",
    "        \"model_class\": ElectraForSequenceClassification\n",
    "    },\n",
    "    \"albert\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-base-v2\", # params 11M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"xlnet\": {\n",
    "        \"tokenizer_class\": XLNetTokenizer,\n",
    "        \"pretrained_model_name\": \"xlnet-base-cased\", # params 110M\n",
    "        \"model_class\": XLNetForSequenceClassification\n",
    "    },\n",
    "    \"mobilebert\": {\n",
    "        \"tokenizer_class\": AutoTokenizer,\n",
    "        \"pretrained_model_name\": \"google/mobilebert-uncased\", # params 25M\n",
    "        \"model_class\": MobileBertForSequenceClassification\n",
    "    },\n",
    "    \"albert-base-v1\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-base-v1\", # params 12M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"albert-large-v2\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-large-v2\", # params 18M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"albert-xlarge-v2\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-xlarge-v2\", # params 60M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"albert-xxlarge-v2\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-xxlarge-v2\", # params 235M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"bert-large-uncased\": {\n",
    "        \"tokenizer_class\": BertTokenizer,\n",
    "        \"pretrained_model_name\": \"bert-large-uncased\", # params 340M\n",
    "        \"model_class\": BertForSequenceClassification\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a3199a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Metric function ----\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba905e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Weighted Trainer ----\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc761c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Function to train and evaluate ----\n",
    "def train_and_evaluate(model_name, train_dataset, test_dataset, data, max_length):\n",
    "    print(f\"\\n===== Training {model_name} =====\")\n",
    "\n",
    "    # Model + tokenizer\n",
    "    cfg = MODEL_CONFIGS[model_name]\n",
    "    tokenizer = cfg[\"tokenizer_class\"].from_pretrained(cfg[\"pretrained_model_name\"])\n",
    "    model = cfg[\"model_class\"].from_pretrained(\n",
    "        cfg[\"pretrained_model_name\"],\n",
    "        num_labels=len(data['label'].unique())\n",
    "    )\n",
    "\n",
    "    # Tokenization\n",
    "    def tokenize_fn(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "    train_enc = train_dataset.map(tokenize_fn, batched=True)\n",
    "    test_enc = test_dataset.map(tokenize_fn, batched=True)\n",
    "    train_enc = train_enc.rename_column(\"label\", \"labels\")\n",
    "    test_enc = test_enc.rename_column(\"label\", \"labels\")\n",
    "    train_enc.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    test_enc.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    # Training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{CONFIG['output_dir']}/{model_name}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "        per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "        num_train_epochs=CONFIG[\"epochs\"],\n",
    "        weight_decay=CONFIG[\"weight_decay\"],\n",
    "        logging_dir=f\"./logs_{model_name}\",\n",
    "        report_to=\"none\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = WeightedLossTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_enc,\n",
    "        eval_dataset=test_enc,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "\n",
    "    # Train + evaluate\n",
    "    # time to train\n",
    "    t_train_start = torch.cuda.Event(enable_timing=True)\n",
    "    t_train_end = torch.cuda.Event(enable_timing=True)\n",
    "    t_train_start.record()\n",
    "    trainer.train()\n",
    "    t_train_end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    t_train_time = t_train_start.elapsed_time(t_train_end) / 1000  # convert to seconds\n",
    "    print(f\"Training time for {model_name}: {t_train_time:.2f} seconds\")\n",
    "\n",
    "    # Predictions\n",
    "    t_pred_start = torch.cuda.Event(enable_timing=True)\n",
    "    t_pred_end = torch.cuda.Event(enable_timing=True)\n",
    "    t_pred_start.record()\n",
    "    preds = trainer.predict(test_enc)\n",
    "    t_pred_end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    t_pred_time = t_pred_start.elapsed_time(t_pred_end) / 1000  # convert to seconds\n",
    "    print(f\"Prediction time for {model_name}: {t_pred_time:.2f} seconds\")\n",
    "    y_true = preds.label_ids\n",
    "    y_pred = np.argmax(preds.predictions, axis=1)\n",
    "\n",
    "    print(f\"\\nClassification Report for {model_name}:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "    performance = {\n",
    "        \"model\": model_name,\n",
    "        \"max_length\": max_length,\n",
    "        \"train_time_sec\": t_train_time,\n",
    "        \"pred_time_sec\": t_pred_time,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1\": precision_recall_fscore_support(y_true, y_pred, average='weighted')[2],\n",
    "        \"precision\": precision_recall_fscore_support(y_true, y_pred, average='weighted')[0],\n",
    "        \"recall\": precision_recall_fscore_support(y_true, y_pred, average='weighted')[1],\n",
    "    }\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95403fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping deberta with max_length=128 (already in CSV).\n",
      "Skipping deberta with max_length=256 (already in CSV).\n",
      "Skipping deberta with max_length=512 (already in CSV).\n",
      "Skipping distilbert with max_length=128 (already in CSV).\n",
      "Skipping distilbert with max_length=256 (already in CSV).\n",
      "Skipping distilbert with max_length=512 (already in CSV).\n",
      "Skipping bert with max_length=128 (already in CSV).\n",
      "Skipping bert with max_length=256 (already in CSV).\n",
      "Skipping bert with max_length=512 (already in CSV).\n",
      "Skipping roberta with max_length=128 (already in CSV).\n",
      "Skipping roberta with max_length=256 (already in CSV).\n",
      "Skipping roberta with max_length=512 (already in CSV).\n",
      "Skipping electra with max_length=128 (already in CSV).\n",
      "Skipping electra with max_length=256 (already in CSV).\n",
      "Skipping electra with max_length=512 (already in CSV).\n",
      "Skipping albert with max_length=128 (already in CSV).\n",
      "Skipping albert with max_length=256 (already in CSV).\n",
      "Skipping albert with max_length=512 (already in CSV).\n",
      "Skipping xlnet with max_length=128 (already in CSV).\n",
      "Skipping xlnet with max_length=256 (already in CSV).\n",
      "Skipping xlnet with max_length=512 (already in CSV).\n",
      "Skipping mobilebert with max_length=128 (already in CSV).\n",
      "Skipping mobilebert with max_length=256 (already in CSV).\n",
      "Skipping mobilebert with max_length=512 (already in CSV).\n",
      "Skipping albert-base-v1 with max_length=128 (already in CSV).\n",
      "Skipping albert-base-v1 with max_length=256 (already in CSV).\n",
      "Skipping albert-base-v1 with max_length=512 (already in CSV).\n",
      "Skipping albert-large-v2 with max_length=128 (already in CSV).\n",
      "Skipping albert-large-v2 with max_length=256 (already in CSV).\n",
      "Skipping albert-large-v2 with max_length=512 (already in CSV).\n",
      "Skipping albert-xlarge-v2 with max_length=128 (already in CSV).\n",
      "Skipping albert-xlarge-v2 with max_length=256 (already in CSV).\n",
      "Skipping albert-xlarge-v2 with max_length=512 (already in CSV).\n",
      "Skipping albert-xxlarge-v2 with max_length=128 (already in CSV).\n",
      "Skipping albert-xxlarge-v2 with max_length=256 (already in CSV).\n",
      "Skipping albert-xxlarge-v2 with max_length=512 (already in CSV).\n",
      "Skipping bert-large-uncased with max_length=128 (already in CSV).\n",
      "Skipping bert-large-uncased with max_length=256 (already in CSV).\n",
      "Skipping bert-large-uncased with max_length=512 (already in CSV).\n"
     ]
    }
   ],
   "source": [
    "for model_name in MODEL_CONFIGS.keys():\n",
    "    for max_len in CONFIG[\"max_length\"]:\n",
    "        # skip if this (model, max_len) already exists in CSV\n",
    "        if os.path.exists(\"model_performance.csv\") and f\"{model_name},{max_len},\" in open(\"model_performance.csv\").read():\n",
    "            print(f\"Skipping {model_name} with max_length={max_len} (already in CSV).\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Training {model_name} with max_length={max_len} ---\")\n",
    "        performance = train_and_evaluate(model_name, train_dataset, test_dataset, data, max_len)\n",
    "        print(performance)\n",
    "        # save performance to a csv file\n",
    "        with open(\"model_performance.csv\", \"a\") as f:\n",
    "            #headers if file is empty\n",
    "            if os.stat(\"model_performance.csv\").st_size == 0:\n",
    "                f.write(\"model,max_length,train_time_sec,pred_time_sec,accuracy,f1,precision,recall\\n\")\n",
    "            f.write(\",\".join([str(performance[k]) for k in performance]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f468f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba7b36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-forge-Copy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
