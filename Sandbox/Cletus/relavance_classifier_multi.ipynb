{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a903082",
   "metadata": {},
   "source": [
    "#### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f725fd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.29.2\n",
      "<class 'transformers.training_args.TrainingArguments'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "import logging\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    # tokenizers\n",
    "    AutoTokenizer,\n",
    "    DebertaV2Tokenizer,\n",
    "    DistilBertTokenizer,\n",
    "    BertTokenizer,\n",
    "    RobertaTokenizer,\n",
    "    ElectraTokenizer,\n",
    "    AlbertTokenizer,\n",
    "    XLNetTokenizer,\n",
    "    MobileBertTokenizer,\n",
    "    # models\n",
    "    DebertaV2ForSequenceClassification,\n",
    "    DistilBertForSequenceClassification,\n",
    "    BertForSequenceClassification,\n",
    "    RobertaForSequenceClassification,\n",
    "    ElectraForSequenceClassification,\n",
    "    AlbertForSequenceClassification,\n",
    "    XLNetForSequenceClassification,\n",
    "    MobileBertForSequenceClassification,\n",
    ")\n",
    "from torch.nn import CrossEntropyLoss\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "print(transformers.TrainingArguments)\n",
    "\n",
    "# Cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eadf70b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='classification.log', level=logging.INFO)\n",
    "logging.info(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3f82b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a855172174642deae4c620a77369b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: D:/huggingface_cache\n",
      "TRANSFORMERS_CACHE: D:/huggingface_cache\n",
      "HUGGINGFACE_HUB_CACHE: D:/huggingface_cache\n"
     ]
    }
   ],
   "source": [
    "# setting huggingface token\n",
    "login(token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"D:/huggingface_cache\" \n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"D:/huggingface_cache\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"D:/huggingface_cache\"\n",
    "\n",
    "print(\"HF_HOME:\", os.getenv(\"HF_HOME\"))\n",
    "print(\"TRANSFORMERS_CACHE:\", os.getenv(\"TRANSFORMERS_CACHE\"))\n",
    "print(\"HUGGINGFACE_HUB_CACHE:\", os.getenv(\"HUGGINGFACE_HUB_CACHE\"))\n",
    "\n",
    "logging.info(f\"HF_HOME: {os.getenv('HF_HOME')}\")\n",
    "logging.info(f\"TRANSFORMERS_CACHE: {os.getenv('TRANSFORMERS_CACHE')}\")\n",
    "logging.info(f\"HUGGINGFACE_HUB_CACHE: {os.getenv('HUGGINGFACE_HUB_CACHE')}\")\n",
    "\n",
    "transformers.utils.hub.TRANSFORMERS_CACHE = \"D:/huggingface_cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e30cc",
   "metadata": {},
   "source": [
    "### LOADING SQLITE DB WITH RECORDS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04c002d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to JSONL file.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "DB_FILE = \"chunks.db\"\n",
    "OUTPUT_FILE = \"exported_chunks.jsonl\"\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Query all data from chunks table\n",
    "cur.execute(\"SELECT text, label FROM chunks\")\n",
    "rows = cur.fetchall()\n",
    "\n",
    "# Write to JSONL\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for text, label in rows:\n",
    "        obj = {\"text\": text}\n",
    "        if label is not None:\n",
    "            obj[\"label\"] = label\n",
    "        f.write(json.dumps(obj) + \"\\n\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"Data exported to JSONL file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47d48089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled chunks after removing label 11: {1: 8199, 0: 800}\n",
      "Final labeled chunks: {1: 5384, 0: 800}\n"
     ]
    }
   ],
   "source": [
    "# Load the labeled chunks\n",
    "with open(\"exported_chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    labeled_chunks = [json.loads(line) for line in f]\n",
    "\n",
    "data = pd.DataFrame(labeled_chunks)\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "\n",
    "# Get the first 9000 rows\n",
    "data = data.head(9000)\n",
    "\n",
    "# Remove rows with label == 11\n",
    "data = data[data['label'] != 11]\n",
    "\n",
    "# Print labeled count after removing label 11\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "print(f\"Labeled chunks after removing label 11: {labeled_count}\")\n",
    "\n",
    "# Remove rows where label == 1 and text length < 100\n",
    "data = data[~((data['label'] == 1) & (data['text'].str.len() < 100))]\n",
    "\n",
    "# Print final labeled count\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "print(f\"Final labeled chunks: {labeled_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ca564",
   "metadata": {},
   "source": [
    "##### Spliting data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5f9aa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([3.8648, 0.5743], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "data['label'] = data['label'].astype(int)\n",
    "\n",
    "# Train-Test Split using stratified sampling\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, stratify=data['label'], random_state=42)\n",
    "\n",
    "# since there is a class imbalance, we will compute class weights\n",
    "# to handle this in the loss function\n",
    "labels = train_df[\"label\"].values\n",
    "# Compute class weights\n",
    "classes = np.unique(labels)\n",
    "weights = compute_class_weight(class_weight=\"balanced\",\n",
    "                            classes=classes,\n",
    "                            y=labels)\n",
    "class_weights = torch.tensor(weights, dtype=torch.float, device=device)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# Convert ing the DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a629272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Tuning parameters ----\n",
    "CONFIG = {\n",
    "    \"epochs\": 2,\n",
    "    \"batch_size\": 16,\n",
    "    \"max_length\": [128, 256, 512], # Max length of input sequences\n",
    "    \"learning_rate\": 5e-5, # Learning rate for the optimizer\n",
    "    \"weight_decay\": 0.01, # Weight decay for regularization\n",
    "    \"output_dir\": \"D:/huggingface_cache/classification_models\"\n",
    "}\n",
    "\n",
    "# ---- Model configurations ----\n",
    "MODEL_CONFIGS = {\n",
    "    \"deberta\": {\n",
    "        \"tokenizer_class\": DebertaV2Tokenizer,\n",
    "        \"pretrained_model_name\": \"microsoft/deberta-v3-small\", # params 55M\n",
    "        \"model_class\": DebertaV2ForSequenceClassification\n",
    "    },\n",
    "    \"distilbert\": {\n",
    "        \"tokenizer_class\": DistilBertTokenizer,\n",
    "        \"pretrained_model_name\": \"distilbert-base-uncased\", # params 66M\n",
    "        \"model_class\": DistilBertForSequenceClassification\n",
    "    },\n",
    "    \"bert\": {\n",
    "        \"tokenizer_class\": BertTokenizer,\n",
    "        \"pretrained_model_name\": \"bert-base-uncased\", # params 110M\n",
    "        \"model_class\": BertForSequenceClassification\n",
    "    },\n",
    "    \"roberta\": {\n",
    "        \"tokenizer_class\": RobertaTokenizer,\n",
    "        \"pretrained_model_name\": \"roberta-base\", # params 125M\n",
    "        \"model_class\": RobertaForSequenceClassification\n",
    "    },\n",
    "    \"electra\": {\n",
    "        \"tokenizer_class\": ElectraTokenizer,\n",
    "        \"pretrained_model_name\": \"google/electra-small-discriminator\", # params 14M\n",
    "        \"model_class\": ElectraForSequenceClassification\n",
    "    },\n",
    "    \"albert\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-base-v2\", # params 11M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"xlnet\": {\n",
    "        \"tokenizer_class\": XLNetTokenizer,\n",
    "        \"pretrained_model_name\": \"xlnet-base-cased\", # params 110M\n",
    "        \"model_class\": XLNetForSequenceClassification\n",
    "    },\n",
    "    \"mobilebert\": {\n",
    "        \"tokenizer_class\": AutoTokenizer,\n",
    "        \"pretrained_model_name\": \"google/mobilebert-uncased\", # params 25M\n",
    "        \"model_class\": MobileBertForSequenceClassification\n",
    "    },\n",
    "    \"albert-base-v1\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-base-v1\", # params 12M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"albert-large-v2\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-large-v2\", # params 18M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"albert-xlarge-v2\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-xlarge-v2\", # params 60M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"albert-xxlarge-v2\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-xxlarge-v2\", # params 235M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"bert-large-uncased\": {\n",
    "        \"tokenizer_class\": BertTokenizer,\n",
    "        \"pretrained_model_name\": \"bert-large-uncased\", # params 340M\n",
    "        \"model_class\": BertForSequenceClassification\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a3199a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Metric function ----\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba905e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Weighted Trainer ----\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc761c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Function to train and evaluate ----\n",
    "def train_and_evaluate(model_name, train_dataset, test_dataset, data, max_length):\n",
    "    print(f\"\\n===== Training {model_name} =====\")\n",
    "\n",
    "    # Model + tokenizer\n",
    "    cfg = MODEL_CONFIGS[model_name]\n",
    "    tokenizer = cfg[\"tokenizer_class\"].from_pretrained(cfg[\"pretrained_model_name\"])\n",
    "    model = cfg[\"model_class\"].from_pretrained(\n",
    "        cfg[\"pretrained_model_name\"],\n",
    "        num_labels=len(data['label'].unique())\n",
    "    )\n",
    "\n",
    "    # Tokenization\n",
    "    def tokenize_fn(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "    train_enc = train_dataset.map(tokenize_fn, batched=True)\n",
    "    test_enc = test_dataset.map(tokenize_fn, batched=True)\n",
    "    train_enc = train_enc.rename_column(\"label\", \"labels\")\n",
    "    test_enc = test_enc.rename_column(\"label\", \"labels\")\n",
    "    train_enc.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    test_enc.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    # Training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{CONFIG['output_dir']}/{model_name}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "        per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "        num_train_epochs=CONFIG[\"epochs\"],\n",
    "        weight_decay=CONFIG[\"weight_decay\"],\n",
    "        logging_dir=f\"./logs_{model_name}\",\n",
    "        report_to=\"none\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = WeightedLossTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_enc,\n",
    "        eval_dataset=test_enc,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "\n",
    "    # Train + evaluate\n",
    "    # time to train\n",
    "    t_train_start = torch.cuda.Event(enable_timing=True)\n",
    "    t_train_end = torch.cuda.Event(enable_timing=True)\n",
    "    t_train_start.record()\n",
    "    trainer.train()\n",
    "    t_train_end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    t_train_time = t_train_start.elapsed_time(t_train_end) / 1000  # convert to seconds\n",
    "    print(f\"Training time for {model_name}: {t_train_time:.2f} seconds\")\n",
    "\n",
    "    # Predictions\n",
    "    t_pred_start = torch.cuda.Event(enable_timing=True)\n",
    "    t_pred_end = torch.cuda.Event(enable_timing=True)\n",
    "    t_pred_start.record()\n",
    "    preds = trainer.predict(test_enc)\n",
    "    t_pred_end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    t_pred_time = t_pred_start.elapsed_time(t_pred_end) / 1000  # convert to seconds\n",
    "    print(f\"Prediction time for {model_name}: {t_pred_time:.2f} seconds\")\n",
    "    y_true = preds.label_ids\n",
    "    y_pred = np.argmax(preds.predictions, axis=1)\n",
    "\n",
    "    print(f\"\\nClassification Report for {model_name}:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "    performance = {\n",
    "        \"model\": model_name,\n",
    "        \"max_length\": max_length,\n",
    "        \"train_time_sec\": t_train_time,\n",
    "        \"pred_time_sec\": t_pred_time,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1\": precision_recall_fscore_support(y_true, y_pred, average='weighted')[2],\n",
    "        \"precision\": precision_recall_fscore_support(y_true, y_pred, average='weighted')[0],\n",
    "        \"recall\": precision_recall_fscore_support(y_true, y_pred, average='weighted')[1],\n",
    "    }\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95403fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training deberta with max_length=128 ---\n",
      "\n",
      "===== Training deberta =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf93e37855343b4bc303d2f44acc4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328bc3af990249a79959879c6c68bf07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9caa03e014b40dab6670ef03f1adad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4daad690fb4d8bb637aab1af3357ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.380266010761261, 'eval_accuracy': 0.973322554567502, 'eval_f1': 0.9719901040015049, 'eval_precision': 0.9741156678100898, 'eval_recall': 0.973322554567502, 'eval_runtime': 1.3136, 'eval_samples_per_second': 941.674, 'eval_steps_per_second': 59.378, 'epoch': 1.0}\n",
      "{'loss': 0.2723, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1d13067485498eb3cec88a3c74dea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23579472303390503, 'eval_accuracy': 0.9765561843168957, 'eval_f1': 0.9763972689014709, 'eval_precision': 0.9763067594594299, 'eval_recall': 0.9765561843168957, 'eval_runtime': 1.3274, 'eval_samples_per_second': 931.909, 'eval_steps_per_second': 58.762, 'epoch': 2.0}\n",
      "{'train_runtime': 70.8153, 'train_samples_per_second': 139.716, 'train_steps_per_second': 8.755, 'train_loss': 0.24663084399315618, 'epoch': 2.0}\n",
      "Training time for deberta: 70.82 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f9588784484895a99c31230bb267bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for deberta: 1.41 seconds\n",
      "\n",
      "Classification Report for deberta:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9226    0.8938    0.9079       160\n",
      "           1     0.9843    0.9889    0.9866      1077\n",
      "\n",
      "    accuracy                         0.9766      1237\n",
      "   macro avg     0.9534    0.9413    0.9473      1237\n",
      "weighted avg     0.9763    0.9766    0.9764      1237\n",
      "\n",
      "{'model': 'deberta', 'max_length': 128, 'train_time_sec': 70.8225390625, 'pred_time_sec': 1.4123701171875, 'accuracy': 0.9765561843168957, 'f1': 0.9763972689014709, 'precision': 0.9763067594594299, 'recall': 0.9765561843168957}\n",
      "\n",
      "--- Training deberta with max_length=256 ---\n",
      "\n",
      "===== Training deberta =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7505969c2ec4597bc6ed3c171fb1751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bbac8989c34b7787ad9b545dd3237d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fddcddf35f9341df8956340aa73667e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30580118976c4bfd97c957b7ca0f6e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3137158155441284, 'eval_accuracy': 0.9717057396928052, 'eval_f1': 0.9713552018317003, 'eval_precision': 0.9712267469724357, 'eval_recall': 0.9717057396928052, 'eval_runtime': 2.9978, 'eval_samples_per_second': 412.638, 'eval_steps_per_second': 26.019, 'epoch': 1.0}\n",
      "{'loss': 0.2672, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04008f1418e7446daae6d524fda092c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2729227542877197, 'eval_accuracy': 0.973322554567502, 'eval_f1': 0.9729154851070881, 'eval_precision': 0.9728458738348225, 'eval_recall': 0.973322554567502, 'eval_runtime': 3.0165, 'eval_samples_per_second': 410.077, 'eval_steps_per_second': 25.858, 'epoch': 2.0}\n",
      "{'train_runtime': 109.709, 'train_samples_per_second': 90.184, 'train_steps_per_second': 5.651, 'train_loss': 0.2453546400993101, 'epoch': 2.0}\n",
      "Training time for deberta: 109.72 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4111957bc9040118342a18ed3133d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for deberta: 2.99 seconds\n",
      "\n",
      "Classification Report for deberta:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9262    0.8625    0.8932       160\n",
      "           1     0.9798    0.9898    0.9848      1077\n",
      "\n",
      "    accuracy                         0.9733      1237\n",
      "   macro avg     0.9530    0.9261    0.9390      1237\n",
      "weighted avg     0.9728    0.9733    0.9729      1237\n",
      "\n",
      "{'model': 'deberta', 'max_length': 256, 'train_time_sec': 109.7162265625, 'pred_time_sec': 2.991547119140625, 'accuracy': 0.973322554567502, 'f1': 0.9729154851070881, 'precision': 0.9728458738348225, 'recall': 0.973322554567502}\n",
      "\n",
      "--- Training deberta with max_length=512 ---\n",
      "\n",
      "===== Training deberta =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfae79a8af624656b21fccd3fcfe9e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d1c90c5b6347168e8e272f6d11a3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb3fc57f2a24980924bd5fd77a87ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a25a24dbf14fc0a383c6f027a6be5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.285459041595459, 'eval_accuracy': 0.9708973322554567, 'eval_f1': 0.9701506253747769, 'eval_precision': 0.9703338646289537, 'eval_recall': 0.9708973322554567, 'eval_runtime': 8.9638, 'eval_samples_per_second': 137.999, 'eval_steps_per_second': 8.702, 'epoch': 1.0}\n",
      "{'loss': 0.294, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a1df2e4823402ead990fc0289b0036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25055503845214844, 'eval_accuracy': 0.973322554567502, 'eval_f1': 0.9729920474413175, 'eval_precision': 0.9728850455627258, 'eval_recall': 0.973322554567502, 'eval_runtime': 8.968, 'eval_samples_per_second': 137.934, 'eval_steps_per_second': 8.698, 'epoch': 2.0}\n",
      "{'train_runtime': 241.1898, 'train_samples_per_second': 41.022, 'train_steps_per_second': 2.571, 'train_loss': 0.2680034452869046, 'epoch': 2.0}\n",
      "Training time for deberta: 241.20 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab41cafc3d54c719155eab18a6a601c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for deberta: 8.90 seconds\n",
      "\n",
      "Classification Report for deberta:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9205    0.8688    0.8939       160\n",
      "           1     0.9807    0.9889    0.9847      1077\n",
      "\n",
      "    accuracy                         0.9733      1237\n",
      "   macro avg     0.9506    0.9288    0.9393      1237\n",
      "weighted avg     0.9729    0.9733    0.9730      1237\n",
      "\n",
      "{'model': 'deberta', 'max_length': 512, 'train_time_sec': 241.19775, 'pred_time_sec': 8.9031904296875, 'accuracy': 0.973322554567502, 'f1': 0.9729920474413175, 'precision': 0.9728850455627258, 'recall': 0.973322554567502}\n",
      "\n",
      "--- Training distilbert with max_length=128 ---\n",
      "\n",
      "===== Training distilbert =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592995cae8fd4b13a794e69c24179f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5dac214e18467fbc0a153a35df9496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e19f8f32ae42cfb34940ffdce19d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc474ab7b6494dd4ad01594114af1582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27784067392349243, 'eval_accuracy': 0.978981406628941, 'eval_f1': 0.978505773652584, 'eval_precision': 0.9787808045597567, 'eval_recall': 0.978981406628941, 'eval_runtime': 0.9318, 'eval_samples_per_second': 1327.522, 'eval_steps_per_second': 83.708, 'epoch': 1.0}\n",
      "{'loss': 0.2407, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60724bf8ea44565b25227231389f0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2534365952014923, 'eval_accuracy': 0.978981406628941, 'eval_f1': 0.9786909612760256, 'eval_precision': 0.9786841229126367, 'eval_recall': 0.978981406628941, 'eval_runtime': 0.9423, 'eval_samples_per_second': 1312.69, 'eval_steps_per_second': 82.773, 'epoch': 2.0}\n",
      "{'train_runtime': 34.9018, 'train_samples_per_second': 283.481, 'train_steps_per_second': 17.764, 'train_loss': 0.21823013213373, 'epoch': 2.0}\n",
      "Training time for distilbert: 34.91 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644da5e282ab4706a341674e0564e287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for distilbert: 1.09 seconds\n",
      "\n",
      "Classification Report for distilbert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9467    0.8875    0.9161       160\n",
      "           1     0.9834    0.9926    0.9880      1077\n",
      "\n",
      "    accuracy                         0.9790      1237\n",
      "   macro avg     0.9651    0.9400    0.9521      1237\n",
      "weighted avg     0.9787    0.9790    0.9787      1237\n",
      "\n",
      "{'model': 'distilbert', 'max_length': 128, 'train_time_sec': 34.908421875, 'pred_time_sec': 1.0913233642578124, 'accuracy': 0.978981406628941, 'f1': 0.9786909612760256, 'precision': 0.9786841229126367, 'recall': 0.978981406628941}\n",
      "\n",
      "--- Training distilbert with max_length=256 ---\n",
      "\n",
      "===== Training distilbert =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb088d2ece14d198c589f76a1a429b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31c668c4eb74ac091c4faa8888e9bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f44438b80054eb4ae9cce987e90e1b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819800138cc145f3bd41c2018af7f369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2508147656917572, 'eval_accuracy': 0.973322554567502, 'eval_f1': 0.9729154851070881, 'eval_precision': 0.9728458738348225, 'eval_recall': 0.973322554567502, 'eval_runtime': 1.8243, 'eval_samples_per_second': 678.065, 'eval_steps_per_second': 42.756, 'epoch': 1.0}\n",
      "{'loss': 0.241, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ddee9b23e0449b2b650d4d094234de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23054678738117218, 'eval_accuracy': 0.9765561843168957, 'eval_f1': 0.9762657386605518, 'eval_precision': 0.9762016427433062, 'eval_recall': 0.9765561843168957, 'eval_runtime': 1.8402, 'eval_samples_per_second': 672.206, 'eval_steps_per_second': 42.386, 'epoch': 2.0}\n",
      "{'train_runtime': 55.4104, 'train_samples_per_second': 178.559, 'train_steps_per_second': 11.189, 'train_loss': 0.21550348343387726, 'epoch': 2.0}\n",
      "Training time for distilbert: 55.42 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da94ba436434e4bbc20ecd23c3294f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for distilbert: 2.01 seconds\n",
      "\n",
      "Classification Report for distilbert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9338    0.8812    0.9068       160\n",
      "           1     0.9825    0.9907    0.9866      1077\n",
      "\n",
      "    accuracy                         0.9766      1237\n",
      "   macro avg     0.9581    0.9360    0.9467      1237\n",
      "weighted avg     0.9762    0.9766    0.9763      1237\n",
      "\n",
      "{'model': 'distilbert', 'max_length': 256, 'train_time_sec': 55.41671484375, 'pred_time_sec': 2.009479248046875, 'accuracy': 0.9765561843168957, 'f1': 0.9762657386605518, 'precision': 0.9762016427433062, 'recall': 0.9765561843168957}\n",
      "\n",
      "--- Training distilbert with max_length=512 ---\n",
      "\n",
      "===== Training distilbert =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22abddc0be745fbb2ea1b8f151b7005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd28502ca284e46a7eef06b39b50175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734962ac8c1a4ece88e0623e9b29c103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9bf04752bd440fa08374ba600495af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26923197507858276, 'eval_accuracy': 0.9741309620048505, 'eval_f1': 0.9734672225553573, 'eval_precision': 0.9737458649639834, 'eval_recall': 0.9741309620048505, 'eval_runtime': 4.7885, 'eval_samples_per_second': 258.325, 'eval_steps_per_second': 16.289, 'epoch': 1.0}\n",
      "{'loss': 0.2293, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7d73a61e8c4b8b88d41f91063f92d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22618120908737183, 'eval_accuracy': 0.9757477768795473, 'eval_f1': 0.9756167583702973, 'eval_precision': 0.9755291603755218, 'eval_recall': 0.9757477768795473, 'eval_runtime': 4.6798, 'eval_samples_per_second': 264.329, 'eval_steps_per_second': 16.668, 'epoch': 2.0}\n",
      "{'train_runtime': 122.7312, 'train_samples_per_second': 80.615, 'train_steps_per_second': 5.052, 'train_loss': 0.2063112074329007, 'epoch': 2.0}\n",
      "Training time for distilbert: 122.74 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a45cb7b6ea44511aa71875a687d2ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for distilbert: 4.66 seconds\n",
      "\n",
      "Classification Report for distilbert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9167    0.8938    0.9051       160\n",
      "           1     0.9843    0.9879    0.9861      1077\n",
      "\n",
      "    accuracy                         0.9757      1237\n",
      "   macro avg     0.9505    0.9408    0.9456      1237\n",
      "weighted avg     0.9755    0.9757    0.9756      1237\n",
      "\n",
      "{'model': 'distilbert', 'max_length': 512, 'train_time_sec': 122.737953125, 'pred_time_sec': 4.6627470703125, 'accuracy': 0.9757477768795473, 'f1': 0.9756167583702973, 'precision': 0.9755291603755218, 'recall': 0.9757477768795473}\n",
      "\n",
      "--- Training bert with max_length=128 ---\n",
      "\n",
      "===== Training bert =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9937895464a249a081b718ab527731e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91dc21544d0a458c9602aac5692e05ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a1003a73984e42939a1a3f8c3332ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed333f4e2a714b8b936777b356d44476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2551727294921875, 'eval_accuracy': 0.9765561843168957, 'eval_f1': 0.9759903078361688, 'eval_precision': 0.9762684861023856, 'eval_recall': 0.9765561843168957, 'eval_runtime': 1.7192, 'eval_samples_per_second': 719.516, 'eval_steps_per_second': 45.37, 'epoch': 1.0}\n",
      "{'loss': 0.2524, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795c49c7df3047e19f7f5767d575d45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15620189905166626, 'eval_accuracy': 0.9797898140662894, 'eval_f1': 0.9797628257971375, 'eval_precision': 0.9797385510679781, 'eval_recall': 0.9797898140662894, 'eval_runtime': 1.7177, 'eval_samples_per_second': 720.155, 'eval_steps_per_second': 45.41, 'epoch': 2.0}\n",
      "{'train_runtime': 60.2796, 'train_samples_per_second': 164.135, 'train_steps_per_second': 10.285, 'train_loss': 0.22457566876565258, 'epoch': 2.0}\n",
      "Training time for bert: 60.29 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69412b5f3412430bbd579410b5e21674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for bert: 1.89 seconds\n",
      "\n",
      "Classification Report for bert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9245    0.9187    0.9216       160\n",
      "           1     0.9879    0.9889    0.9884      1077\n",
      "\n",
      "    accuracy                         0.9798      1237\n",
      "   macro avg     0.9562    0.9538    0.9550      1237\n",
      "weighted avg     0.9797    0.9798    0.9798      1237\n",
      "\n",
      "{'model': 'bert', 'max_length': 128, 'train_time_sec': 60.2867421875, 'pred_time_sec': 1.89179541015625, 'accuracy': 0.9797898140662894, 'f1': 0.9797628257971375, 'precision': 0.9797385510679781, 'recall': 0.9797898140662894}\n",
      "\n",
      "--- Training bert with max_length=256 ---\n",
      "\n",
      "===== Training bert =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94012dbfff3e4de292a1b138b7e42ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af7d18860ea4c708402cee4bc182d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a684f4e945442538b1e961e2ad8e3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0955b0b914cc4b5b94e4c3ca7ee8095f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20972925424575806, 'eval_accuracy': 0.9822150363783346, 'eval_f1': 0.9818656145808211, 'eval_precision': 0.9820916888502574, 'eval_recall': 0.9822150363783346, 'eval_runtime': 3.4909, 'eval_samples_per_second': 354.348, 'eval_steps_per_second': 22.344, 'epoch': 1.0}\n",
      "{'loss': 0.2578, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea1dd6736fe40bd844939973a7b7f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16365188360214233, 'eval_accuracy': 0.9830234438156831, 'eval_f1': 0.9827643996136015, 'eval_precision': 0.9828558101771253, 'eval_recall': 0.9830234438156831, 'eval_runtime': 3.5279, 'eval_samples_per_second': 350.636, 'eval_steps_per_second': 22.11, 'epoch': 2.0}\n",
      "{'train_runtime': 104.0068, 'train_samples_per_second': 95.128, 'train_steps_per_second': 5.961, 'train_loss': 0.2365403575281943, 'epoch': 2.0}\n",
      "Training time for bert: 104.01 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a68bd6d16954126b886024f23a7d40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for bert: 3.71 seconds\n",
      "\n",
      "Classification Report for bert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9664    0.9000    0.9320       160\n",
      "           1     0.9853    0.9954    0.9903      1077\n",
      "\n",
      "    accuracy                         0.9830      1237\n",
      "   macro avg     0.9759    0.9477    0.9612      1237\n",
      "weighted avg     0.9829    0.9830    0.9828      1237\n",
      "\n",
      "{'model': 'bert', 'max_length': 256, 'train_time_sec': 104.014390625, 'pred_time_sec': 3.7053486328125, 'accuracy': 0.9830234438156831, 'f1': 0.9827643996136015, 'precision': 0.9828558101771253, 'recall': 0.9830234438156831}\n",
      "\n",
      "--- Training bert with max_length=512 ---\n",
      "\n",
      "===== Training bert =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd07d848a854f48a9605085ea075bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0a817673c2425c8674baba6b7d54bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753fffbbca9b401babe4db32f68d293a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8c382a22044ff082a79b6bd7244e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24555543065071106, 'eval_accuracy': 0.9773645917542442, 'eval_f1': 0.9768523716258597, 'eval_precision': 0.9770860005971218, 'eval_recall': 0.9773645917542442, 'eval_runtime': 8.9603, 'eval_samples_per_second': 138.053, 'eval_steps_per_second': 8.705, 'epoch': 1.0}\n",
      "{'loss': 0.2526, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61335a5086394ecfa686d87a9bd439d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2109844982624054, 'eval_accuracy': 0.9757477768795473, 'eval_f1': 0.9752712926102107, 'eval_precision': 0.975355850056463, 'eval_recall': 0.9757477768795473, 'eval_runtime': 8.9293, 'eval_samples_per_second': 138.533, 'eval_steps_per_second': 8.735, 'epoch': 2.0}\n",
      "{'train_runtime': 230.205, 'train_samples_per_second': 42.979, 'train_steps_per_second': 2.693, 'train_loss': 0.22169009485552388, 'epoch': 2.0}\n",
      "Training time for bert: 230.21 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d98c43311e14d978200e09865a6380b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for bert: 9.05 seconds\n",
      "\n",
      "Classification Report for bert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9583    0.8625    0.9079       160\n",
      "           1     0.9799    0.9944    0.9871      1077\n",
      "\n",
      "    accuracy                         0.9774      1237\n",
      "   macro avg     0.9691    0.9285    0.9475      1237\n",
      "weighted avg     0.9771    0.9774    0.9769      1237\n",
      "\n",
      "{'model': 'bert', 'max_length': 512, 'train_time_sec': 230.21215625, 'pred_time_sec': 9.0454580078125, 'accuracy': 0.9773645917542442, 'f1': 0.9768523716258597, 'precision': 0.9770860005971218, 'recall': 0.9773645917542442}\n",
      "\n",
      "--- Training roberta with max_length=128 ---\n",
      "\n",
      "===== Training roberta =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26bc73872824587a7e4d3ca097f66d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0645ce471046469db3acb2ba0b9390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72a441cef4a4a92b7abea7550e26e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee10394de25b46c79e97fc32b3c21c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42896267771720886, 'eval_accuracy': 0.9717057396928052, 'eval_f1': 0.9703889418941817, 'eval_precision': 0.9720635934062385, 'eval_recall': 0.9717057396928052, 'eval_runtime': 1.7101, 'eval_samples_per_second': 723.36, 'eval_steps_per_second': 45.612, 'epoch': 1.0}\n",
      "{'loss': 0.3148, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1964cb617643c4a60a96cabd52d50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21346116065979004, 'eval_accuracy': 0.9830234438156831, 'eval_f1': 0.9827643996136015, 'eval_precision': 0.9828558101771253, 'eval_recall': 0.9830234438156831, 'eval_runtime': 1.6672, 'eval_samples_per_second': 741.942, 'eval_steps_per_second': 46.784, 'epoch': 2.0}\n",
      "{'train_runtime': 70.3441, 'train_samples_per_second': 140.651, 'train_steps_per_second': 8.814, 'train_loss': 0.28170710225259105, 'epoch': 2.0}\n",
      "Training time for roberta: 70.35 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dea60731b0b484eb5d76b913349d838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for roberta: 1.80 seconds\n",
      "\n",
      "Classification Report for roberta:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9664    0.9000    0.9320       160\n",
      "           1     0.9853    0.9954    0.9903      1077\n",
      "\n",
      "    accuracy                         0.9830      1237\n",
      "   macro avg     0.9759    0.9477    0.9612      1237\n",
      "weighted avg     0.9829    0.9830    0.9828      1237\n",
      "\n",
      "{'model': 'roberta', 'max_length': 128, 'train_time_sec': 70.35253125, 'pred_time_sec': 1.798475830078125, 'accuracy': 0.9830234438156831, 'f1': 0.9827643996136015, 'precision': 0.9828558101771253, 'recall': 0.9830234438156831}\n",
      "\n",
      "--- Training roberta with max_length=256 ---\n",
      "\n",
      "===== Training roberta =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678e9bbc9961415db67be4258d81bb97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f00ff86bde746b4acb6c1bce80afeca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad811b693dd45ccad681bb5bc96b78a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f686b4ee95d8458d8109966d3fdc8792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43413907289505005, 'eval_accuracy': 0.9708973322554567, 'eval_f1': 0.9693935710230959, 'eval_precision': 0.9715450815205092, 'eval_recall': 0.9708973322554567, 'eval_runtime': 3.3577, 'eval_samples_per_second': 368.409, 'eval_steps_per_second': 23.23, 'epoch': 1.0}\n",
      "{'loss': 0.3597, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a405c071f777413fa5b57362ba46b4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25350719690322876, 'eval_accuracy': 0.9805982215036378, 'eval_f1': 0.9802170340881687, 'eval_precision': 0.9804077291518087, 'eval_recall': 0.9805982215036378, 'eval_runtime': 3.3625, 'eval_samples_per_second': 367.876, 'eval_steps_per_second': 23.197, 'epoch': 2.0}\n",
      "{'train_runtime': 111.6651, 'train_samples_per_second': 88.604, 'train_steps_per_second': 5.552, 'train_loss': 0.3209612815610824, 'epoch': 2.0}\n",
      "Training time for roberta: 111.67 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7343d9b0544ce5979fc44120eec1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for roberta: 3.42 seconds\n",
      "\n",
      "Classification Report for roberta:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9658    0.8812    0.9216       160\n",
      "           1     0.9826    0.9954    0.9889      1077\n",
      "\n",
      "    accuracy                         0.9806      1237\n",
      "   macro avg     0.9742    0.9383    0.9552      1237\n",
      "weighted avg     0.9804    0.9806    0.9802      1237\n",
      "\n",
      "{'model': 'roberta', 'max_length': 256, 'train_time_sec': 111.674484375, 'pred_time_sec': 3.4168330078125, 'accuracy': 0.9805982215036378, 'f1': 0.9802170340881687, 'precision': 0.9804077291518087, 'recall': 0.9805982215036378}\n",
      "\n",
      "--- Training roberta with max_length=512 ---\n",
      "\n",
      "===== Training roberta =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba9f6b4b1db4eaf96d34e4195ee341a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9deb965f5d4c55be3fd5c6157e0ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdae0ea5bf6e46efb77faac7aa19128c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674eadbc535641e79140298cd0c4076e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.306217759847641, 'eval_accuracy': 0.9773645917542442, 'eval_f1': 0.9765716530616949, 'eval_precision': 0.9775374424549051, 'eval_recall': 0.9773645917542442, 'eval_runtime': 8.5258, 'eval_samples_per_second': 145.089, 'eval_steps_per_second': 9.149, 'epoch': 1.0}\n",
      "{'loss': 0.3362, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b787aba375944189ec05b089df6c022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20447534322738647, 'eval_accuracy': 0.978981406628941, 'eval_f1': 0.9787508247974432, 'eval_precision': 0.9786998071233577, 'eval_recall': 0.978981406628941, 'eval_runtime': 8.7918, 'eval_samples_per_second': 140.699, 'eval_steps_per_second': 8.872, 'epoch': 2.0}\n",
      "{'train_runtime': 240.2127, 'train_samples_per_second': 41.189, 'train_steps_per_second': 2.581, 'train_loss': 0.3002931840958134, 'epoch': 2.0}\n",
      "Training time for roberta: 240.22 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb1bb4fd25944c5a4843e56a4e3b515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for roberta: 8.61 seconds\n",
      "\n",
      "Classification Report for roberta:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9408    0.8938    0.9167       160\n",
      "           1     0.9843    0.9916    0.9880      1077\n",
      "\n",
      "    accuracy                         0.9790      1237\n",
      "   macro avg     0.9626    0.9427    0.9523      1237\n",
      "weighted avg     0.9787    0.9790    0.9788      1237\n",
      "\n",
      "{'model': 'roberta', 'max_length': 512, 'train_time_sec': 240.22140625, 'pred_time_sec': 8.612994140625, 'accuracy': 0.978981406628941, 'f1': 0.9787508247974432, 'precision': 0.9786998071233577, 'recall': 0.978981406628941}\n",
      "\n",
      "--- Training electra with max_length=128 ---\n",
      "\n",
      "===== Training electra =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c2665146e74a469a202cb4a965867a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bc0955bbff4567873924743a5c8bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db8bdf1509844f2a10186d71f2f4631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83831f0d082341cb8af184026feea78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.295249879360199, 'eval_accuracy': 0.9749393694421988, 'eval_f1': 0.9738571024734121, 'eval_precision': 0.9753963172183064, 'eval_recall': 0.9749393694421988, 'eval_runtime': 0.8716, 'eval_samples_per_second': 1419.148, 'eval_steps_per_second': 89.485, 'epoch': 1.0}\n",
      "{'loss': 0.2905, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64225af6551b4d1f926691f568197de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2568901777267456, 'eval_accuracy': 0.9708973322554567, 'eval_f1': 0.9705780651041521, 'eval_precision': 0.9704327969897362, 'eval_recall': 0.9708973322554567, 'eval_runtime': 0.9521, 'eval_samples_per_second': 1299.299, 'eval_steps_per_second': 81.928, 'epoch': 2.0}\n",
      "{'train_runtime': 32.7042, 'train_samples_per_second': 302.53, 'train_steps_per_second': 18.958, 'train_loss': 0.26939038615072924, 'epoch': 2.0}\n",
      "Training time for electra: 32.71 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4492226d25934d70b02d3936f4442459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for electra: 0.94 seconds\n",
      "\n",
      "Classification Report for electra:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9924    0.8125    0.8935       160\n",
      "           1     0.9729    0.9991    0.9858      1077\n",
      "\n",
      "    accuracy                         0.9749      1237\n",
      "   macro avg     0.9826    0.9058    0.9396      1237\n",
      "weighted avg     0.9754    0.9749    0.9739      1237\n",
      "\n",
      "{'model': 'electra', 'max_length': 128, 'train_time_sec': 32.711224609375, 'pred_time_sec': 0.9381182861328125, 'accuracy': 0.9749393694421988, 'f1': 0.9738571024734121, 'precision': 0.9753963172183064, 'recall': 0.9749393694421988}\n",
      "\n",
      "--- Training electra with max_length=256 ---\n",
      "\n",
      "===== Training electra =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401fbfd828234768a37dd1278dbe4bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4fa105a5404efea622f23879ac2a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7facbec3a634a9eb955cba564dc921a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1450376c65184689892dcd94926f7469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3352561891078949, 'eval_accuracy': 0.9692805173807599, 'eval_f1': 0.9677987247099427, 'eval_precision': 0.9694906377088174, 'eval_recall': 0.9692805173807599, 'eval_runtime': 0.8202, 'eval_samples_per_second': 1508.238, 'eval_steps_per_second': 95.103, 'epoch': 1.0}\n",
      "{'loss': 0.2899, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdaebb8fe0045208c4da613f670cff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23326022922992706, 'eval_accuracy': 0.9741309620048505, 'eval_f1': 0.9739197348103219, 'eval_precision': 0.9738066044069531, 'eval_recall': 0.9741309620048505, 'eval_runtime': 0.936, 'eval_samples_per_second': 1321.623, 'eval_steps_per_second': 83.336, 'epoch': 2.0}\n",
      "{'train_runtime': 40.2549, 'train_samples_per_second': 245.784, 'train_steps_per_second': 15.402, 'train_loss': 0.2682643675035046, 'epoch': 2.0}\n",
      "Training time for electra: 40.26 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93782de51ab457eb3c677aa873c5cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for electra: 0.86 seconds\n",
      "\n",
      "Classification Report for electra:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9156    0.8812    0.8981       160\n",
      "           1     0.9825    0.9879    0.9852      1077\n",
      "\n",
      "    accuracy                         0.9741      1237\n",
      "   macro avg     0.9490    0.9346    0.9416      1237\n",
      "weighted avg     0.9738    0.9741    0.9739      1237\n",
      "\n",
      "{'model': 'electra', 'max_length': 256, 'train_time_sec': 40.26189453125, 'pred_time_sec': 0.864918701171875, 'accuracy': 0.9741309620048505, 'f1': 0.9739197348103219, 'precision': 0.9738066044069531, 'recall': 0.9741309620048505}\n",
      "\n",
      "--- Training electra with max_length=512 ---\n",
      "\n",
      "===== Training electra =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4ed126c4244186b084d02ddbda6276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f0389b63c8471886a61cc0fb70d114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6831ac57cfb4276904f9696a5b468b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32c5ff8e0784bff8a2e7ad1d890edbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3093465566635132, 'eval_accuracy': 0.9708973322554567, 'eval_f1': 0.9696887254279255, 'eval_precision': 0.9708669158286354, 'eval_recall': 0.9708973322554567, 'eval_runtime': 2.4145, 'eval_samples_per_second': 512.33, 'eval_steps_per_second': 32.305, 'epoch': 1.0}\n",
      "{'loss': 0.2831, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5173b0eb316a40ffbc86f5b3a08e2c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26695767045021057, 'eval_accuracy': 0.9692805173807599, 'eval_f1': 0.9688560203264991, 'eval_precision': 0.9687044929951657, 'eval_recall': 0.9692805173807599, 'eval_runtime': 2.428, 'eval_samples_per_second': 509.464, 'eval_steps_per_second': 32.125, 'epoch': 2.0}\n",
      "{'train_runtime': 76.542, 'train_samples_per_second': 129.262, 'train_steps_per_second': 8.1, 'train_loss': 0.26500001107492754, 'epoch': 2.0}\n",
      "Training time for electra: 76.55 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998b0f9406f045108795060cced6939a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for electra: 2.43 seconds\n",
      "\n",
      "Classification Report for electra:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9697    0.8000    0.8767       160\n",
      "           1     0.9710    0.9963    0.9835      1077\n",
      "\n",
      "    accuracy                         0.9709      1237\n",
      "   macro avg     0.9704    0.8981    0.9301      1237\n",
      "weighted avg     0.9709    0.9709    0.9697      1237\n",
      "\n",
      "{'model': 'electra', 'max_length': 512, 'train_time_sec': 76.5481953125, 'pred_time_sec': 2.4304140625, 'accuracy': 0.9708973322554567, 'f1': 0.9696887254279255, 'precision': 0.9708669158286354, 'recall': 0.9708973322554567}\n",
      "\n",
      "--- Training albert with max_length=128 ---\n",
      "\n",
      "===== Training albert =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dedfcf63b104203a40654a10fc9a526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc43fb2cb594bb5b284cda7c80bae8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0ac6145b404ed2a400d7809c2c2300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a13357463141dda2a2f047243b4af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5847723484039307, 'eval_accuracy': 0.9571544058205336, 'eval_f1': 0.9537574054957654, 'eval_precision': 0.9582348243213428, 'eval_recall': 0.9571544058205336, 'eval_runtime': 2.058, 'eval_samples_per_second': 601.066, 'eval_steps_per_second': 37.901, 'epoch': 1.0}\n",
      "{'loss': 0.4122, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2800c10e9a664a0bb7d3981e0dff815c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4008103013038635, 'eval_accuracy': 0.9684721099434115, 'eval_f1': 0.9673170445490343, 'eval_precision': 0.9680110400297258, 'eval_recall': 0.9684721099434115, 'eval_runtime': 2.1177, 'eval_samples_per_second': 584.116, 'eval_steps_per_second': 36.832, 'epoch': 2.0}\n",
      "{'train_runtime': 52.3993, 'train_samples_per_second': 188.819, 'train_steps_per_second': 11.832, 'train_loss': 0.3993192857311618, 'epoch': 2.0}\n",
      "Training time for albert: 52.40 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04816835b2c740ca88744af29f21dd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert: 2.10 seconds\n",
      "\n",
      "Classification Report for albert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9481    0.8000    0.8678       160\n",
      "           1     0.9710    0.9935    0.9821      1077\n",
      "\n",
      "    accuracy                         0.9685      1237\n",
      "   macro avg     0.9596    0.8968    0.9249      1237\n",
      "weighted avg     0.9680    0.9685    0.9673      1237\n",
      "\n",
      "{'model': 'albert', 'max_length': 128, 'train_time_sec': 52.4049140625, 'pred_time_sec': 2.095880615234375, 'accuracy': 0.9684721099434115, 'f1': 0.9673170445490343, 'precision': 0.9680110400297258, 'recall': 0.9684721099434115}\n",
      "\n",
      "--- Training albert with max_length=256 ---\n",
      "\n",
      "===== Training albert =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22088d536b8d4b87bd8f7d4fe42f26e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092bfb4dba6840ec8170d6adc502a771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863a920ebbf6419985fa9d9e02444235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d51c25825244ef4934dc8cb95b0cbd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48803243041038513, 'eval_accuracy': 0.9636216653193209, 'eval_f1': 0.9615507416118981, 'eval_precision': 0.9637498038746753, 'eval_recall': 0.9636216653193209, 'eval_runtime': 4.9767, 'eval_samples_per_second': 248.56, 'eval_steps_per_second': 15.673, 'epoch': 1.0}\n",
      "{'loss': 0.4027, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e964b427ef4755988ca870067821ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3633476495742798, 'eval_accuracy': 0.957962813257882, 'eval_f1': 0.9578501186142488, 'eval_precision': 0.9577474459523042, 'eval_recall': 0.957962813257882, 'eval_runtime': 4.9936, 'eval_samples_per_second': 247.716, 'eval_steps_per_second': 15.62, 'epoch': 2.0}\n",
      "{'train_runtime': 114.1407, 'train_samples_per_second': 86.683, 'train_steps_per_second': 5.432, 'train_loss': 0.3797174576790102, 'epoch': 2.0}\n",
      "Training time for albert: 114.15 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4953b211d55b4480bc47006dccc468df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert: 5.00 seconds\n",
      "\n",
      "Classification Report for albert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9675    0.7438    0.8410       160\n",
      "           1     0.9632    0.9963    0.9795      1077\n",
      "\n",
      "    accuracy                         0.9636      1237\n",
      "   macro avg     0.9653    0.8700    0.9102      1237\n",
      "weighted avg     0.9637    0.9636    0.9616      1237\n",
      "\n",
      "{'model': 'albert', 'max_length': 256, 'train_time_sec': 114.1465625, 'pred_time_sec': 5.001201171875, 'accuracy': 0.9636216653193209, 'f1': 0.9615507416118981, 'precision': 0.9637498038746753, 'recall': 0.9636216653193209}\n",
      "\n",
      "--- Training albert with max_length=512 ---\n",
      "\n",
      "===== Training albert =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35817f62605450ab5b5e549c4371bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8776660789f4f5fa603ff53100f2404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a0b81bf9c14182839c6f7da9303e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fffce70072f4b27a0f1cc6d6e981b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5117623805999756, 'eval_accuracy': 0.788197251414713, 'eval_f1': 0.8166650858902188, 'eval_precision': 0.87612981022703, 'eval_recall': 0.788197251414713, 'eval_runtime': 12.1886, 'eval_samples_per_second': 101.488, 'eval_steps_per_second': 6.399, 'epoch': 1.0}\n",
      "{'loss': 0.5416, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d745fd8be74f21a69b1103a275f1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.472799152135849, 'eval_accuracy': 0.9620048504446241, 'eval_f1': 0.9603639940725923, 'eval_precision': 0.9611996903405482, 'eval_recall': 0.9620048504446241, 'eval_runtime': 12.134, 'eval_samples_per_second': 101.945, 'eval_steps_per_second': 6.428, 'epoch': 2.0}\n",
      "{'train_runtime': 270.341, 'train_samples_per_second': 36.598, 'train_steps_per_second': 2.293, 'train_loss': 0.5097725099132907, 'epoch': 2.0}\n",
      "Training time for albert: 270.35 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bf5157c20a446687854e23b9caba59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert: 12.12 seconds\n",
      "\n",
      "Classification Report for albert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9313    0.7625    0.8385       160\n",
      "           1     0.9656    0.9916    0.9785      1077\n",
      "\n",
      "    accuracy                         0.9620      1237\n",
      "   macro avg     0.9485    0.8771    0.9085      1237\n",
      "weighted avg     0.9612    0.9620    0.9604      1237\n",
      "\n",
      "{'model': 'albert', 'max_length': 512, 'train_time_sec': 270.346375, 'pred_time_sec': 12.1168916015625, 'accuracy': 0.9620048504446241, 'f1': 0.9603639940725923, 'precision': 0.9611996903405482, 'recall': 0.9620048504446241}\n",
      "\n",
      "--- Training xlnet with max_length=128 ---\n",
      "\n",
      "===== Training xlnet =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933311e57559484bac06d716298d1e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316a35b5a5b04c55bfacdd2494b66d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0f1cb71e924ce4b08a284de7c2aaaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d082a53d274cb7a5494b427b04e169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.38627681136131287, 'eval_accuracy': 0.9749393694421988, 'eval_f1': 0.9741013910784672, 'eval_precision': 0.9748528800386662, 'eval_recall': 0.9749393694421988, 'eval_runtime': 2.3303, 'eval_samples_per_second': 530.822, 'eval_steps_per_second': 33.471, 'epoch': 1.0}\n",
      "{'loss': 0.3283, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ed50fa767c4e6ebff5c5dc97422138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.260641872882843, 'eval_accuracy': 0.9652384801940178, 'eval_f1': 0.96528455305351, 'eval_precision': 0.965333168338002, 'eval_recall': 0.9652384801940178, 'eval_runtime': 2.3355, 'eval_samples_per_second': 529.658, 'eval_steps_per_second': 33.398, 'epoch': 2.0}\n",
      "{'train_runtime': 79.5791, 'train_samples_per_second': 124.329, 'train_steps_per_second': 7.791, 'train_loss': 0.3019749826000583, 'epoch': 2.0}\n",
      "Training time for xlnet: 79.59 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e1568b86604a94a12107745566430d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for xlnet: 2.43 seconds\n",
      "\n",
      "Classification Report for xlnet:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9708    0.8313    0.8956       160\n",
      "           1     0.9755    0.9963    0.9858      1077\n",
      "\n",
      "    accuracy                         0.9749      1237\n",
      "   macro avg     0.9731    0.9138    0.9407      1237\n",
      "weighted avg     0.9749    0.9749    0.9741      1237\n",
      "\n",
      "{'model': 'xlnet', 'max_length': 128, 'train_time_sec': 79.58709375, 'pred_time_sec': 2.426048828125, 'accuracy': 0.9749393694421988, 'f1': 0.9741013910784672, 'precision': 0.9748528800386662, 'recall': 0.9749393694421988}\n",
      "\n",
      "--- Training xlnet with max_length=256 ---\n",
      "\n",
      "===== Training xlnet =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b69403ed554e5cbccf19c168cf8f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4dea5e2aa549bfbedc49429ef88ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023dbdf9ec0349c697ba07829a34b147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9524875eb3ad49b78f49a670e039aaf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5054289102554321, 'eval_accuracy': 0.9684721099434115, 'eval_f1': 0.9668973956381423, 'eval_precision': 0.9687071485135125, 'eval_recall': 0.9684721099434115, 'eval_runtime': 5.5305, 'eval_samples_per_second': 223.667, 'eval_steps_per_second': 14.104, 'epoch': 1.0}\n",
      "{'loss': 0.357, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692c8435a65c453ca4fc05714001c807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31602728366851807, 'eval_accuracy': 0.9684721099434115, 'eval_f1': 0.9678991504476525, 'eval_precision': 0.9677933968303749, 'eval_recall': 0.9684721099434115, 'eval_runtime': 5.5423, 'eval_samples_per_second': 223.194, 'eval_steps_per_second': 14.074, 'epoch': 2.0}\n",
      "{'train_runtime': 160.509, 'train_samples_per_second': 61.641, 'train_steps_per_second': 3.863, 'train_loss': 0.32652853381249214, 'epoch': 2.0}\n",
      "Training time for xlnet: 160.52 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7988232e26264dbc96a17f22d6b735b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for xlnet: 5.64 seconds\n",
      "\n",
      "Classification Report for xlnet:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9116    0.8375    0.8730       160\n",
      "           1     0.9761    0.9879    0.9820      1077\n",
      "\n",
      "    accuracy                         0.9685      1237\n",
      "   macro avg     0.9439    0.9127    0.9275      1237\n",
      "weighted avg     0.9678    0.9685    0.9679      1237\n",
      "\n",
      "{'model': 'xlnet', 'max_length': 256, 'train_time_sec': 160.51875, 'pred_time_sec': 5.63979296875, 'accuracy': 0.9684721099434115, 'f1': 0.9678991504476525, 'precision': 0.9677933968303749, 'recall': 0.9684721099434115}\n",
      "\n",
      "--- Training xlnet with max_length=512 ---\n",
      "\n",
      "===== Training xlnet =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36eff8a2c9874b6ea873eb037b033408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e1d2391b2847ada400d05d1c2bf2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5d09cee2b54d31ad148f4698b7cf6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a50ef4719c4e4c8ad9be0d555a562c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36374807357788086, 'eval_accuracy': 0.9644300727566694, 'eval_f1': 0.9637312291616426, 'eval_precision': 0.9635681321673231, 'eval_recall': 0.9644300727566694, 'eval_runtime': 16.5635, 'eval_samples_per_second': 74.682, 'eval_steps_per_second': 4.709, 'epoch': 1.0}\n",
      "{'loss': 0.3291, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab98fad34ac4fbbabd486c0dd11535c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23734351992607117, 'eval_accuracy': 0.9749393694421988, 'eval_f1': 0.9745569708581736, 'eval_precision': 0.9745141965585394, 'eval_recall': 0.9749393694421988, 'eval_runtime': 16.5359, 'eval_samples_per_second': 74.807, 'eval_steps_per_second': 4.717, 'epoch': 2.0}\n",
      "{'train_runtime': 427.9047, 'train_samples_per_second': 23.122, 'train_steps_per_second': 1.449, 'train_loss': 0.3002438083771736, 'epoch': 2.0}\n",
      "Training time for xlnet: 427.91 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73615ac5b4b84a34b4eb5489e74d3d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for xlnet: 16.48 seconds\n",
      "\n",
      "Classification Report for xlnet:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9329    0.8688    0.8997       160\n",
      "           1     0.9807    0.9907    0.9857      1077\n",
      "\n",
      "    accuracy                         0.9749      1237\n",
      "   macro avg     0.9568    0.9297    0.9427      1237\n",
      "weighted avg     0.9745    0.9749    0.9746      1237\n",
      "\n",
      "{'model': 'xlnet', 'max_length': 512, 'train_time_sec': 427.9133125, 'pred_time_sec': 16.477611328125, 'accuracy': 0.9749393694421988, 'f1': 0.9745569708581736, 'precision': 0.9745141965585394, 'recall': 0.9749393694421988}\n",
      "\n",
      "--- Training mobilebert with max_length=128 ---\n",
      "\n",
      "===== Training mobilebert =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.dense.weight']\n",
      "- This IS expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074c8775a6b24dc4b9ff2ab9ea337cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785cb58525bf4e3aaee898b336a53d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3cb72e363034de49e7061a63f1d2dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a MobileBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe98217cccd4cfa99eaa0490506c81f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.775018215179443, 'eval_accuracy': 0.9620048504446241, 'eval_f1': 0.9602365791150439, 'eval_precision': 0.9613328008201311, 'eval_recall': 0.9620048504446241, 'eval_runtime': 3.2395, 'eval_samples_per_second': 381.846, 'eval_steps_per_second': 24.078, 'epoch': 1.0}\n",
      "{'loss': 62947.676, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7a67349f6b485a8686918134df01d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.36194908618927, 'eval_accuracy': 0.9660468876313663, 'eval_f1': 0.9652785574387894, 'eval_precision': 0.9652223728586796, 'eval_recall': 0.9660468876313663, 'eval_runtime': 2.6125, 'eval_samples_per_second': 473.495, 'eval_steps_per_second': 29.857, 'epoch': 2.0}\n",
      "{'train_runtime': 146.1009, 'train_samples_per_second': 67.72, 'train_steps_per_second': 4.244, 'train_loss': 50764.41195059745, 'epoch': 2.0}\n",
      "Training time for mobilebert: 146.13 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ed81edc0ae4281aab728fddd06456b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for mobilebert: 2.66 seconds\n",
      "\n",
      "Classification Report for mobilebert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9097    0.8187    0.8618       160\n",
      "           1     0.9735    0.9879    0.9806      1077\n",
      "\n",
      "    accuracy                         0.9660      1237\n",
      "   macro avg     0.9416    0.9033    0.9212      1237\n",
      "weighted avg     0.9652    0.9660    0.9653      1237\n",
      "\n",
      "{'model': 'mobilebert', 'max_length': 128, 'train_time_sec': 146.126796875, 'pred_time_sec': 2.658034912109375, 'accuracy': 0.9660468876313663, 'f1': 0.9652785574387894, 'precision': 0.9652223728586796, 'recall': 0.9660468876313663}\n",
      "\n",
      "--- Training mobilebert with max_length=256 ---\n",
      "\n",
      "===== Training mobilebert =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.dense.weight']\n",
      "- This IS expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb65f2d542ce44638b81fec706f4b58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99b78c018cb472eb24abc6aba102eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8775d735f4d14a14ad6471f0b67ddaca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a MobileBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8710fe8826aa4588aacec418583516ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5493566989898682, 'eval_accuracy': 0.9700889248181084, 'eval_f1': 0.9686968814309919, 'eval_precision': 0.9702751279752208, 'eval_recall': 0.9700889248181084, 'eval_runtime': 2.6448, 'eval_samples_per_second': 467.703, 'eval_steps_per_second': 29.491, 'epoch': 1.0}\n",
      "{'loss': 60764.664, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe0696b5f234267b56b261e0cb5634a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3112986981868744, 'eval_accuracy': 0.973322554567502, 'eval_f1': 0.9725145088670764, 'eval_precision': 0.9730072098807431, 'eval_recall': 0.973322554567502, 'eval_runtime': 2.4611, 'eval_samples_per_second': 502.628, 'eval_steps_per_second': 31.694, 'epoch': 2.0}\n",
      "{'train_runtime': 142.6492, 'train_samples_per_second': 69.359, 'train_steps_per_second': 4.346, 'train_loss': 49003.797861394574, 'epoch': 2.0}\n",
      "Training time for mobilebert: 142.67 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf5f8cd6257446e94e649001c2dc1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for mobilebert: 2.50 seconds\n",
      "\n",
      "Classification Report for mobilebert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9568    0.8313    0.8896       160\n",
      "           1     0.9754    0.9944    0.9848      1077\n",
      "\n",
      "    accuracy                         0.9733      1237\n",
      "   macro avg     0.9661    0.9128    0.9372      1237\n",
      "weighted avg     0.9730    0.9733    0.9725      1237\n",
      "\n",
      "{'model': 'mobilebert', 'max_length': 256, 'train_time_sec': 142.673921875, 'pred_time_sec': 2.50087890625, 'accuracy': 0.973322554567502, 'f1': 0.9725145088670764, 'precision': 0.9730072098807431, 'recall': 0.973322554567502}\n",
      "\n",
      "--- Training mobilebert with max_length=512 ---\n",
      "\n",
      "===== Training mobilebert =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.dense.weight']\n",
      "- This IS expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4913a6dfea7f40458dfd9b760c9d101a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2ecc76520743c3be33830653f73068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a0905c34ae449bb81636d328a910c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a MobileBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80926c4138345fea059ff64d6b72ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3945624828338623, 'eval_accuracy': 0.9700889248181084, 'eval_f1': 0.9689930935465195, 'eval_precision': 0.9697592204337492, 'eval_recall': 0.9700889248181084, 'eval_runtime': 4.9037, 'eval_samples_per_second': 252.26, 'eval_steps_per_second': 15.906, 'epoch': 1.0}\n",
      "{'loss': 58091.616, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858637bed1224821bd286d93191b9999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26719382405281067, 'eval_accuracy': 0.973322554567502, 'eval_f1': 0.9725972048136188, 'eval_precision': 0.9729243876440257, 'eval_recall': 0.973322554567502, 'eval_runtime': 4.9235, 'eval_samples_per_second': 251.246, 'eval_steps_per_second': 15.843, 'epoch': 2.0}\n",
      "{'train_runtime': 209.8274, 'train_samples_per_second': 47.153, 'train_steps_per_second': 2.955, 'train_loss': 46848.108150854416, 'epoch': 2.0}\n",
      "Training time for mobilebert: 209.85 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c92b2a1527e427193429d9779f70235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for mobilebert: 4.94 seconds\n",
      "\n",
      "Classification Report for mobilebert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9504    0.8375    0.8904       160\n",
      "           1     0.9763    0.9935    0.9848      1077\n",
      "\n",
      "    accuracy                         0.9733      1237\n",
      "   macro avg     0.9633    0.9155    0.9376      1237\n",
      "weighted avg     0.9729    0.9733    0.9726      1237\n",
      "\n",
      "{'model': 'mobilebert', 'max_length': 512, 'train_time_sec': 209.85103125, 'pred_time_sec': 4.9377783203125, 'accuracy': 0.973322554567502, 'f1': 0.9725972048136188, 'precision': 0.9729243876440257, 'recall': 0.973322554567502}\n",
      "\n",
      "--- Training albert-base-v1 with max_length=128 ---\n",
      "\n",
      "===== Training albert-base-v1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at albert-base-v1 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9613c062c80842c0890f7a54561cc632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e7b845edd943f5923bc0d612277779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad75cf19aa0c474cab577e48e0086e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6671173b2be74b5bad37013881f464fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3856833875179291, 'eval_accuracy': 0.9708973322554567, 'eval_f1': 0.9696887254279255, 'eval_precision': 0.9708669158286354, 'eval_recall': 0.9708973322554567, 'eval_runtime': 1.7541, 'eval_samples_per_second': 705.189, 'eval_steps_per_second': 44.466, 'epoch': 1.0}\n",
      "{'loss': 0.3129, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6384370994d415bb751c1be45f74eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2440875768661499, 'eval_accuracy': 0.9717057396928052, 'eval_f1': 0.9711078155015639, 'eval_precision': 0.9711470685810719, 'eval_recall': 0.9717057396928052, 'eval_runtime': 1.7601, 'eval_samples_per_second': 702.804, 'eval_steps_per_second': 44.316, 'epoch': 2.0}\n",
      "{'train_runtime': 43.5767, 'train_samples_per_second': 227.048, 'train_steps_per_second': 14.228, 'train_loss': 0.28822908401489256, 'epoch': 2.0}\n",
      "Training time for albert-base-v1: 43.58 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62305120c06b4a84b3b861572ea2c983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-base-v1: 1.76 seconds\n",
      "\n",
      "Classification Report for albert-base-v1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9310    0.8438    0.8852       160\n",
      "           1     0.9771    0.9907    0.9839      1077\n",
      "\n",
      "    accuracy                         0.9717      1237\n",
      "   macro avg     0.9541    0.9172    0.9346      1237\n",
      "weighted avg     0.9711    0.9717    0.9711      1237\n",
      "\n",
      "{'model': 'albert-base-v1', 'max_length': 128, 'train_time_sec': 43.58189453125, 'pred_time_sec': 1.757774169921875, 'accuracy': 0.9717057396928052, 'f1': 0.9711078155015639, 'precision': 0.9711470685810719, 'recall': 0.9717057396928052}\n",
      "\n",
      "--- Training albert-base-v1 with max_length=256 ---\n",
      "\n",
      "===== Training albert-base-v1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at albert-base-v1 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3ec4b2b1d64e0f9c599bb63173d7d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afb7c67101d441faaed8a658fb97d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436659ec6f304931a9ac89aff6d19651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473fc7251ebc414abd7d667ec0cf8f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.38635721802711487, 'eval_accuracy': 0.973322554567502, 'eval_f1': 0.9723451915414905, 'eval_precision': 0.9732555812417957, 'eval_recall': 0.973322554567502, 'eval_runtime': 3.6432, 'eval_samples_per_second': 339.541, 'eval_steps_per_second': 21.41, 'epoch': 1.0}\n",
      "{'loss': 0.3056, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f4eb577d40426d8181af775165909c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22760340571403503, 'eval_accuracy': 0.9797898140662894, 'eval_f1': 0.9794225323382386, 'eval_precision': 0.9795440527564181, 'eval_recall': 0.9797898140662894, 'eval_runtime': 3.6572, 'eval_samples_per_second': 338.241, 'eval_steps_per_second': 21.328, 'epoch': 2.0}\n",
      "{'train_runtime': 86.6825, 'train_samples_per_second': 114.141, 'train_steps_per_second': 7.153, 'train_loss': 0.27678232192993163, 'epoch': 2.0}\n",
      "Training time for albert-base-v1: 86.69 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1079ca04e3cb43ebaee254b320b25e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-base-v1: 3.69 seconds\n",
      "\n",
      "Classification Report for albert-base-v1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9592    0.8812    0.9186       160\n",
      "           1     0.9826    0.9944    0.9885      1077\n",
      "\n",
      "    accuracy                         0.9798      1237\n",
      "   macro avg     0.9709    0.9378    0.9535      1237\n",
      "weighted avg     0.9795    0.9798    0.9794      1237\n",
      "\n",
      "{'model': 'albert-base-v1', 'max_length': 256, 'train_time_sec': 86.6874296875, 'pred_time_sec': 3.68730712890625, 'accuracy': 0.9797898140662894, 'f1': 0.9794225323382386, 'precision': 0.9795440527564181, 'recall': 0.9797898140662894}\n",
      "\n",
      "--- Training albert-base-v1 with max_length=512 ---\n",
      "\n",
      "===== Training albert-base-v1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at albert-base-v1 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ab03b1cbd5496882e78e4d806d3d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0753d74a979b4e5392731cbe231b8551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa476597bad4e8882bb67a08ae32c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2797bc6eabcd4600ae70e8468d2cbc50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4071013331413269, 'eval_accuracy': 0.9708973322554567, 'eval_f1': 0.9696887254279255, 'eval_precision': 0.9708669158286354, 'eval_recall': 0.9708973322554567, 'eval_runtime': 9.0622, 'eval_samples_per_second': 136.501, 'eval_steps_per_second': 8.607, 'epoch': 1.0}\n",
      "{'loss': 0.293, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ad6e821cf4404b9a5b62797e2421a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22431425750255585, 'eval_accuracy': 0.9741309620048505, 'eval_f1': 0.9739197348103219, 'eval_precision': 0.9738066044069531, 'eval_recall': 0.9741309620048505, 'eval_runtime': 9.0207, 'eval_samples_per_second': 137.129, 'eval_steps_per_second': 8.647, 'epoch': 2.0}\n",
      "{'train_runtime': 215.2808, 'train_samples_per_second': 45.959, 'train_steps_per_second': 2.88, 'train_loss': 0.2732754891918552, 'epoch': 2.0}\n",
      "Training time for albert-base-v1: 215.29 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5bea5232b3242b48cb1102d264cd353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-base-v1: 9.02 seconds\n",
      "\n",
      "Classification Report for albert-base-v1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9156    0.8812    0.8981       160\n",
      "           1     0.9825    0.9879    0.9852      1077\n",
      "\n",
      "    accuracy                         0.9741      1237\n",
      "   macro avg     0.9490    0.9346    0.9416      1237\n",
      "weighted avg     0.9738    0.9741    0.9739      1237\n",
      "\n",
      "{'model': 'albert-base-v1', 'max_length': 512, 'train_time_sec': 215.28553125, 'pred_time_sec': 9.0176826171875, 'accuracy': 0.9741309620048505, 'f1': 0.9739197348103219, 'precision': 0.9738066044069531, 'recall': 0.9741309620048505}\n",
      "\n",
      "--- Training albert-large-v2 with max_length=128 ---\n",
      "\n",
      "===== Training albert-large-v2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-large-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b4e5c334b6471bbb6e12d9aa8280db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5bf4799b4394235b34a28b5585114dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b62521ec774a4299277fb1d3503247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59442911f6e42c3903c6daa06f7d20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.689713180065155, 'eval_accuracy': 0.8706548100242523, 'eval_f1': 0.8104539588557645, 'eval_precision': 0.7580397982183668, 'eval_recall': 0.8706548100242523, 'eval_runtime': 6.621, 'eval_samples_per_second': 186.829, 'eval_steps_per_second': 11.781, 'epoch': 1.0}\n",
      "{'loss': 0.6534, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd1f3605c5b40528e6e18bafd0ac4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.692589282989502, 'eval_accuracy': 0.8706548100242523, 'eval_f1': 0.8104539588557645, 'eval_precision': 0.7580397982183668, 'eval_recall': 0.8706548100242523, 'eval_runtime': 6.6115, 'eval_samples_per_second': 187.098, 'eval_steps_per_second': 11.798, 'epoch': 2.0}\n",
      "{'train_runtime': 155.6488, 'train_samples_per_second': 63.566, 'train_steps_per_second': 3.983, 'train_loss': 0.6623490118211316, 'epoch': 2.0}\n",
      "Training time for albert-large-v2: 155.66 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c1f1ac61634e799f2a64d0dbbeef6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-large-v2: 6.45 seconds\n",
      "\n",
      "Classification Report for albert-large-v2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       160\n",
      "           1     0.8707    1.0000    0.9309      1077\n",
      "\n",
      "    accuracy                         0.8707      1237\n",
      "   macro avg     0.4353    0.5000    0.4654      1237\n",
      "weighted avg     0.7580    0.8707    0.8105      1237\n",
      "\n",
      "{'model': 'albert-large-v2', 'max_length': 128, 'train_time_sec': 155.655171875, 'pred_time_sec': 6.45188037109375, 'accuracy': 0.8706548100242523, 'f1': 0.8104539588557645, 'precision': 0.7580397982183668, 'recall': 0.8706548100242523}\n",
      "\n",
      "--- Training albert-large-v2 with max_length=256 ---\n",
      "\n",
      "===== Training albert-large-v2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-large-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd811143e28d4419893f983e37047528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872a4e5c27d74718b62806b9b412e7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d987cc99855b47e08bf6e359b359422b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e6ec6d2fca40ccbe7f52d31bc825e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.692845344543457, 'eval_accuracy': 0.8706548100242523, 'eval_f1': 0.8104539588557645, 'eval_precision': 0.7580397982183668, 'eval_recall': 0.8706548100242523, 'eval_runtime': 15.3258, 'eval_samples_per_second': 80.713, 'eval_steps_per_second': 5.089, 'epoch': 1.0}\n",
      "{'loss': 0.7083, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d22b28687b4b32aa72031d934c83be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.69178706407547, 'eval_accuracy': 0.8706548100242523, 'eval_f1': 0.8104539588557645, 'eval_precision': 0.7580397982183668, 'eval_recall': 0.8706548100242523, 'eval_runtime': 15.346, 'eval_samples_per_second': 80.607, 'eval_steps_per_second': 5.083, 'epoch': 2.0}\n",
      "{'train_runtime': 342.6419, 'train_samples_per_second': 28.876, 'train_steps_per_second': 1.809, 'train_loss': 0.7065734124952747, 'epoch': 2.0}\n",
      "Training time for albert-large-v2: 342.65 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fc4722f65a4aaebce6c02269193937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-large-v2: 15.10 seconds\n",
      "\n",
      "Classification Report for albert-large-v2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       160\n",
      "           1     0.8707    1.0000    0.9309      1077\n",
      "\n",
      "    accuracy                         0.8707      1237\n",
      "   macro avg     0.4353    0.5000    0.4654      1237\n",
      "weighted avg     0.7580    0.8707    0.8105      1237\n",
      "\n",
      "{'model': 'albert-large-v2', 'max_length': 256, 'train_time_sec': 342.648125, 'pred_time_sec': 15.096859375, 'accuracy': 0.8706548100242523, 'f1': 0.8104539588557645, 'precision': 0.7580397982183668, 'recall': 0.8706548100242523}\n",
      "\n",
      "--- Training albert-large-v2 with max_length=512 ---\n",
      "\n",
      "===== Training albert-large-v2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-large-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa062974096431ab2ee870e38e07d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f473dbed3f94ef2beb42c0793c4fdf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d793bc98a16c4dbfa1ea3ffda2b7b16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90e929212b8443584ef13b60aef4a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4396832585334778, 'eval_accuracy': 0.8900565885206144, 'eval_f1': 0.8977182843313056, 'eval_precision': 0.9120539278515735, 'eval_recall': 0.8900565885206144, 'eval_runtime': 395.4489, 'eval_samples_per_second': 3.128, 'eval_steps_per_second': 0.197, 'epoch': 1.0}\n",
      "{'loss': 0.5494, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b040ae5eb144f198e7db1216fe0a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3511715531349182, 'eval_accuracy': 0.9708973322554567, 'eval_f1': 0.9698778396507508, 'eval_precision': 0.9705700302540556, 'eval_recall': 0.9708973322554567, 'eval_runtime': 371.4645, 'eval_samples_per_second': 3.33, 'eval_steps_per_second': 0.21, 'epoch': 2.0}\n",
      "{'train_runtime': 4661.7777, 'train_samples_per_second': 2.122, 'train_steps_per_second': 0.133, 'train_loss': 0.5064292415495841, 'epoch': 2.0}\n",
      "Training time for albert-large-v2: 4661.78 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59e8c009f5b40ae914de1530dfb4950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-large-v2: 371.64 seconds\n",
      "\n",
      "Classification Report for albert-large-v2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9559    0.8125    0.8784       160\n",
      "           1     0.9728    0.9944    0.9835      1077\n",
      "\n",
      "    accuracy                         0.9709      1237\n",
      "   macro avg     0.9643    0.9035    0.9309      1237\n",
      "weighted avg     0.9706    0.9709    0.9699      1237\n",
      "\n",
      "{'model': 'albert-large-v2', 'max_length': 512, 'train_time_sec': 4661.7805, 'pred_time_sec': 371.643625, 'accuracy': 0.9708973322554567, 'f1': 0.9698778396507508, 'precision': 0.9705700302540556, 'recall': 0.9708973322554567}\n",
      "\n",
      "--- Training albert-xlarge-v2 with max_length=128 ---\n",
      "\n",
      "===== Training albert-xlarge-v2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at albert-xlarge-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-xlarge-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1327aea29f30476596d522022d0c27d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f5b5a33bd544588dab5dad42d7821c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3853fc530bc474887735848fe06129c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180ece01e6d74c7e9f12773e97634e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34922975301742554, 'eval_accuracy': 0.9506871463217461, 'eval_f1': 0.9502155927462494, 'eval_precision': 0.9498660170395512, 'eval_recall': 0.9506871463217461, 'eval_runtime': 22.6741, 'eval_samples_per_second': 54.556, 'eval_steps_per_second': 3.44, 'epoch': 1.0}\n",
      "{'loss': 0.4891, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabbc7ee93414168b6384f2f54f0efba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.38900822401046753, 'eval_accuracy': 0.9684721099434115, 'eval_f1': 0.9673170445490343, 'eval_precision': 0.9680110400297258, 'eval_recall': 0.9684721099434115, 'eval_runtime': 22.5656, 'eval_samples_per_second': 54.818, 'eval_steps_per_second': 3.457, 'epoch': 2.0}\n",
      "{'train_runtime': 780.0783, 'train_samples_per_second': 12.683, 'train_steps_per_second': 0.795, 'train_loss': 0.4566705519153226, 'epoch': 2.0}\n",
      "Training time for albert-xlarge-v2: 780.09 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfcc2c305be041158ce0c8d2a85081b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-xlarge-v2: 22.92 seconds\n",
      "\n",
      "Classification Report for albert-xlarge-v2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9481    0.8000    0.8678       160\n",
      "           1     0.9710    0.9935    0.9821      1077\n",
      "\n",
      "    accuracy                         0.9685      1237\n",
      "   macro avg     0.9596    0.8968    0.9249      1237\n",
      "weighted avg     0.9680    0.9685    0.9673      1237\n",
      "\n",
      "{'model': 'albert-xlarge-v2', 'max_length': 128, 'train_time_sec': 780.0850625, 'pred_time_sec': 22.922814453125, 'accuracy': 0.9684721099434115, 'f1': 0.9673170445490343, 'precision': 0.9680110400297258, 'recall': 0.9684721099434115}\n",
      "\n",
      "--- Training albert-xlarge-v2 with max_length=256 ---\n",
      "\n",
      "===== Training albert-xlarge-v2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at albert-xlarge-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-xlarge-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e565bb242ada4e38ba4486ac3e62b8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695c30b718a346fba5b98b1b1ccb5c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd08e0c451849b4a2765b5fcb6e1527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f712b068aa4f7cb5070246feda5697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6915135383605957, 'eval_accuracy': 0.8698464025869038, 'eval_f1': 0.8100515136931218, 'eval_precision': 0.7579486857492681, 'eval_recall': 0.8698464025869038, 'eval_runtime': 331.6743, 'eval_samples_per_second': 3.73, 'eval_steps_per_second': 0.235, 'epoch': 1.0}\n",
      "{'loss': 0.7368, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa843df948114c14851f619463b7903c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.690826952457428, 'eval_accuracy': 0.8706548100242523, 'eval_f1': 0.8104539588557645, 'eval_precision': 0.7580397982183668, 'eval_recall': 0.8706548100242523, 'eval_runtime': 330.5914, 'eval_samples_per_second': 3.742, 'eval_steps_per_second': 0.236, 'epoch': 2.0}\n",
      "{'train_runtime': 2655.4206, 'train_samples_per_second': 3.726, 'train_steps_per_second': 0.233, 'train_loss': 0.733499502366589, 'epoch': 2.0}\n",
      "Training time for albert-xlarge-v2: 2655.43 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f5262d48294707bd797450e46a8881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-xlarge-v2: 331.56 seconds\n",
      "\n",
      "Classification Report for albert-xlarge-v2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       160\n",
      "           1     0.8707    1.0000    0.9309      1077\n",
      "\n",
      "    accuracy                         0.8707      1237\n",
      "   macro avg     0.4353    0.5000    0.4654      1237\n",
      "weighted avg     0.7580    0.8707    0.8105      1237\n",
      "\n",
      "{'model': 'albert-xlarge-v2', 'max_length': 256, 'train_time_sec': 2655.4265, 'pred_time_sec': 331.5628125, 'accuracy': 0.8706548100242523, 'f1': 0.8104539588557645, 'precision': 0.7580397982183668, 'recall': 0.8706548100242523}\n",
      "\n",
      "--- Training albert-xlarge-v2 with max_length=512 ---\n",
      "\n",
      "===== Training albert-xlarge-v2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-xlarge-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-xlarge-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c9db62bbd9429ca70c945f5223ccd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0fa534374b4c8691e0701578ef59ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0cc4e305304b8c82b02696c7c03e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m max_len \u001b[38;5;129;01min\u001b[39;00m CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with max_length=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     performance \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(performance)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# save performance to a csv file\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[35], line 57\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model_name, train_dataset, test_dataset, data, max_length)\u001b[0m\n\u001b[0;32m     55\u001b[0m t_train_end \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mEvent(enable_timing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     56\u001b[0m t_train_start\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[1;32m---> 57\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m t_train_end\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[0;32m     59\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\trainer.py:1664\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1661\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1662\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1663\u001b[0m )\n\u001b[1;32m-> 1664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\trainer.py:1940\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1938\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1939\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1940\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1943\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1944\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1945\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1946\u001b[0m ):\n\u001b[0;32m   1947\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\trainer.py:2735\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2735\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2738\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[34], line 9\u001b[0m, in \u001b[0;36mWeightedLossTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      8\u001b[0m     labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     10\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weights)\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:1074\u001b[0m, in \u001b[0;36mAlbertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1074\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1086\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1088\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:730\u001b[0m, in \u001b[0;36mAlbertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    725\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    727\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    728\u001b[0m     input_ids, position_ids\u001b[38;5;241m=\u001b[39mposition_ids, token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids, inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds\n\u001b[0;32m    729\u001b[0m )\n\u001b[1;32m--> 730\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    739\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    741\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler_activation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output[:, \u001b[38;5;241m0\u001b[39m])) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:479\u001b[0m, in \u001b[0;36mAlbertTransformer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Index of the hidden group\u001b[39;00m\n\u001b[0;32m    477\u001b[0m group_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(i \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_groups))\n\u001b[1;32m--> 479\u001b[0m layer_group_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malbert_layer_groups\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgroup_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgroup_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlayers_per_group\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlayers_per_group\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    486\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_group_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:431\u001b[0m, in \u001b[0;36mAlbertLayerGroup.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[0;32m    428\u001b[0m layer_attentions \u001b[38;5;241m=\u001b[39m ()\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_index, albert_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malbert_layers):\n\u001b[1;32m--> 431\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[43malbert_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:396\u001b[0m, in \u001b[0;36mAlbertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    388\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     output_hidden_states: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    393\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    394\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(hidden_states, attention_mask, head_mask, output_attentions)\n\u001b[1;32m--> 396\u001b[0m     ffn_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mff_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_layer_layer_norm(ffn_output \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\pytorch_utils.py:236\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:409\u001b[0m, in \u001b[0;36mAlbertLayer.ff_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    407\u001b[0m ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(attention_output)\n\u001b[0;32m    408\u001b[0m ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(ffn_output)\n\u001b[1;32m--> 409\u001b[0m ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mffn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ffn_output\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for model_name in MODEL_CONFIGS.keys():\n",
    "    for max_len in CONFIG[\"max_length\"]:\n",
    "        print(f\"\\n--- Training {model_name} with max_length={max_len} ---\")\n",
    "        performance = train_and_evaluate(model_name, train_dataset, test_dataset, data,max_len)\n",
    "        print(performance)\n",
    "        # save performance to a csv file\n",
    "        with open(\"model_performance.csv\", \"a\") as f:\n",
    "            #headers if file is empty\n",
    "            if os.stat(\"model_performance.csv\").st_size == 0:\n",
    "                f.write(\"model,max_length,train_time_sec,pred_time_sec,accuracy,f1,precision,recall\\n\")\n",
    "            f.write(\",\".join([str(performance[k]) for k in performance]) + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f468f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba7b36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-forge-Copy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
