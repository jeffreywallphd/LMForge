{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a903082",
   "metadata": {},
   "source": [
    "#### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f725fd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.29.2\n",
      "<class 'transformers.training_args.TrainingArguments'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "import logging\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    # tokenizers\n",
    "    AutoTokenizer,\n",
    "    DebertaV2Tokenizer,\n",
    "    DistilBertTokenizer,\n",
    "    BertTokenizer,\n",
    "    RobertaTokenizer,\n",
    "    ElectraTokenizer,\n",
    "    AlbertTokenizer,\n",
    "    XLNetTokenizer,\n",
    "    MobileBertTokenizer,\n",
    "    # models\n",
    "    DebertaV2ForSequenceClassification,\n",
    "    DistilBertForSequenceClassification,\n",
    "    BertForSequenceClassification,\n",
    "    RobertaForSequenceClassification,\n",
    "    ElectraForSequenceClassification,\n",
    "    AlbertForSequenceClassification,\n",
    "    XLNetForSequenceClassification,\n",
    "    MobileBertForSequenceClassification,\n",
    ")\n",
    "from torch.nn import CrossEntropyLoss\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "print(transformers.TrainingArguments)\n",
    "\n",
    "# Cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eadf70b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='classification.log', level=logging.INFO)\n",
    "logging.info(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f82b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: D:/huggingface_cache\n",
      "TRANSFORMERS_CACHE: D:/huggingface_cache\n",
      "HUGGINGFACE_HUB_CACHE: D:/huggingface_cache\n"
     ]
    }
   ],
   "source": [
    "# setting huggingface token\n",
    "login(token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"D:/huggingface_cache\" \n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"D:/huggingface_cache\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"D:/huggingface_cache\"\n",
    "\n",
    "print(\"HF_HOME:\", os.getenv(\"HF_HOME\"))\n",
    "print(\"TRANSFORMERS_CACHE:\", os.getenv(\"TRANSFORMERS_CACHE\"))\n",
    "print(\"HUGGINGFACE_HUB_CACHE:\", os.getenv(\"HUGGINGFACE_HUB_CACHE\"))\n",
    "\n",
    "logging.info(f\"HF_HOME: {os.getenv('HF_HOME')}\")\n",
    "logging.info(f\"TRANSFORMERS_CACHE: {os.getenv('TRANSFORMERS_CACHE')}\")\n",
    "logging.info(f\"HUGGINGFACE_HUB_CACHE: {os.getenv('HUGGINGFACE_HUB_CACHE')}\")\n",
    "\n",
    "transformers.utils.hub.TRANSFORMERS_CACHE = \"D:/huggingface_cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e30cc",
   "metadata": {},
   "source": [
    "### LOADING SQLITE DB WITH RECORDS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c002d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to JSONL file.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "DB_FILE = \"chunks.db\"\n",
    "OUTPUT_FILE = \"exported_chunks.jsonl\"\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Query all data from chunks table\n",
    "cur.execute(\"SELECT text, label FROM chunks\")\n",
    "rows = cur.fetchall()\n",
    "\n",
    "# Write to JSONL\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for text, label in rows:\n",
    "        obj = {\"text\": text}\n",
    "        if label is not None:\n",
    "            obj[\"label\"] = label\n",
    "        f.write(json.dumps(obj) + \"\\n\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"Data exported to JSONL file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47d48089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled chunks after removing label 11: {1: 8199, 0: 800}\n",
      "Final labeled chunks: {1: 5384, 0: 800}\n"
     ]
    }
   ],
   "source": [
    "# Load the labeled chunks\n",
    "with open(\"exported_chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    labeled_chunks = [json.loads(line) for line in f]\n",
    "\n",
    "data = pd.DataFrame(labeled_chunks)\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "\n",
    "# Get the first 9000 rows\n",
    "data = data.head(9000)\n",
    "\n",
    "# Remove rows with label == 11\n",
    "data = data[data['label'] != 11]\n",
    "\n",
    "# Print labeled count after removing label 11\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "print(f\"Labeled chunks after removing label 11: {labeled_count}\")\n",
    "\n",
    "# Remove rows where label == 1 and text length < 100\n",
    "data = data[~((data['label'] == 1) & (data['text'].str.len() < 100))]\n",
    "\n",
    "# Print final labeled count\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "print(f\"Final labeled chunks: {labeled_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ca564",
   "metadata": {},
   "source": [
    "##### Spliting data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f9aa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([3.8648, 0.5743], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "data['label'] = data['label'].astype(int)\n",
    "\n",
    "# Train-Test Split using stratified sampling\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, stratify=data['label'], random_state=42)\n",
    "\n",
    "# since there is a class imbalance, we will compute class weights\n",
    "# to handle this in the loss function\n",
    "labels = train_df[\"label\"].values\n",
    "# Compute class weights\n",
    "classes = np.unique(labels)\n",
    "weights = compute_class_weight(class_weight=\"balanced\",\n",
    "                            classes=classes,\n",
    "                            y=labels)\n",
    "class_weights = torch.tensor(weights, dtype=torch.float, device=device)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# Convert ing the DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a629272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Tuning parameters ----\n",
    "CONFIG = {\n",
    "    \"epochs\": 2,\n",
    "    \"batch_size\": 16,\n",
    "    \"max_length\": [128, 256, 512], # Max length of input sequences\n",
    "    \"learning_rate\": 5e-5, # Learning rate for the optimizer\n",
    "    \"weight_decay\": 0.01, # Weight decay for regularization\n",
    "    \"output_dir\": \"D:/huggingface_cache/classification_models\"\n",
    "}\n",
    "\n",
    "# ---- Model configurations ----\n",
    "MODEL_CONFIGS = {\n",
    "    \"deberta\": {\n",
    "        \"tokenizer_class\": DebertaV2Tokenizer,\n",
    "        \"pretrained_model_name\": \"microsoft/deberta-v3-small\", # params 55M\n",
    "        \"model_class\": DebertaV2ForSequenceClassification\n",
    "    },\n",
    "    \"distilbert\": {\n",
    "        \"tokenizer_class\": DistilBertTokenizer,\n",
    "        \"pretrained_model_name\": \"distilbert-base-uncased\", # params 66M\n",
    "        \"model_class\": DistilBertForSequenceClassification\n",
    "    },\n",
    "    \"bert\": {\n",
    "        \"tokenizer_class\": BertTokenizer,\n",
    "        \"pretrained_model_name\": \"bert-base-uncased\", # params 110M\n",
    "        \"model_class\": BertForSequenceClassification\n",
    "    },\n",
    "    \"roberta\": {\n",
    "        \"tokenizer_class\": RobertaTokenizer,\n",
    "        \"pretrained_model_name\": \"roberta-base\", # params 125M\n",
    "        \"model_class\": RobertaForSequenceClassification\n",
    "    },\n",
    "    \"electra\": {\n",
    "        \"tokenizer_class\": ElectraTokenizer,\n",
    "        \"pretrained_model_name\": \"google/electra-small-discriminator\", # params 14M\n",
    "        \"model_class\": ElectraForSequenceClassification\n",
    "    },\n",
    "    \"albert\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-base-v2\", # params 11M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"xlnet\": {\n",
    "        \"tokenizer_class\": XLNetTokenizer,\n",
    "        \"pretrained_model_name\": \"xlnet-base-cased\", # params 110M\n",
    "        \"model_class\": XLNetForSequenceClassification\n",
    "    },\n",
    "    \"mobilebert\": {\n",
    "        \"tokenizer_class\": AutoTokenizer,\n",
    "        \"pretrained_model_name\": \"google/mobilebert-uncased\", # params 25M\n",
    "        \"model_class\": MobileBertForSequenceClassification\n",
    "    },\n",
    "    \"albert-base-v1\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-base-v1\", # params 12M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"albert-large-v2\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-large-v2\", # params 18M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"albert-xlarge-v2\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-xlarge-v2\", # params 60M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"albert-xxlarge-v2\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-xxlarge-v2\", # params 235M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"bert-large-uncased\": {\n",
    "        \"tokenizer_class\": BertTokenizer,\n",
    "        \"pretrained_model_name\": \"bert-large-uncased\", # params 340M\n",
    "        \"model_class\": BertForSequenceClassification\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a3199a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Metric function ----\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba905e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Weighted Trainer ----\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc761c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Function to train and evaluate ----\n",
    "def train_and_evaluate(model_name, train_dataset, test_dataset, data, max_length):\n",
    "    print(f\"\\n===== Training {model_name} =====\")\n",
    "\n",
    "    # Model + tokenizer\n",
    "    cfg = MODEL_CONFIGS[model_name]\n",
    "    tokenizer = cfg[\"tokenizer_class\"].from_pretrained(cfg[\"pretrained_model_name\"])\n",
    "    model = cfg[\"model_class\"].from_pretrained(\n",
    "        cfg[\"pretrained_model_name\"],\n",
    "        num_labels=len(data['label'].unique())\n",
    "    )\n",
    "\n",
    "    # Tokenization\n",
    "    def tokenize_fn(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "    train_enc = train_dataset.map(tokenize_fn, batched=True)\n",
    "    test_enc = test_dataset.map(tokenize_fn, batched=True)\n",
    "    train_enc = train_enc.rename_column(\"label\", \"labels\")\n",
    "    test_enc = test_enc.rename_column(\"label\", \"labels\")\n",
    "    train_enc.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    test_enc.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    # Training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{CONFIG['output_dir']}/{model_name}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "        per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "        num_train_epochs=CONFIG[\"epochs\"],\n",
    "        weight_decay=CONFIG[\"weight_decay\"],\n",
    "        logging_dir=f\"./logs_{model_name}\",\n",
    "        report_to=\"none\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = WeightedLossTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_enc,\n",
    "        eval_dataset=test_enc,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "\n",
    "    # Train + evaluate\n",
    "    # time to train\n",
    "    t_train_start = torch.cuda.Event(enable_timing=True)\n",
    "    t_train_end = torch.cuda.Event(enable_timing=True)\n",
    "    t_train_start.record()\n",
    "    trainer.train()\n",
    "    t_train_end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    t_train_time = t_train_start.elapsed_time(t_train_end) / 1000  # convert to seconds\n",
    "    print(f\"Training time for {model_name}: {t_train_time:.2f} seconds\")\n",
    "\n",
    "    # Predictions\n",
    "    t_pred_start = torch.cuda.Event(enable_timing=True)\n",
    "    t_pred_end = torch.cuda.Event(enable_timing=True)\n",
    "    t_pred_start.record()\n",
    "    preds = trainer.predict(test_enc)\n",
    "    t_pred_end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    t_pred_time = t_pred_start.elapsed_time(t_pred_end) / 1000  # convert to seconds\n",
    "    print(f\"Prediction time for {model_name}: {t_pred_time:.2f} seconds\")\n",
    "    y_true = preds.label_ids\n",
    "    y_pred = np.argmax(preds.predictions, axis=1)\n",
    "\n",
    "    print(f\"\\nClassification Report for {model_name}:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "    performance = {\n",
    "        \"model\": model_name,\n",
    "        \"max_length\": max_length,\n",
    "        \"train_time_sec\": t_train_time,\n",
    "        \"pred_time_sec\": t_pred_time,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1\": precision_recall_fscore_support(y_true, y_pred, average='weighted')[2],\n",
    "        \"precision\": precision_recall_fscore_support(y_true, y_pred, average='weighted')[0],\n",
    "        \"recall\": precision_recall_fscore_support(y_true, y_pred, average='weighted')[1],\n",
    "    }\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95403fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping deberta with max_length=128 (already in CSV).\n",
      "Skipping deberta with max_length=256 (already in CSV).\n",
      "Skipping deberta with max_length=512 (already in CSV).\n",
      "Skipping distilbert with max_length=128 (already in CSV).\n",
      "Skipping distilbert with max_length=256 (already in CSV).\n",
      "Skipping distilbert with max_length=512 (already in CSV).\n",
      "Skipping bert with max_length=128 (already in CSV).\n",
      "Skipping bert with max_length=256 (already in CSV).\n",
      "Skipping bert with max_length=512 (already in CSV).\n",
      "Skipping roberta with max_length=128 (already in CSV).\n",
      "Skipping roberta with max_length=256 (already in CSV).\n",
      "Skipping roberta with max_length=512 (already in CSV).\n",
      "Skipping electra with max_length=128 (already in CSV).\n",
      "Skipping electra with max_length=256 (already in CSV).\n",
      "Skipping electra with max_length=512 (already in CSV).\n",
      "Skipping albert with max_length=128 (already in CSV).\n",
      "Skipping albert with max_length=256 (already in CSV).\n",
      "Skipping albert with max_length=512 (already in CSV).\n",
      "Skipping xlnet with max_length=128 (already in CSV).\n",
      "Skipping xlnet with max_length=256 (already in CSV).\n",
      "Skipping xlnet with max_length=512 (already in CSV).\n",
      "Skipping mobilebert with max_length=128 (already in CSV).\n",
      "Skipping mobilebert with max_length=256 (already in CSV).\n",
      "Skipping mobilebert with max_length=512 (already in CSV).\n",
      "Skipping albert-base-v1 with max_length=128 (already in CSV).\n",
      "Skipping albert-base-v1 with max_length=256 (already in CSV).\n",
      "Skipping albert-base-v1 with max_length=512 (already in CSV).\n",
      "Skipping albert-large-v2 with max_length=128 (already in CSV).\n",
      "Skipping albert-large-v2 with max_length=256 (already in CSV).\n",
      "Skipping albert-large-v2 with max_length=512 (already in CSV).\n",
      "Skipping albert-xlarge-v2 with max_length=128 (already in CSV).\n",
      "Skipping albert-xlarge-v2 with max_length=256 (already in CSV).\n",
      "\n",
      "--- Training albert-xlarge-v2 with max_length=512 ---\n",
      "\n",
      "===== Training albert-xlarge-v2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at albert-xlarge-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-xlarge-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ae84a1abd440edb81d7a3737f152f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7167b5cac2864f6ba1b1a589b93bf56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a73a7b0948443e8578caa353b4f509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc7cdc2294d4366893de938a496a015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6895041465759277, 'eval_accuracy': 0.8706548100242523, 'eval_f1': 0.8104539588557645, 'eval_precision': 0.7580397982183668, 'eval_recall': 0.8706548100242523, 'eval_runtime': 1992.071, 'eval_samples_per_second': 0.621, 'eval_steps_per_second': 0.039, 'epoch': 1.0}\n",
      "{'loss': 0.7273, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6830c5d849cd4518a59cd5a271e63964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.692097008228302, 'eval_accuracy': 0.8706548100242523, 'eval_f1': 0.8104539588557645, 'eval_precision': 0.7580397982183668, 'eval_recall': 0.8706548100242523, 'eval_runtime': 1996.6732, 'eval_samples_per_second': 0.62, 'eval_steps_per_second': 0.039, 'epoch': 2.0}\n",
      "{'train_runtime': 26934.4514, 'train_samples_per_second': 0.367, 'train_steps_per_second': 0.023, 'train_loss': 0.7234828333700857, 'epoch': 2.0}\n",
      "Training time for albert-xlarge-v2: 26935.85 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f4f9620bea439090ef6fae5431124f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-xlarge-v2: 1981.44 seconds\n",
      "\n",
      "Classification Report for albert-xlarge-v2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       160\n",
      "           1     0.8707    1.0000    0.9309      1077\n",
      "\n",
      "    accuracy                         0.8707      1237\n",
      "   macro avg     0.4353    0.5000    0.4654      1237\n",
      "weighted avg     0.7580    0.8707    0.8105      1237\n",
      "\n",
      "{'model': 'albert-xlarge-v2', 'max_length': 512, 'train_time_sec': 26935.85, 'pred_time_sec': 1981.443125, 'accuracy': 0.8706548100242523, 'f1': 0.8104539588557645, 'precision': 0.7580397982183668, 'recall': 0.8706548100242523}\n",
      "\n",
      "--- Training albert-xxlarge-v2 with max_length=128 ---\n",
      "\n",
      "===== Training albert-xxlarge-v2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-xxlarge-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-xxlarge-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179e616d337247e4bd92bf69eed9e7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a292821bd710451d90ed9b64ce15b997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bbd26dee814f4ea442f033baa65d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ca53f815cc41169eae35cd3f04dd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44189542531967163, 'eval_accuracy': 0.8900565885206144, 'eval_f1': 0.9003244451057969, 'eval_precision': 0.9244461759977711, 'eval_recall': 0.8900565885206144, 'eval_runtime': 2033.7023, 'eval_samples_per_second': 0.608, 'eval_steps_per_second': 0.038, 'epoch': 1.0}\n",
      "{'loss': 0.4906, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e786db98384a435c8b4985a73a8d57a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33212143182754517, 'eval_accuracy': 0.9611964430072757, 'eval_f1': 0.9598371195343344, 'eval_precision': 0.9601189119527814, 'eval_recall': 0.9611964430072757, 'eval_runtime': 2029.5978, 'eval_samples_per_second': 0.609, 'eval_steps_per_second': 0.038, 'epoch': 2.0}\n",
      "{'train_runtime': 32451.0065, 'train_samples_per_second': 0.305, 'train_steps_per_second': 0.019, 'train_loss': 0.4589925150717458, 'epoch': 2.0}\n",
      "Training time for albert-xxlarge-v2: 32450.98 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396ee454f6054e61a4306089719d856f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-xxlarge-v2: 2035.35 seconds\n",
      "\n",
      "Classification Report for albert-xxlarge-v2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9118    0.7750    0.8378       160\n",
      "           1     0.9673    0.9889    0.9780      1077\n",
      "\n",
      "    accuracy                         0.9612      1237\n",
      "   macro avg     0.9395    0.8819    0.9079      1237\n",
      "weighted avg     0.9601    0.9612    0.9598      1237\n",
      "\n",
      "{'model': 'albert-xxlarge-v2', 'max_length': 128, 'train_time_sec': 32450.976, 'pred_time_sec': 2035.3515, 'accuracy': 0.9611964430072757, 'f1': 0.9598371195343344, 'precision': 0.9601189119527814, 'recall': 0.9611964430072757}\n",
      "\n",
      "--- Training albert-xxlarge-v2 with max_length=256 ---\n",
      "\n",
      "===== Training albert-xxlarge-v2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at albert-xxlarge-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-xxlarge-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01a0159f1eb48a1adc019003a8ad208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c1d005c0614f3e9908eebf602b2f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842770bea7264317a28a380ef0725f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42ada4dcf694e0ab10655d0deab6631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42707252502441406, 'eval_accuracy': 0.9652384801940178, 'eval_f1': 0.9636207000414232, 'eval_precision': 0.9649097316821669, 'eval_recall': 0.9652384801940178, 'eval_runtime': 4290.3132, 'eval_samples_per_second': 0.288, 'eval_steps_per_second': 0.018, 'epoch': 1.0}\n",
      "{'loss': 0.4781, 'learning_rate': 9.67741935483871e-06, 'epoch': 1.61}\n"
     ]
    }
   ],
   "source": [
    "for model_name in MODEL_CONFIGS.keys():\n",
    "    for max_len in CONFIG[\"max_length\"]:\n",
    "        # skip if this (model, max_len) already exists in CSV\n",
    "        if os.path.exists(\"model_performance.csv\") and f\"{model_name},{max_len},\" in open(\"model_performance.csv\").read():\n",
    "            print(f\"Skipping {model_name} with max_length={max_len} (already in CSV).\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Training {model_name} with max_length={max_len} ---\")\n",
    "        performance = train_and_evaluate(model_name, train_dataset, test_dataset, data, max_len)\n",
    "        print(performance)\n",
    "        # save performance to a csv file\n",
    "        with open(\"model_performance.csv\", \"a\") as f:\n",
    "            #headers if file is empty\n",
    "            if os.stat(\"model_performance.csv\").st_size == 0:\n",
    "                f.write(\"model,max_length,train_time_sec,pred_time_sec,accuracy,f1,precision,recall\\n\")\n",
    "            f.write(\",\".join([str(performance[k]) for k in performance]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f468f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba7b36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-forge-Copy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
