{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a903082",
   "metadata": {},
   "source": [
    "#### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f725fd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53.2\n",
      "<class 'transformers.training_args.TrainingArguments'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "import logging\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    # tokenizers\n",
    "    AutoTokenizer,\n",
    "    DebertaV2Tokenizer,\n",
    "    DistilBertTokenizer,\n",
    "    BertTokenizer,\n",
    "    RobertaTokenizer,\n",
    "    ElectraTokenizer,\n",
    "    AlbertTokenizer,\n",
    "    XLNetTokenizer,\n",
    "    MobileBertTokenizer,\n",
    "    # models\n",
    "    DebertaV2ForSequenceClassification,\n",
    "    DistilBertForSequenceClassification,\n",
    "    BertForSequenceClassification,\n",
    "    RobertaForSequenceClassification,\n",
    "    ElectraForSequenceClassification,\n",
    "    AlbertForSequenceClassification,\n",
    "    XLNetForSequenceClassification,\n",
    "    MobileBertForSequenceClassification,\n",
    ")\n",
    "from torch.nn import CrossEntropyLoss\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "print(transformers.TrainingArguments)\n",
    "\n",
    "# Cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eadf70b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='classification_nvidia.log', level=logging.INFO)\n",
    "logging.info(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f82b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31b61f66c8947818e235b5623fb6f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: huggingface_cache\n",
      "TRANSFORMERS_CACHE: huggingface_cache\n",
      "HUGGINGFACE_HUB_CACHE: huggingface_cache\n"
     ]
    }
   ],
   "source": [
    "# setting huggingface token\n",
    "login(token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"huggingface_cache\" \n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"huggingface_cache\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"huggingface_cache\"\n",
    "\n",
    "print(\"HF_HOME:\", os.getenv(\"HF_HOME\"))\n",
    "print(\"TRANSFORMERS_CACHE:\", os.getenv(\"TRANSFORMERS_CACHE\"))\n",
    "print(\"HUGGINGFACE_HUB_CACHE:\", os.getenv(\"HUGGINGFACE_HUB_CACHE\"))\n",
    "\n",
    "logging.info(f\"HF_HOME: {os.getenv('HF_HOME')}\")\n",
    "logging.info(f\"TRANSFORMERS_CACHE: {os.getenv('TRANSFORMERS_CACHE')}\")\n",
    "logging.info(f\"HUGGINGFACE_HUB_CACHE: {os.getenv('HUGGINGFACE_HUB_CACHE')}\")\n",
    "\n",
    "transformers.utils.hub.TRANSFORMERS_CACHE = \"huggingface_cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e30cc",
   "metadata": {},
   "source": [
    "### LOADING SQLITE DB WITH RECORDS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c002d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to JSONL file.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "DB_FILE = \"chunks.db\"\n",
    "OUTPUT_FILE = \"exported_chunks.jsonl\"\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Query all data from chunks table\n",
    "cur.execute(\"SELECT text, label FROM chunks\")\n",
    "rows = cur.fetchall()\n",
    "\n",
    "# Write to JSONL\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for text, label in rows:\n",
    "        obj = {\"text\": text}\n",
    "        if label is not None:\n",
    "            obj[\"label\"] = label\n",
    "        f.write(json.dumps(obj) + \"\\n\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"Data exported to JSONL file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47d48089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled chunks after removing label 11: {1: 8199, 0: 800}\n",
      "Final labeled chunks: {1: 5384, 0: 800}\n"
     ]
    }
   ],
   "source": [
    "# Load the labeled chunks\n",
    "with open(\"exported_chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    labeled_chunks = [json.loads(line) for line in f]\n",
    "\n",
    "data = pd.DataFrame(labeled_chunks)\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "\n",
    "# Get the first 9000 rows\n",
    "data = data.head(9000)\n",
    "\n",
    "# Remove rows with label == 11\n",
    "data = data[data['label'] != 11]\n",
    "\n",
    "# Print labeled count after removing label 11\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "print(f\"Labeled chunks after removing label 11: {labeled_count}\")\n",
    "\n",
    "# Remove rows where label == 1 and text length < 100\n",
    "data = data[~((data['label'] == 1) & (data['text'].str.len() < 100))]\n",
    "\n",
    "# Print final labeled count\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "print(f\"Final labeled chunks: {labeled_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ca564",
   "metadata": {},
   "source": [
    "##### Spliting data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f9aa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([3.8648, 0.5743], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "data['label'] = data['label'].astype(int)\n",
    "\n",
    "# Train-Test Split using stratified sampling\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, stratify=data['label'], random_state=42)\n",
    "\n",
    "# since there is a class imbalance, we will compute class weights\n",
    "# to handle this in the loss function\n",
    "labels = train_df[\"label\"].values\n",
    "# Compute class weights\n",
    "classes = np.unique(labels)\n",
    "weights = compute_class_weight(class_weight=\"balanced\",\n",
    "                            classes=classes,\n",
    "                            y=labels)\n",
    "class_weights = torch.tensor(weights, dtype=torch.float, device=device)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# Convert ing the DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a629272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Tuning parameters ----\n",
    "CONFIG = {\n",
    "    \"epochs\": 2,\n",
    "    \"batch_size\": 16,\n",
    "    \"batch_size_larger\": 8, # Batch size for larger models\n",
    "    \"max_length\": [128, 256, 512], # Max length of input sequences\n",
    "    \"learning_rate\": 5e-5, # Learning rate for the optimizer\n",
    "    \"learning_rate_larger\": 3e-5, # Learning rate for larger models\n",
    "    \"weight_decay\": 0.01, # Weight decay for regularization\n",
    "    \"output_dir\": \"huggingface_cache/classification_models\"\n",
    "}\n",
    "\n",
    "# ---- Model configurations ----\n",
    "MODEL_CONFIGS = {\n",
    "    \"deberta\": {\n",
    "        \"tokenizer_class\": DebertaV2Tokenizer,\n",
    "        \"pretrained_model_name\": \"microsoft/deberta-v3-small\", # params 55M\n",
    "        \"model_class\": DebertaV2ForSequenceClassification\n",
    "    },\n",
    "    \"distilbert\": {\n",
    "        \"tokenizer_class\": DistilBertTokenizer,\n",
    "        \"pretrained_model_name\": \"distilbert-base-uncased\", # params 66M\n",
    "        \"model_class\": DistilBertForSequenceClassification\n",
    "    },\n",
    "    \"bert\": {\n",
    "        \"tokenizer_class\": BertTokenizer,\n",
    "        \"pretrained_model_name\": \"bert-base-uncased\", # params 110M\n",
    "        \"model_class\": BertForSequenceClassification\n",
    "    },\n",
    "    \"roberta\": {\n",
    "        \"tokenizer_class\": RobertaTokenizer,\n",
    "        \"pretrained_model_name\": \"roberta-base\", # params 125M\n",
    "        \"model_class\": RobertaForSequenceClassification\n",
    "    },\n",
    "    \"electra\": {\n",
    "        \"tokenizer_class\": ElectraTokenizer,\n",
    "        \"pretrained_model_name\": \"google/electra-small-discriminator\", # params 14M\n",
    "        \"model_class\": ElectraForSequenceClassification\n",
    "    },\n",
    "    \"albert\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-base-v2\", # params 11M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"xlnet\": {\n",
    "        \"tokenizer_class\": XLNetTokenizer,\n",
    "        \"pretrained_model_name\": \"xlnet-base-cased\", # params 110M\n",
    "        \"model_class\": XLNetForSequenceClassification\n",
    "    },\n",
    "    \"mobilebert\": {\n",
    "        \"tokenizer_class\": AutoTokenizer,\n",
    "        \"pretrained_model_name\": \"google/mobilebert-uncased\", # params 25M\n",
    "        \"model_class\": MobileBertForSequenceClassification\n",
    "    },\n",
    "    \"albert-base-v1\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-base-v1\", # params 12M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"albert-large-v2\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-large-v2\", # params 18M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"albert-xlarge-v2\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-xlarge-v2\", # params 60M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"albert-xxlarge-v2\": {\n",
    "        \"tokenizer_class\": AlbertTokenizer,\n",
    "        \"pretrained_model_name\": \"albert-xxlarge-v2\", # params 235M\n",
    "        \"model_class\": AlbertForSequenceClassification\n",
    "    },\n",
    "    \"bert-large-uncased\": {\n",
    "        \"tokenizer_class\": BertTokenizer,\n",
    "        \"pretrained_model_name\": \"bert-large-uncased\", # params 340M\n",
    "        \"model_class\": BertForSequenceClassification\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a3199a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Metric function ----\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba905e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Weighted Trainer ----\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc761c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Function to train and evaluate ----\n",
    "def train_and_evaluate(model_name, train_dataset, test_dataset, data, max_length):\n",
    "    print(f\"\\n===== Training {model_name} =====\")\n",
    "\n",
    "    # Model + tokenizer\n",
    "    cfg = MODEL_CONFIGS[model_name]\n",
    "    tokenizer = cfg[\"tokenizer_class\"].from_pretrained(cfg[\"pretrained_model_name\"])\n",
    "    model = cfg[\"model_class\"].from_pretrained(\n",
    "        cfg[\"pretrained_model_name\"],\n",
    "        num_labels=len(data['label'].unique())\n",
    "    )\n",
    "\n",
    "    # Tokenization\n",
    "    def tokenize_fn(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "    train_enc = train_dataset.map(tokenize_fn, batched=True)\n",
    "    test_enc = test_dataset.map(tokenize_fn, batched=True)\n",
    "    train_enc = train_enc.rename_column(\"label\", \"labels\")\n",
    "    test_enc = test_enc.rename_column(\"label\", \"labels\")\n",
    "    train_enc.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    test_enc.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    # Training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{CONFIG['output_dir']}/{model_name}\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "        per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "        num_train_epochs=CONFIG[\"epochs\"],\n",
    "        weight_decay=CONFIG[\"weight_decay\"],\n",
    "        logging_dir=f\"./logs_{model_name}\",\n",
    "        report_to=\"none\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = WeightedLossTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_enc,\n",
    "        eval_dataset=test_enc,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "\n",
    "    # Train + evaluate\n",
    "    # time to train\n",
    "    t_train_start = torch.cuda.Event(enable_timing=True)\n",
    "    t_train_end = torch.cuda.Event(enable_timing=True)\n",
    "    t_train_start.record()\n",
    "    trainer.train()\n",
    "    t_train_end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    t_train_time = t_train_start.elapsed_time(t_train_end) / 1000  # convert to seconds\n",
    "    print(f\"Training time for {model_name}: {t_train_time:.2f} seconds\")\n",
    "\n",
    "    # Predictions\n",
    "    t_pred_start = torch.cuda.Event(enable_timing=True)\n",
    "    t_pred_end = torch.cuda.Event(enable_timing=True)\n",
    "    t_pred_start.record()\n",
    "    preds = trainer.predict(test_enc)\n",
    "    t_pred_end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    t_pred_time = t_pred_start.elapsed_time(t_pred_end) / 1000  # convert to seconds\n",
    "    print(f\"Prediction time for {model_name}: {t_pred_time:.2f} seconds\")\n",
    "    y_true = preds.label_ids\n",
    "    y_pred = np.argmax(preds.predictions, axis=1)\n",
    "\n",
    "    print(f\"\\nClassification Report for {model_name}:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "    performance = {\n",
    "        \"model\": model_name,\n",
    "        \"max_length\": max_length,\n",
    "        \"train_time_sec\": t_train_time,\n",
    "        \"pred_time_sec\": t_pred_time,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1\": precision_recall_fscore_support(y_true, y_pred, average='weighted')[2],\n",
    "        \"precision\": precision_recall_fscore_support(y_true, y_pred, average='weighted')[0],\n",
    "        \"recall\": precision_recall_fscore_support(y_true, y_pred, average='weighted')[1],\n",
    "    }\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95403fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training deberta with max_length=128 ---\n",
      "\n",
      "===== Training deberta =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26c778ca8a248a6ba003d6830927cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2afc596369e4c2792cf41446e39bd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 07:55, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.322136</td>\n",
       "      <td>0.969281</td>\n",
       "      <td>0.968302</td>\n",
       "      <td>0.968735</td>\n",
       "      <td>0.969281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.283200</td>\n",
       "      <td>0.284359</td>\n",
       "      <td>0.974131</td>\n",
       "      <td>0.973773</td>\n",
       "      <td>0.973694</td>\n",
       "      <td>0.974131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for deberta: 477.04 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for deberta: 13.01 seconds\n",
      "\n",
      "Classification Report for deberta:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9267    0.8688    0.8968       160\n",
      "           1     0.9807    0.9898    0.9852      1077\n",
      "\n",
      "    accuracy                         0.9741      1237\n",
      "   macro avg     0.9537    0.9293    0.9410      1237\n",
      "weighted avg     0.9737    0.9741    0.9738      1237\n",
      "\n",
      "{'model': 'deberta', 'max_length': 128, 'train_time_sec': 477.0391875, 'pred_time_sec': 13.0142890625, 'accuracy': 0.9741309620048505, 'f1': 0.9737734908012624, 'precision': 0.9736943079539011, 'recall': 0.9741309620048505}\n",
      "\n",
      "--- Training deberta with max_length=256 ---\n",
      "\n",
      "===== Training deberta =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fb04aa4b4c452285f3c96c31ba761d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae0fe7e33764cd395491ed5f690f12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 13:48, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.376166</td>\n",
       "      <td>0.972514</td>\n",
       "      <td>0.971463</td>\n",
       "      <td>0.972458</td>\n",
       "      <td>0.972514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.300600</td>\n",
       "      <td>0.308724</td>\n",
       "      <td>0.976556</td>\n",
       "      <td>0.976061</td>\n",
       "      <td>0.976215</td>\n",
       "      <td>0.976556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for deberta: 830.52 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for deberta: 28.20 seconds\n",
      "\n",
      "Classification Report for deberta:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9517    0.8625    0.9049       160\n",
      "           1     0.9799    0.9935    0.9866      1077\n",
      "\n",
      "    accuracy                         0.9766      1237\n",
      "   macro avg     0.9658    0.9280    0.9458      1237\n",
      "weighted avg     0.9762    0.9766    0.9761      1237\n",
      "\n",
      "{'model': 'deberta', 'max_length': 256, 'train_time_sec': 830.5245, 'pred_time_sec': 28.2044921875, 'accuracy': 0.9765561843168957, 'f1': 0.9760607614155817, 'precision': 0.9762150847786284, 'recall': 0.9765561843168957}\n",
      "\n",
      "--- Training deberta with max_length=512 ---\n",
      "\n",
      "===== Training deberta =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841c07ebc1874d8b889ed7b77e584f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f22345fa58d4ca0b4bf635827ef6c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 31:13, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.325287</td>\n",
       "      <td>0.974131</td>\n",
       "      <td>0.973388</td>\n",
       "      <td>0.973821</td>\n",
       "      <td>0.974131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.258200</td>\n",
       "      <td>0.306613</td>\n",
       "      <td>0.977365</td>\n",
       "      <td>0.976714</td>\n",
       "      <td>0.977256</td>\n",
       "      <td>0.977365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for deberta: 1877.73 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for deberta: 79.17 seconds\n",
      "\n",
      "Classification Report for deberta:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9714    0.8500    0.9067       160\n",
      "           1     0.9781    0.9963    0.9871      1077\n",
      "\n",
      "    accuracy                         0.9774      1237\n",
      "   macro avg     0.9748    0.9231    0.9469      1237\n",
      "weighted avg     0.9773    0.9774    0.9767      1237\n",
      "\n",
      "{'model': 'deberta', 'max_length': 512, 'train_time_sec': 1877.734375, 'pred_time_sec': 79.1731484375, 'accuracy': 0.9773645917542442, 'f1': 0.9767141968592342, 'precision': 0.9772563689588809, 'recall': 0.9773645917542442}\n",
      "\n",
      "--- Training distilbert with max_length=128 ---\n",
      "\n",
      "===== Training distilbert =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94bf0a22a3349f9a88888bf3d83f01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85536e5ef7b4bcba6333920c2c40189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ed049416cc487fba9c1d19da9a1ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f957e2270804d55bebfe850f925c07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da6995285a04402a0ffbf8a868b7aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d88cf8fcc5441395eab9304e06c52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58b0967f0be4190a0a084b3aa1bb3e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 04:07, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.250081</td>\n",
       "      <td>0.974131</td>\n",
       "      <td>0.973699</td>\n",
       "      <td>0.973671</td>\n",
       "      <td>0.974131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.234100</td>\n",
       "      <td>0.247255</td>\n",
       "      <td>0.974939</td>\n",
       "      <td>0.974769</td>\n",
       "      <td>0.974668</td>\n",
       "      <td>0.974939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for distilbert: 248.81 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for distilbert: 8.01 seconds\n",
      "\n",
      "Classification Report for distilbert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9161    0.8875    0.9016       160\n",
      "           1     0.9834    0.9879    0.9856      1077\n",
      "\n",
      "    accuracy                         0.9749      1237\n",
      "   macro avg     0.9497    0.9377    0.9436      1237\n",
      "weighted avg     0.9747    0.9749    0.9748      1237\n",
      "\n",
      "{'model': 'distilbert', 'max_length': 128, 'train_time_sec': 248.805546875, 'pred_time_sec': 8.0120302734375, 'accuracy': 0.9749393694421988, 'f1': 0.9747694943429517, 'precision': 0.9746676026345088, 'recall': 0.9749393694421988}\n",
      "\n",
      "--- Training distilbert with max_length=256 ---\n",
      "\n",
      "===== Training distilbert =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf21ed3643a4e8e8ea5b17ab42360a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c673e0099e43e5a9bed7df16ce6a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 06:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.264911</td>\n",
       "      <td>0.976556</td>\n",
       "      <td>0.975919</td>\n",
       "      <td>0.976348</td>\n",
       "      <td>0.976556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.238400</td>\n",
       "      <td>0.279296</td>\n",
       "      <td>0.974939</td>\n",
       "      <td>0.974484</td>\n",
       "      <td>0.974508</td>\n",
       "      <td>0.974939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for distilbert: 392.08 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for distilbert: 13.07 seconds\n",
      "\n",
      "Classification Report for distilbert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9645    0.8500    0.9037       160\n",
      "           1     0.9781    0.9954    0.9867      1077\n",
      "\n",
      "    accuracy                         0.9766      1237\n",
      "   macro avg     0.9713    0.9227    0.9452      1237\n",
      "weighted avg     0.9763    0.9766    0.9759      1237\n",
      "\n",
      "{'model': 'distilbert', 'max_length': 256, 'train_time_sec': 392.075125, 'pred_time_sec': 13.0747568359375, 'accuracy': 0.9765561843168957, 'f1': 0.9759187557453016, 'precision': 0.9763478573394604, 'recall': 0.9765561843168957}\n",
      "\n",
      "--- Training distilbert with max_length=512 ---\n",
      "\n",
      "===== Training distilbert =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e175d058c02c41ff8cfd22b62c0805b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108ba161d49e440990ea6290e4647fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 12:12, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.272270</td>\n",
       "      <td>0.978173</td>\n",
       "      <td>0.977580</td>\n",
       "      <td>0.978060</td>\n",
       "      <td>0.978173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.248100</td>\n",
       "      <td>0.282182</td>\n",
       "      <td>0.978981</td>\n",
       "      <td>0.978442</td>\n",
       "      <td>0.978864</td>\n",
       "      <td>0.978981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for distilbert: 733.91 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for distilbert: 26.81 seconds\n",
      "\n",
      "Classification Report for distilbert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9718    0.8625    0.9139       160\n",
      "           1     0.9799    0.9963    0.9880      1077\n",
      "\n",
      "    accuracy                         0.9790      1237\n",
      "   macro avg     0.9759    0.9294    0.9510      1237\n",
      "weighted avg     0.9789    0.9790    0.9784      1237\n",
      "\n",
      "{'model': 'distilbert', 'max_length': 512, 'train_time_sec': 733.914625, 'pred_time_sec': 26.807705078125, 'accuracy': 0.978981406628941, 'f1': 0.9784421183262277, 'precision': 0.9788638654665281, 'recall': 0.978981406628941}\n",
      "Skipping bert with max_length=128 (already in CSV).\n",
      "Skipping bert with max_length=256 (already in CSV).\n",
      "Skipping bert with max_length=512 (already in CSV).\n",
      "\n",
      "--- Training roberta with max_length=128 ---\n",
      "\n",
      "===== Training roberta =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc101c222c214c73ae7f0d3069a00531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ff581e0f8b472dac361ea39850388b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a7a8b866784944af56de503d9c95ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d0f37bc8d3424c872acaacaecbe8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e19b2b30f547d1bff626c6a2d20132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bd29063756432a9ee47ba5c25f7fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a149b9876f2f4905a8c78fb9c8f9e645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c77c31dafa47118088951f29d64cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 07:40, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.421690</td>\n",
       "      <td>0.967664</td>\n",
       "      <td>0.966104</td>\n",
       "      <td>0.967695</td>\n",
       "      <td>0.967664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.353500</td>\n",
       "      <td>0.282394</td>\n",
       "      <td>0.976556</td>\n",
       "      <td>0.976266</td>\n",
       "      <td>0.976202</td>\n",
       "      <td>0.976556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for roberta: 461.77 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for roberta: 13.71 seconds\n",
      "\n",
      "Classification Report for roberta:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9338    0.8812    0.9068       160\n",
      "           1     0.9825    0.9907    0.9866      1077\n",
      "\n",
      "    accuracy                         0.9766      1237\n",
      "   macro avg     0.9581    0.9360    0.9467      1237\n",
      "weighted avg     0.9762    0.9766    0.9763      1237\n",
      "\n",
      "{'model': 'roberta', 'max_length': 128, 'train_time_sec': 461.76853125, 'pred_time_sec': 13.706326171875, 'accuracy': 0.9765561843168957, 'f1': 0.9762657386605518, 'precision': 0.9762016427433062, 'recall': 0.9765561843168957}\n",
      "\n",
      "--- Training roberta with max_length=256 ---\n",
      "\n",
      "===== Training roberta =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acaf6ce4c0a848ac9ae8e4a6c289ce9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c83e8911704a1291b89e74bb33e304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 12:35, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.478067</td>\n",
       "      <td>0.970089</td>\n",
       "      <td>0.968697</td>\n",
       "      <td>0.970275</td>\n",
       "      <td>0.970089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.338600</td>\n",
       "      <td>0.263430</td>\n",
       "      <td>0.979790</td>\n",
       "      <td>0.979423</td>\n",
       "      <td>0.979544</td>\n",
       "      <td>0.979790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for roberta: 757.37 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for roberta: 24.29 seconds\n",
      "\n",
      "Classification Report for roberta:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9592    0.8812    0.9186       160\n",
      "           1     0.9826    0.9944    0.9885      1077\n",
      "\n",
      "    accuracy                         0.9798      1237\n",
      "   macro avg     0.9709    0.9378    0.9535      1237\n",
      "weighted avg     0.9795    0.9798    0.9794      1237\n",
      "\n",
      "{'model': 'roberta', 'max_length': 256, 'train_time_sec': 757.3684375, 'pred_time_sec': 24.287298828125, 'accuracy': 0.9797898140662894, 'f1': 0.9794225323382386, 'precision': 0.9795440527564181, 'recall': 0.9797898140662894}\n",
      "\n",
      "--- Training roberta with max_length=512 ---\n",
      "\n",
      "===== Training roberta =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f97ff5faff8402dbf75c8ab235e1f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b113d41ddaf40418654e9e79eb69092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 24:07, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.298596</td>\n",
       "      <td>0.972514</td>\n",
       "      <td>0.971724</td>\n",
       "      <td>0.972104</td>\n",
       "      <td>0.972514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.310300</td>\n",
       "      <td>0.277755</td>\n",
       "      <td>0.980598</td>\n",
       "      <td>0.980274</td>\n",
       "      <td>0.980365</td>\n",
       "      <td>0.980598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for roberta: 1449.71 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for roberta: 50.86 seconds\n",
      "\n",
      "Classification Report for roberta:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9595    0.8875    0.9221       160\n",
      "           1     0.9835    0.9944    0.9889      1077\n",
      "\n",
      "    accuracy                         0.9806      1237\n",
      "   macro avg     0.9715    0.9410    0.9555      1237\n",
      "weighted avg     0.9804    0.9806    0.9803      1237\n",
      "\n",
      "{'model': 'roberta', 'max_length': 512, 'train_time_sec': 1449.71025, 'pred_time_sec': 50.8598359375, 'accuracy': 0.9805982215036378, 'f1': 0.9802740093184538, 'precision': 0.9803652874866222, 'recall': 0.9805982215036378}\n",
      "\n",
      "--- Training electra with max_length=128 ---\n",
      "\n",
      "===== Training electra =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202f4d8250914c7c9b3c73d9939e35b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e130935c08841ed8e7b32df3c10bee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41186774711f4a96b9853658ac9c891d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879b7c24b35b4ebd942675f5eb581745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e58f76266d9414cb60e907b9f768cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/54.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad114e57a434cf680f4351a9bac8cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f8e1bf47c9466589aec383be3b76d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/54.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49081cba88d947b1a460b9317bfecf53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 01:44, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.293449</td>\n",
       "      <td>0.970897</td>\n",
       "      <td>0.969784</td>\n",
       "      <td>0.970704</td>\n",
       "      <td>0.970897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.273800</td>\n",
       "      <td>0.277040</td>\n",
       "      <td>0.971706</td>\n",
       "      <td>0.970936</td>\n",
       "      <td>0.971213</td>\n",
       "      <td>0.971706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for electra: 105.04 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for electra: 4.41 seconds\n",
      "\n",
      "Classification Report for electra:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9433    0.8313    0.8837       160\n",
      "           1     0.9754    0.9926    0.9839      1077\n",
      "\n",
      "    accuracy                         0.9717      1237\n",
      "   macro avg     0.9593    0.9119    0.9338      1237\n",
      "weighted avg     0.9712    0.9717    0.9709      1237\n",
      "\n",
      "{'model': 'electra', 'max_length': 128, 'train_time_sec': 105.0444609375, 'pred_time_sec': 4.405046875, 'accuracy': 0.9717057396928052, 'f1': 0.9709364293477776, 'precision': 0.9712126527963085, 'recall': 0.9717057396928052}\n",
      "\n",
      "--- Training electra with max_length=256 ---\n",
      "\n",
      "===== Training electra =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db7d22a48c442ecb84f65b3a10dd69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa53fc2a78654de2bf0b6932780225bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 03:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.284488</td>\n",
       "      <td>0.973323</td>\n",
       "      <td>0.972431</td>\n",
       "      <td>0.973117</td>\n",
       "      <td>0.973323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.276000</td>\n",
       "      <td>0.267624</td>\n",
       "      <td>0.975748</td>\n",
       "      <td>0.975126</td>\n",
       "      <td>0.975452</td>\n",
       "      <td>0.975748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for electra: 189.46 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for electra: 6.85 seconds\n",
      "\n",
      "Classification Report for electra:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9577    0.8500    0.9007       160\n",
      "           1     0.9781    0.9944    0.9862      1077\n",
      "\n",
      "    accuracy                         0.9757      1237\n",
      "   macro avg     0.9679    0.9222    0.9434      1237\n",
      "weighted avg     0.9755    0.9757    0.9751      1237\n",
      "\n",
      "{'model': 'electra', 'max_length': 256, 'train_time_sec': 189.4565, 'pred_time_sec': 6.84640966796875, 'accuracy': 0.9757477768795473, 'f1': 0.9751255211456473, 'precision': 0.9754518651314984, 'recall': 0.9757477768795473}\n",
      "\n",
      "--- Training electra with max_length=512 ---\n",
      "\n",
      "===== Training electra =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32827c3f508426ca7a503de74f5cba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4f56a1a4c948bc9c309b72295fcf07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 08:45, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.279696</td>\n",
       "      <td>0.970089</td>\n",
       "      <td>0.969276</td>\n",
       "      <td>0.969501</td>\n",
       "      <td>0.970089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.276900</td>\n",
       "      <td>0.308584</td>\n",
       "      <td>0.971706</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>0.971382</td>\n",
       "      <td>0.971706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for electra: 526.51 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for electra: 20.03 seconds\n",
      "\n",
      "Classification Report for electra:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9562    0.8187    0.8822       160\n",
      "           1     0.9736    0.9944    0.9839      1077\n",
      "\n",
      "    accuracy                         0.9717      1237\n",
      "   macro avg     0.9649    0.9066    0.9330      1237\n",
      "weighted avg     0.9714    0.9717    0.9708      1237\n",
      "\n",
      "{'model': 'electra', 'max_length': 512, 'train_time_sec': 526.513, 'pred_time_sec': 20.027138671875, 'accuracy': 0.9717057396928052, 'f1': 0.970759635088592, 'precision': 0.9713816203446165, 'recall': 0.9717057396928052}\n",
      "\n",
      "--- Training albert with max_length=128 ---\n",
      "\n",
      "===== Training albert =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079cea88658b4816bb1a02af9c523f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59ad850c5124fb4bd3c3ffdde8e6f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350e7f64c3b94604984ba15eca0559b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56706d8eeeb9474791662912966e5204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55ae0c2b06d47f9a6708b0a8013dd97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c0a1aad4264cea8dc5bf00f044b510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7aa8a23a5574d78952ff8edfc0da791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 07:58, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.541814</td>\n",
       "      <td>0.922393</td>\n",
       "      <td>0.908325</td>\n",
       "      <td>0.926186</td>\n",
       "      <td>0.922393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.513400</td>\n",
       "      <td>0.444349</td>\n",
       "      <td>0.941795</td>\n",
       "      <td>0.935867</td>\n",
       "      <td>0.941700</td>\n",
       "      <td>0.941795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for albert: 479.51 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert: 20.56 seconds\n",
      "\n",
      "Classification Report for albert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9400    0.5875    0.7231       160\n",
      "           1     0.9420    0.9944    0.9675      1077\n",
      "\n",
      "    accuracy                         0.9418      1237\n",
      "   macro avg     0.9410    0.7910    0.8453      1237\n",
      "weighted avg     0.9417    0.9418    0.9359      1237\n",
      "\n",
      "{'model': 'albert', 'max_length': 128, 'train_time_sec': 479.50625, 'pred_time_sec': 20.561255859375, 'accuracy': 0.9417946645109135, 'f1': 0.9358673544449516, 'precision': 0.9416999592596779, 'recall': 0.9417946645109135}\n",
      "\n",
      "--- Training albert with max_length=256 ---\n",
      "\n",
      "===== Training albert =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45711a004d0e4e9aa90667b817d03153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3c78b26d28419988c39353dfb14db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 15:24, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.349967</td>\n",
       "      <td>0.960388</td>\n",
       "      <td>0.960009</td>\n",
       "      <td>0.959757</td>\n",
       "      <td>0.960388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.449939</td>\n",
       "      <td>0.971706</td>\n",
       "      <td>0.970293</td>\n",
       "      <td>0.972313</td>\n",
       "      <td>0.971706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for albert: 926.66 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert: 38.97 seconds\n",
      "\n",
      "Classification Report for albert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9921    0.7875    0.8780       160\n",
      "           1     0.9694    0.9991    0.9840      1077\n",
      "\n",
      "    accuracy                         0.9717      1237\n",
      "   macro avg     0.9807    0.8933    0.9310      1237\n",
      "weighted avg     0.9723    0.9717    0.9703      1237\n",
      "\n",
      "{'model': 'albert', 'max_length': 256, 'train_time_sec': 926.660875, 'pred_time_sec': 38.9680546875, 'accuracy': 0.9717057396928052, 'f1': 0.9702925345470507, 'precision': 0.9723128280445642, 'recall': 0.9717057396928052}\n",
      "\n",
      "--- Training albert with max_length=512 ---\n",
      "\n",
      "===== Training albert =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a84c70cfeb2412eb6b50f33adfc3187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d7ba28a9364dc18ce09fc28d496047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 31:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.374696</td>\n",
       "      <td>0.970089</td>\n",
       "      <td>0.968896</td>\n",
       "      <td>0.969901</td>\n",
       "      <td>0.970089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.335500</td>\n",
       "      <td>0.366208</td>\n",
       "      <td>0.975748</td>\n",
       "      <td>0.975051</td>\n",
       "      <td>0.975539</td>\n",
       "      <td>0.975748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for albert: 1895.25 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert: 80.36 seconds\n",
      "\n",
      "Classification Report for albert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9643    0.8438    0.9000       160\n",
      "           1     0.9772    0.9954    0.9862      1077\n",
      "\n",
      "    accuracy                         0.9757      1237\n",
      "   macro avg     0.9707    0.9196    0.9431      1237\n",
      "weighted avg     0.9755    0.9757    0.9751      1237\n",
      "\n",
      "{'model': 'albert', 'max_length': 512, 'train_time_sec': 1895.24925, 'pred_time_sec': 80.358671875, 'accuracy': 0.9757477768795473, 'f1': 0.9750509252063224, 'precision': 0.9755388058204071, 'recall': 0.9757477768795473}\n",
      "\n",
      "--- Training xlnet with max_length=128 ---\n",
      "\n",
      "===== Training xlnet =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6206881ebb4277a6c975d314887ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb3be2d2dcd480ab60d000a138250af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd82f23f1df442219bb005845167757a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b954c0bf6dc43dab1eb57b8c4c596c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/467M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6623faedcec94e099ff1c313be102624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8f2ff2a31948bf978c8b73f296396c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/467M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf29aeecd2eb40e69f03e8cdc7a2e094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 09:55, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.381294</td>\n",
       "      <td>0.974939</td>\n",
       "      <td>0.973940</td>\n",
       "      <td>0.975184</td>\n",
       "      <td>0.974939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.359300</td>\n",
       "      <td>0.305158</td>\n",
       "      <td>0.977365</td>\n",
       "      <td>0.976643</td>\n",
       "      <td>0.977383</td>\n",
       "      <td>0.977365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for xlnet: 596.65 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for xlnet: 19.04 seconds\n",
      "\n",
      "Classification Report for xlnet:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9783    0.8438    0.9060       160\n",
      "           1     0.9773    0.9972    0.9871      1077\n",
      "\n",
      "    accuracy                         0.9774      1237\n",
      "   macro avg     0.9778    0.9205    0.9466      1237\n",
      "weighted avg     0.9774    0.9774    0.9766      1237\n",
      "\n",
      "{'model': 'xlnet', 'max_length': 128, 'train_time_sec': 596.6463125, 'pred_time_sec': 19.0395703125, 'accuracy': 0.9773645917542442, 'f1': 0.9766434818679605, 'precision': 0.9773825336212518, 'recall': 0.9773645917542442}\n",
      "\n",
      "--- Training xlnet with max_length=256 ---\n",
      "\n",
      "===== Training xlnet =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ec2872f7484d2687dbf0bc6a7b0702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5398978d94e4c21a59799b9718bb5ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 21:01, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.365717</td>\n",
       "      <td>0.965238</td>\n",
       "      <td>0.963009</td>\n",
       "      <td>0.966205</td>\n",
       "      <td>0.965238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.348400</td>\n",
       "      <td>0.263828</td>\n",
       "      <td>0.977365</td>\n",
       "      <td>0.976986</td>\n",
       "      <td>0.977018</td>\n",
       "      <td>0.977365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for xlnet: 1263.63 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for xlnet: 45.48 seconds\n",
      "\n",
      "Classification Report for xlnet:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9459    0.8750    0.9091       160\n",
      "           1     0.9816    0.9926    0.9871      1077\n",
      "\n",
      "    accuracy                         0.9774      1237\n",
      "   macro avg     0.9638    0.9338    0.9481      1237\n",
      "weighted avg     0.9770    0.9774    0.9770      1237\n",
      "\n",
      "{'model': 'xlnet', 'max_length': 256, 'train_time_sec': 1263.63075, 'pred_time_sec': 45.47715234375, 'accuracy': 0.9773645917542442, 'f1': 0.9769863442048629, 'precision': 0.9770183807811498, 'recall': 0.9773645917542442}\n",
      "\n",
      "--- Training xlnet with max_length=512 ---\n",
      "\n",
      "===== Training xlnet =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f973924387bc4455bf454dbc87d84703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755b0e5e459242eba4fe2fafef36892d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 54:10, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.461371</td>\n",
       "      <td>0.966855</td>\n",
       "      <td>0.965424</td>\n",
       "      <td>0.966523</td>\n",
       "      <td>0.966855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.393300</td>\n",
       "      <td>0.369984</td>\n",
       "      <td>0.975748</td>\n",
       "      <td>0.975126</td>\n",
       "      <td>0.975452</td>\n",
       "      <td>0.975748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for xlnet: 3257.27 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for xlnet: 124.90 seconds\n",
      "\n",
      "Classification Report for xlnet:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9577    0.8500    0.9007       160\n",
      "           1     0.9781    0.9944    0.9862      1077\n",
      "\n",
      "    accuracy                         0.9757      1237\n",
      "   macro avg     0.9679    0.9222    0.9434      1237\n",
      "weighted avg     0.9755    0.9757    0.9751      1237\n",
      "\n",
      "{'model': 'xlnet', 'max_length': 512, 'train_time_sec': 3257.2715, 'pred_time_sec': 124.9036328125, 'accuracy': 0.9757477768795473, 'f1': 0.9751255211456473, 'precision': 0.9754518651314984, 'recall': 0.9757477768795473}\n",
      "\n",
      "--- Training mobilebert with max_length=128 ---\n",
      "\n",
      "===== Training mobilebert =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462968cfcff24bb9acbe423bc1db263c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/847 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8753e6dfe74500bee2e88fb09d9e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca09b6deefa64294b201127f70911f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04f2212042b4439a868b85fd3266ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/147M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df107d73dea145e2aad88ceada0b30ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429a6135b835426a9916a46e940643ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/147M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae98f6869034c0a98d3d054b1218606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 04:23, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.341277</td>\n",
       "      <td>0.971706</td>\n",
       "      <td>0.971192</td>\n",
       "      <td>0.971151</td>\n",
       "      <td>0.971706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8210.993000</td>\n",
       "      <td>0.334536</td>\n",
       "      <td>0.973323</td>\n",
       "      <td>0.972838</td>\n",
       "      <td>0.972829</td>\n",
       "      <td>0.973323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for mobilebert: 265.17 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for mobilebert: 9.23 seconds\n",
      "\n",
      "Classification Report for mobilebert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9320    0.8562    0.8925       160\n",
      "           1     0.9789    0.9907    0.9848      1077\n",
      "\n",
      "    accuracy                         0.9733      1237\n",
      "   macro avg     0.9554    0.9235    0.9386      1237\n",
      "weighted avg     0.9728    0.9733    0.9728      1237\n",
      "\n",
      "{'model': 'mobilebert', 'max_length': 128, 'train_time_sec': 265.1703125, 'pred_time_sec': 9.2333017578125, 'accuracy': 0.973322554567502, 'f1': 0.9728377426864748, 'precision': 0.9728293922272504, 'recall': 0.973322554567502}\n",
      "\n",
      "--- Training mobilebert with max_length=256 ---\n",
      "\n",
      "===== Training mobilebert =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de52016aa23f485bb52bd1d5bb552402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e7717f1c484a088da220426be652d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 07:58, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.419742</td>\n",
       "      <td>0.968472</td>\n",
       "      <td>0.967517</td>\n",
       "      <td>0.967837</td>\n",
       "      <td>0.968472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5993.056000</td>\n",
       "      <td>0.330779</td>\n",
       "      <td>0.972514</td>\n",
       "      <td>0.971809</td>\n",
       "      <td>0.972040</td>\n",
       "      <td>0.972514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for mobilebert: 479.89 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for mobilebert: 16.82 seconds\n",
      "\n",
      "Classification Report for mobilebert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9437    0.8375    0.8874       160\n",
      "           1     0.9763    0.9926    0.9843      1077\n",
      "\n",
      "    accuracy                         0.9725      1237\n",
      "   macro avg     0.9600    0.9150    0.9359      1237\n",
      "weighted avg     0.9720    0.9725    0.9718      1237\n",
      "\n",
      "{'model': 'mobilebert', 'max_length': 256, 'train_time_sec': 479.89309375, 'pred_time_sec': 16.823578125, 'accuracy': 0.9725141471301536, 'f1': 0.9718089239650671, 'precision': 0.9720398647964685, 'recall': 0.9725141471301536}\n",
      "\n",
      "--- Training mobilebert with max_length=512 ---\n",
      "\n",
      "===== Training mobilebert =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af228c21eb704aa9b93281fbacc11ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c29c431efcd4a2983bfd6c92d555ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 20:58, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>538.605713</td>\n",
       "      <td>0.909458</td>\n",
       "      <td>0.889746</td>\n",
       "      <td>0.911141</td>\n",
       "      <td>0.909458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9682.771000</td>\n",
       "      <td>0.563883</td>\n",
       "      <td>0.964430</td>\n",
       "      <td>0.963297</td>\n",
       "      <td>0.963546</td>\n",
       "      <td>0.964430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for mobilebert: 1261.53 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for mobilebert: 47.25 seconds\n",
      "\n",
      "Classification Report for mobilebert:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9203    0.7937    0.8523       160\n",
      "           1     0.9700    0.9898    0.9798      1077\n",
      "\n",
      "    accuracy                         0.9644      1237\n",
      "   macro avg     0.9451    0.8918    0.9161      1237\n",
      "weighted avg     0.9635    0.9644    0.9633      1237\n",
      "\n",
      "{'model': 'mobilebert', 'max_length': 512, 'train_time_sec': 1261.532875, 'pred_time_sec': 47.2532421875, 'accuracy': 0.9644300727566694, 'f1': 0.9632969000782238, 'precision': 0.9635464651233187, 'recall': 0.9644300727566694}\n",
      "\n",
      "--- Training albert-base-v1 with max_length=128 ---\n",
      "\n",
      "===== Training albert-base-v1 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ba9efedef8449e96e68688bb59918c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf74902c8aa4f35ba0b120037f4af6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6c7fd3baa34d73bd089d5a53e26b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9bfa4b732a4c3d985f7499435d1c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453dcd5182494e78bcc3b869c6718e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221e99a387084ddba5e12c222cac1c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec01b980d0d429f97a07135d620f1d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 05:27, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.424813</td>\n",
       "      <td>0.966047</td>\n",
       "      <td>0.965176</td>\n",
       "      <td>0.965216</td>\n",
       "      <td>0.966047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.317000</td>\n",
       "      <td>0.401290</td>\n",
       "      <td>0.967664</td>\n",
       "      <td>0.967028</td>\n",
       "      <td>0.966936</td>\n",
       "      <td>0.967664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for albert-base-v1: 328.85 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-base-v1: 13.97 seconds\n",
      "\n",
      "Classification Report for albert-base-v1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9110    0.8313    0.8693       160\n",
      "           1     0.9753    0.9879    0.9815      1077\n",
      "\n",
      "    accuracy                         0.9677      1237\n",
      "   macro avg     0.9431    0.9096    0.9254      1237\n",
      "weighted avg     0.9669    0.9677    0.9670      1237\n",
      "\n",
      "{'model': 'albert-base-v1', 'max_length': 128, 'train_time_sec': 328.8515, 'pred_time_sec': 13.9677998046875, 'accuracy': 0.967663702506063, 'f1': 0.9670283901469479, 'precision': 0.96693605156422, 'recall': 0.967663702506063}\n",
      "\n",
      "--- Training albert-base-v1 with max_length=256 ---\n",
      "\n",
      "===== Training albert-base-v1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ee7c7c21294bec87e3548936d393b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4501c814a2bd419ca626ac6aad83690a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 10:23, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.423654</td>\n",
       "      <td>0.965238</td>\n",
       "      <td>0.964504</td>\n",
       "      <td>0.964390</td>\n",
       "      <td>0.965238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.325900</td>\n",
       "      <td>0.359128</td>\n",
       "      <td>0.975748</td>\n",
       "      <td>0.974975</td>\n",
       "      <td>0.975653</td>\n",
       "      <td>0.975748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for albert-base-v1: 625.30 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-base-v1: 24.91 seconds\n",
      "\n",
      "Classification Report for albert-base-v1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9710    0.8375    0.8993       160\n",
      "           1     0.9763    0.9963    0.9862      1077\n",
      "\n",
      "    accuracy                         0.9757      1237\n",
      "   macro avg     0.9737    0.9169    0.9428      1237\n",
      "weighted avg     0.9757    0.9757    0.9750      1237\n",
      "\n",
      "{'model': 'albert-base-v1', 'max_length': 256, 'train_time_sec': 625.3001875, 'pred_time_sec': 24.90740625, 'accuracy': 0.9757477768795473, 'f1': 0.9749751591442434, 'precision': 0.9756530250590102, 'recall': 0.9757477768795473}\n",
      "\n",
      "--- Training albert-base-v1 with max_length=512 ---\n",
      "\n",
      "===== Training albert-base-v1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742de9e80e124140aee3e6bed064d764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53725dca66264743894f4f7a8a123571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 21:46, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.396304</td>\n",
       "      <td>0.968472</td>\n",
       "      <td>0.967418</td>\n",
       "      <td>0.967910</td>\n",
       "      <td>0.968472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.308900</td>\n",
       "      <td>0.278369</td>\n",
       "      <td>0.977365</td>\n",
       "      <td>0.976920</td>\n",
       "      <td>0.977040</td>\n",
       "      <td>0.977365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for albert-base-v1: 1308.87 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-base-v1: 52.46 seconds\n",
      "\n",
      "Classification Report for albert-base-v1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9521    0.8688    0.9085       160\n",
      "           1     0.9808    0.9935    0.9871      1077\n",
      "\n",
      "    accuracy                         0.9774      1237\n",
      "   macro avg     0.9664    0.9311    0.9478      1237\n",
      "weighted avg     0.9770    0.9774    0.9769      1237\n",
      "\n",
      "{'model': 'albert-base-v1', 'max_length': 512, 'train_time_sec': 1308.87075, 'pred_time_sec': 52.46413671875, 'accuracy': 0.9773645917542442, 'f1': 0.9769198731028634, 'precision': 0.9770398097549116, 'recall': 0.9773645917542442}\n",
      "\n",
      "--- Training albert-large-v2 with max_length=128 ---\n",
      "\n",
      "===== Training albert-large-v2 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b83b458254542258c3e96ca465b89db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83317170eb3d4a568e4580dbbdd8fb4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c490ac8e33411f8db24d49878caeec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d9b80165ec4ccfbbb66769c9e35961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ac2de3f4064f7dadd32b397684403f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/71.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-large-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dadf41b25ec47b78d059cf2478431d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8daeeba8fc884b108c73bdc18ea6ecbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 23:26, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.634853</td>\n",
       "      <td>0.902991</td>\n",
       "      <td>0.879941</td>\n",
       "      <td>0.902714</td>\n",
       "      <td>0.902991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.711700</td>\n",
       "      <td>0.433383</td>\n",
       "      <td>0.932094</td>\n",
       "      <td>0.929496</td>\n",
       "      <td>0.928593</td>\n",
       "      <td>0.932094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for albert-large-v2: 1408.74 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-large-v2: 59.23 seconds\n",
      "\n",
      "Classification Report for albert-large-v2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7836    0.6562    0.7143       160\n",
      "           1     0.9501    0.9731    0.9615      1077\n",
      "\n",
      "    accuracy                         0.9321      1237\n",
      "   macro avg     0.8669    0.8147    0.8379      1237\n",
      "weighted avg     0.9286    0.9321    0.9295      1237\n",
      "\n",
      "{'model': 'albert-large-v2', 'max_length': 128, 'train_time_sec': 1408.73625, 'pred_time_sec': 59.23441015625, 'accuracy': 0.9320937752627324, 'f1': 0.9294960644437404, 'precision': 0.9285930464971179, 'recall': 0.9320937752627324}\n",
      "\n",
      "--- Training albert-large-v2 with max_length=256 ---\n",
      "\n",
      "===== Training albert-large-v2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-large-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b77e29594ae4a929e3de8f0fa3bc6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17692fc076f44e32a3b8d2066b57f829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 45:59, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.690005</td>\n",
       "      <td>0.870655</td>\n",
       "      <td>0.810454</td>\n",
       "      <td>0.758040</td>\n",
       "      <td>0.870655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.716700</td>\n",
       "      <td>0.689622</td>\n",
       "      <td>0.870655</td>\n",
       "      <td>0.810454</td>\n",
       "      <td>0.758040</td>\n",
       "      <td>0.870655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for albert-large-v2: 2764.15 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-large-v2: 117.35 seconds\n",
      "\n",
      "Classification Report for albert-large-v2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       160\n",
      "           1     0.8707    1.0000    0.9309      1077\n",
      "\n",
      "    accuracy                         0.8707      1237\n",
      "   macro avg     0.4353    0.5000    0.4654      1237\n",
      "weighted avg     0.7580    0.8707    0.8105      1237\n",
      "\n",
      "{'model': 'albert-large-v2', 'max_length': 256, 'train_time_sec': 2764.15225, 'pred_time_sec': 117.3475859375, 'accuracy': 0.8706548100242523, 'f1': 0.8104539588557645, 'precision': 0.7580397982183668, 'recall': 0.8706548100242523}\n",
      "\n",
      "--- Training albert-large-v2 with max_length=512 ---\n",
      "\n",
      "===== Training albert-large-v2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-large-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062a2bb80fac4496a0f87bbe19d09fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0791941b644e22ac9045ecf72eafdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 1:32:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.461458</td>\n",
       "      <td>0.936944</td>\n",
       "      <td>0.932068</td>\n",
       "      <td>0.933920</td>\n",
       "      <td>0.936944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.586600</td>\n",
       "      <td>0.429393</td>\n",
       "      <td>0.955538</td>\n",
       "      <td>0.953468</td>\n",
       "      <td>0.954179</td>\n",
       "      <td>0.955538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for albert-large-v2: 5588.21 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-large-v2: 238.23 seconds\n",
      "\n",
      "Classification Report for albert-large-v2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9070    0.7312    0.8097       160\n",
      "           1     0.9612    0.9889    0.9748      1077\n",
      "\n",
      "    accuracy                         0.9555      1237\n",
      "   macro avg     0.9341    0.8601    0.8923      1237\n",
      "weighted avg     0.9542    0.9555    0.9535      1237\n",
      "\n",
      "{'model': 'albert-large-v2', 'max_length': 512, 'train_time_sec': 5588.2065, 'pred_time_sec': 238.229984375, 'accuracy': 0.9555375909458367, 'f1': 0.9534683372622854, 'precision': 0.9541789390960591, 'recall': 0.9555375909458367}\n",
      "\n",
      "--- Training albert-xlarge-v2 with max_length=128 ---\n",
      "\n",
      "===== Training albert-xlarge-v2 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16528940a4b84e23ac5af52d76cc90a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c6733d58904a78a9ef9d3f0759732a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27a36d158554181993c211c590488bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf8f422ea8c42039bd29113c18cb7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4121ab970bcd44d392689d4321ae11aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/236M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-xlarge-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a5c888b85e40c2881910aebf6dd5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4047c71f514de8950d514fbacf6441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 1:21:15, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.690866</td>\n",
       "      <td>0.870655</td>\n",
       "      <td>0.810454</td>\n",
       "      <td>0.758040</td>\n",
       "      <td>0.870655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.715200</td>\n",
       "      <td>0.689503</td>\n",
       "      <td>0.870655</td>\n",
       "      <td>0.810454</td>\n",
       "      <td>0.758040</td>\n",
       "      <td>0.870655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for albert-xlarge-v2: 4883.73 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-xlarge-v2: 252.47 seconds\n",
      "\n",
      "Classification Report for albert-xlarge-v2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       160\n",
      "           1     0.8707    1.0000    0.9309      1077\n",
      "\n",
      "    accuracy                         0.8707      1237\n",
      "   macro avg     0.4353    0.5000    0.4654      1237\n",
      "weighted avg     0.7580    0.8707    0.8105      1237\n",
      "\n",
      "{'model': 'albert-xlarge-v2', 'max_length': 128, 'train_time_sec': 4883.7335, 'pred_time_sec': 252.47365625, 'accuracy': 0.8706548100242523, 'f1': 0.8104539588557645, 'precision': 0.7580397982183668, 'recall': 0.8706548100242523}\n",
      "\n",
      "--- Training albert-xlarge-v2 with max_length=256 ---\n",
      "\n",
      "===== Training albert-xlarge-v2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-xlarge-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01343da5280d48fa95acd974e9f813e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974dace13a244184b404f8a9ff9c98a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 2:39:04, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.694994</td>\n",
       "      <td>0.129345</td>\n",
       "      <td>0.029628</td>\n",
       "      <td>0.016730</td>\n",
       "      <td>0.129345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.738300</td>\n",
       "      <td>0.689611</td>\n",
       "      <td>0.870655</td>\n",
       "      <td>0.810454</td>\n",
       "      <td>0.758040</td>\n",
       "      <td>0.870655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for albert-xlarge-v2: 9559.82 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-xlarge-v2: 501.68 seconds\n",
      "\n",
      "Classification Report for albert-xlarge-v2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       160\n",
      "           1     0.8707    1.0000    0.9309      1077\n",
      "\n",
      "    accuracy                         0.8707      1237\n",
      "   macro avg     0.4353    0.5000    0.4654      1237\n",
      "weighted avg     0.7580    0.8707    0.8105      1237\n",
      "\n",
      "{'model': 'albert-xlarge-v2', 'max_length': 256, 'train_time_sec': 9559.815, 'pred_time_sec': 501.6791875, 'accuracy': 0.8706548100242523, 'f1': 0.8104539588557645, 'precision': 0.7580397982183668, 'recall': 0.8706548100242523}\n",
      "\n",
      "--- Training albert-xlarge-v2 with max_length=512 ---\n",
      "\n",
      "===== Training albert-xlarge-v2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-xlarge-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6648f44c69c445bf93dbd6127b0a4b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27bca9a78df4a47a9605f1f75c5b6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 4:39:12, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.681307</td>\n",
       "      <td>0.870655</td>\n",
       "      <td>0.810454</td>\n",
       "      <td>0.758040</td>\n",
       "      <td>0.870655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.724600</td>\n",
       "      <td>0.681360</td>\n",
       "      <td>0.870655</td>\n",
       "      <td>0.810454</td>\n",
       "      <td>0.758040</td>\n",
       "      <td>0.870655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for albert-xlarge-v2: 16844.53 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-xlarge-v2: 788.57 seconds\n",
      "\n",
      "Classification Report for albert-xlarge-v2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       160\n",
      "           1     0.8707    1.0000    0.9309      1077\n",
      "\n",
      "    accuracy                         0.8707      1237\n",
      "   macro avg     0.4353    0.5000    0.4654      1237\n",
      "weighted avg     0.7580    0.8707    0.8105      1237\n",
      "\n",
      "{'model': 'albert-xlarge-v2', 'max_length': 512, 'train_time_sec': 16844.526, 'pred_time_sec': 788.5704375, 'accuracy': 0.8706548100242523, 'f1': 0.8104539588557645, 'precision': 0.7580397982183668, 'recall': 0.8706548100242523}\n",
      "\n",
      "--- Training albert-xxlarge-v2 with max_length=128 ---\n",
      "\n",
      "===== Training albert-xxlarge-v2 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d099e75ac64772897e7adb74edece6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7287e1823f40849c39b1b7e51d1db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1fbd11d11845f5bc1c0c9c40e2ba42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7952fac99ad4ad8993f863e2d7f1d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/710 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5744aa7b1446c69ccaf950efc90833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/893M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-xxlarge-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9399f47875fd49f79dde1bee043155a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a1475509d4430ab588e72eeb69c1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 2:28:26, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.344984</td>\n",
       "      <td>0.957154</td>\n",
       "      <td>0.954866</td>\n",
       "      <td>0.956237</td>\n",
       "      <td>0.957154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>0.448747</td>\n",
       "      <td>0.971706</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>0.971382</td>\n",
       "      <td>0.971706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for albert-xxlarge-v2: 8921.32 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-xxlarge-v2: 491.27 seconds\n",
      "\n",
      "Classification Report for albert-xxlarge-v2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9562    0.8187    0.8822       160\n",
      "           1     0.9736    0.9944    0.9839      1077\n",
      "\n",
      "    accuracy                         0.9717      1237\n",
      "   macro avg     0.9649    0.9066    0.9330      1237\n",
      "weighted avg     0.9714    0.9717    0.9708      1237\n",
      "\n",
      "{'model': 'albert-xxlarge-v2', 'max_length': 128, 'train_time_sec': 8921.315, 'pred_time_sec': 491.27440625, 'accuracy': 0.9717057396928052, 'f1': 0.970759635088592, 'precision': 0.9713816203446165, 'recall': 0.9717057396928052}\n",
      "\n",
      "--- Training albert-xxlarge-v2 with max_length=256 ---\n",
      "\n",
      "===== Training albert-xxlarge-v2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-xxlarge-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb25298d31014a7990e813f1b6a66299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f108f9b8b5ce4696ae661997a96c51c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 4:10:01, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.691153</td>\n",
       "      <td>0.870655</td>\n",
       "      <td>0.810454</td>\n",
       "      <td>0.758040</td>\n",
       "      <td>0.870655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.705900</td>\n",
       "      <td>0.689785</td>\n",
       "      <td>0.870655</td>\n",
       "      <td>0.810454</td>\n",
       "      <td>0.758040</td>\n",
       "      <td>0.870655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for albert-xxlarge-v2: 15023.97 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/spark/anaconda3/envs/lmforge/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-xxlarge-v2: 764.08 seconds\n",
      "\n",
      "Classification Report for albert-xxlarge-v2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       160\n",
      "           1     0.8707    1.0000    0.9309      1077\n",
      "\n",
      "    accuracy                         0.8707      1237\n",
      "   macro avg     0.4353    0.5000    0.4654      1237\n",
      "weighted avg     0.7580    0.8707    0.8105      1237\n",
      "\n",
      "{'model': 'albert-xxlarge-v2', 'max_length': 256, 'train_time_sec': 15023.971, 'pred_time_sec': 764.084875, 'accuracy': 0.8706548100242523, 'f1': 0.8104539588557645, 'precision': 0.7580397982183668, 'recall': 0.8706548100242523}\n",
      "\n",
      "--- Training albert-xxlarge-v2 with max_length=512 ---\n",
      "\n",
      "===== Training albert-xxlarge-v2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-xxlarge-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28bfaba7e4645b99f6985fa51aaba1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829b5a8f71b34ffe89d4caadc29df5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 2:31:40, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.435945</td>\n",
       "      <td>0.924010</td>\n",
       "      <td>0.924210</td>\n",
       "      <td>0.924420</td>\n",
       "      <td>0.924010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.578000</td>\n",
       "      <td>0.414999</td>\n",
       "      <td>0.870655</td>\n",
       "      <td>0.884330</td>\n",
       "      <td>0.916262</td>\n",
       "      <td>0.870655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for albert-xxlarge-v2: 9261.66 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for albert-xxlarge-v2: 527.08 seconds\n",
      "\n",
      "Classification Report for albert-xxlarge-v2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7037    0.7125    0.7081       160\n",
      "           1     0.9572    0.9554    0.9563      1077\n",
      "\n",
      "    accuracy                         0.9240      1237\n",
      "   macro avg     0.8305    0.8340    0.8322      1237\n",
      "weighted avg     0.9244    0.9240    0.9242      1237\n",
      "\n",
      "{'model': 'albert-xxlarge-v2', 'max_length': 512, 'train_time_sec': 9261.659, 'pred_time_sec': 527.0794375, 'accuracy': 0.9240097008892482, 'f1': 0.9242103841297016, 'precision': 0.9244195725119191, 'recall': 0.9240097008892482}\n",
      "\n",
      "--- Training bert-large-uncased with max_length=128 ---\n",
      "\n",
      "===== Training bert-large-uncased =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f81f6931e24ed29732ed0cae925bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a807cf9b6b7648b99541daefdac3b300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1217ff84d643c0a20f3f0111c25307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077c483aa98c401da0689eb5911ceb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48db8316812946f6ad70e6250ef15eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6facf0e09b4e4d8d82d40d93203b45c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a8c4d315364eb9a031e3522f8cf696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 06:12, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.207506</td>\n",
       "      <td>0.967664</td>\n",
       "      <td>0.968158</td>\n",
       "      <td>0.969003</td>\n",
       "      <td>0.967664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.358200</td>\n",
       "      <td>0.247776</td>\n",
       "      <td>0.967664</td>\n",
       "      <td>0.967749</td>\n",
       "      <td>0.967845</td>\n",
       "      <td>0.967664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for bert-large-uncased: 375.89 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for bert-large-uncased: 11.77 seconds\n",
      "\n",
      "Classification Report for bert-large-uncased:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8488    0.9125    0.8795       160\n",
      "           1     0.9869    0.9759    0.9813      1077\n",
      "\n",
      "    accuracy                         0.9677      1237\n",
      "   macro avg     0.9178    0.9442    0.9304      1237\n",
      "weighted avg     0.9690    0.9677    0.9682      1237\n",
      "\n",
      "{'model': 'bert-large-uncased', 'max_length': 128, 'train_time_sec': 375.89234375, 'pred_time_sec': 11.772083984375, 'accuracy': 0.967663702506063, 'f1': 0.9681575155604993, 'precision': 0.9690025925703306, 'recall': 0.967663702506063}\n",
      "\n",
      "--- Training bert-large-uncased with max_length=256 ---\n",
      "\n",
      "===== Training bert-large-uncased =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5988da0f68044ac3bc06f4dc67ba1c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb0a78bb6c246b5ab09d90daa9fe578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 10:48, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.288586</td>\n",
       "      <td>0.977365</td>\n",
       "      <td>0.976714</td>\n",
       "      <td>0.977256</td>\n",
       "      <td>0.977365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.241077</td>\n",
       "      <td>0.979790</td>\n",
       "      <td>0.979539</td>\n",
       "      <td>0.979518</td>\n",
       "      <td>0.979790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for bert-large-uncased: 649.37 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for bert-large-uncased: 23.58 seconds\n",
      "\n",
      "Classification Report for bert-large-uncased:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9470    0.8938    0.9196       160\n",
      "           1     0.9843    0.9926    0.9884      1077\n",
      "\n",
      "    accuracy                         0.9798      1237\n",
      "   macro avg     0.9657    0.9432    0.9540      1237\n",
      "weighted avg     0.9795    0.9798    0.9795      1237\n",
      "\n",
      "{'model': 'bert-large-uncased', 'max_length': 256, 'train_time_sec': 649.3740625, 'pred_time_sec': 23.580330078125, 'accuracy': 0.9797898140662894, 'f1': 0.979539429879786, 'precision': 0.9795182399238865, 'recall': 0.9797898140662894}\n",
      "\n",
      "--- Training bert-large-uncased with max_length=512 ---\n",
      "\n",
      "===== Training bert-large-uncased =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db9aea80a874d5ea6ea980ab2086c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e8e755145b47afade9448359d18231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066673/3185563332.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 20:11, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.278760</td>\n",
       "      <td>0.971706</td>\n",
       "      <td>0.971743</td>\n",
       "      <td>0.971783</td>\n",
       "      <td>0.971706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.279400</td>\n",
       "      <td>0.205149</td>\n",
       "      <td>0.979790</td>\n",
       "      <td>0.979763</td>\n",
       "      <td>0.979739</td>\n",
       "      <td>0.979790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for bert-large-uncased: 1213.27 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time for bert-large-uncased: 47.68 seconds\n",
      "\n",
      "Classification Report for bert-large-uncased:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9245    0.9187    0.9216       160\n",
      "           1     0.9879    0.9889    0.9884      1077\n",
      "\n",
      "    accuracy                         0.9798      1237\n",
      "   macro avg     0.9562    0.9538    0.9550      1237\n",
      "weighted avg     0.9797    0.9798    0.9798      1237\n",
      "\n",
      "{'model': 'bert-large-uncased', 'max_length': 512, 'train_time_sec': 1213.265, 'pred_time_sec': 47.68050390625, 'accuracy': 0.9797898140662894, 'f1': 0.9797628257971375, 'precision': 0.9797385510679781, 'recall': 0.9797898140662894}\n"
     ]
    }
   ],
   "source": [
    "for model_name in MODEL_CONFIGS.keys():\n",
    "    for max_len in CONFIG[\"max_length\"]:\n",
    "        # skip if this (model, max_len) already exists in CSV\n",
    "        if os.path.exists(\"model_performance_nvidia.csv\") and f\"{model_name},{max_len},\" in open(\"model_performance_nvidia.csv\").read():\n",
    "            print(f\"Skipping {model_name} with max_length={max_len} (already in CSV).\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Training {model_name} with max_length={max_len} ---\")\n",
    "        performance = train_and_evaluate(model_name, train_dataset, test_dataset, data, max_len)\n",
    "        print(performance)\n",
    "        # save performance to a csv file\n",
    "        with open(\"model_performance_nvidia.csv\", \"a\") as f:\n",
    "            #headers if file is empty\n",
    "            if os.stat(\"model_performance_nvidia.csv\").st_size == 0:\n",
    "                f.write(\"model,max_length,train_time_sec,pred_time_sec,accuracy,f1,precision,recall\\n\")\n",
    "            f.write(\",\".join([str(performance[k]) for k in performance]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f468f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba7b36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmforge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
