{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA RTX 5000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.device_count())  # Should return the number of GPUs\n",
    "print(torch.cuda.get_device_name(0))  # Should show the GPU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import logging\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import shutil \n",
    "\n",
    "import evaluate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=\"../../.env\") # path is relative to this script, adjust as needed\n",
    "\n",
    "\n",
    "run_id = \"LMForge_RUN01\"  # <- Change this manually for each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: D:/huggingface_cache\n",
      "TRANSFORMERS_CACHE: D:/huggingface_cache\n",
      "HUGGINGFACE_HUB_CACHE: D:/huggingface_cache\n"
     ]
    }
   ],
   "source": [
    "# setting huggingface token\n",
    "login(token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"D:/huggingface_cache\" \n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"D:/huggingface_cache\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"D:/huggingface_cache\"\n",
    "\n",
    "print(\"HF_HOME:\", os.getenv(\"HF_HOME\"))\n",
    "print(\"TRANSFORMERS_CACHE:\", os.getenv(\"TRANSFORMERS_CACHE\"))\n",
    "print(\"HUGGINGFACE_HUB_CACHE:\", os.getenv(\"HUGGINGFACE_HUB_CACHE\"))\n",
    "\n",
    "transformers.utils.hub.TRANSFORMERS_CACHE = \"D:/huggingface_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee703884f13f4daa897fb94507d87e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf41e237d7448c6966eca2c4067a03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_sizes = [128]#,256, 512, 1024]\n",
    "questions_num = 2\n",
    "max_token_list = [128]#,256,512,1024,2048]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=[\n",
    "    \"chunk_size\", \"questions_num\", \"qa_count_mismatch\", \"total_questions\", \"token_Size\",\n",
    "    \"total_chunks\", \"success_count\", \"fail_count\",\n",
    "    \"elapsed_time\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SAVE_INTERVAL = 10\n",
    "checkpoint_path = \"checkpoint.csv\"\n",
    "\n",
    "# Loading an existing checkpoint if it exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    completed_runs = pd.read_csv(checkpoint_path)\n",
    "    completed_set = set(tuple(row) for row in completed_runs.values)\n",
    "else:\n",
    "    completed_set = set()\n",
    "    pd.DataFrame(columns=[\"chunk_size\", \"max_tokens\", \"doc_name\", \"chunk_index\"]).to_csv(checkpoint_path, index=False)\n",
    "pd.DataFrame(columns=[\"chunk_size\", \"max_tokens\", \"doc_name\", \"chunk_index\"]).to_csv(checkpoint_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POWER ANALYSIS\n",
    "##### This function performs a power analysis for generated text against a reference text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_analysis(chunk_size, max_tokens, qa_results,elapsed_time):\n",
    "    \"\"\"\n",
    "    Perform power analysis based on the provided parameters for the current run.\n",
    "    \"\"\"\n",
    "    \n",
    "    # https://huggingface.co/spaces/evaluate-metric/bertscore\n",
    "    # https://huggingface.co/tasks/sentence-similarity\n",
    "    # 1 Metric: ROUGE\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "    originals = []\n",
    "    generations = []\n",
    "\n",
    "    for doc in qa_results.values():\n",
    "        for item in doc:\n",
    "            chunk = item[\"chunk\"]\n",
    "            qa = item[\"qa_pairs\"]\n",
    "            for pair in qa:\n",
    "                originals.append(chunk)  # reference\n",
    "                generations.append(pair[\"answer\"])  # model-generated answer\n",
    "\n",
    "    scores = rouge.compute(predictions=generations, references=originals)\n",
    "    print(f\"ROUGE Scores: {scores}\")\n",
    "\n",
    "    # 2 Metric: BERTScore\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    bert_scores = bertscore.compute(predictions=generations, references=originals, model_type=\"bert-base-uncased\", lang=\"en\")\n",
    "    P = bert_scores[\"precision\"]\n",
    "    R = bert_scores[\"recall\"]\n",
    "    F1 = bert_scores[\"f1\"] \n",
    "\n",
    "    print(f\"BERTScore: {bert_scores}\")\n",
    "\n",
    "    # 3 Metric: STS (Semantic Textual Similarity)\n",
    "    sts_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    original_embeddings = sts_model.encode(originals, convert_to_tensor=True) \n",
    "    generated_embeddings = sts_model.encode(generations, convert_to_tensor=True)\n",
    "    sts_scores = util.pytorch_cos_sim(original_embeddings, generated_embeddings).diagonal().cpu().tolist()\n",
    "\n",
    "    print(f\"STS Scores: {sts_scores}\")\n",
    "\n",
    "    # save the scores to a CSV file\n",
    "    scores_df = pd.DataFrame({\n",
    "        \"chunk_size\": [chunk_size],\n",
    "        \"max_tokens\": [max_tokens],\n",
    "        \"questions_num\": [questions_num],\n",
    "        \"rouge1\": [scores[\"rouge1\"]],\n",
    "        \"rouge2\": [scores[\"rouge2\"]],\n",
    "        \"rougeL\": [scores[\"rougeL\"]],\n",
    "        \"rougeLsum\": [scores[\"rougeLsum\"]],\n",
    "        \"bert_score_P\": [np.mean(P)],\n",
    "        \"bert_score_R\": [np.mean(R)],\n",
    "        \"bert_score_F1\": [np.mean(F1)],\n",
    "        \"sts_score\": [np.mean(sts_scores)],\n",
    "        \"elapsed_time\": [elapsed_time],\n",
    "    })\n",
    "    \n",
    "    print(\"Scores saved to scores.csv\")   \n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert logs to Panda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run: Chunk=128, Max Tokens=128\n",
      "Error generating for chunk: 'Documents/Legal Aspects of Corporate Management and Finance.pdf'\n",
      "Error generating for chunk: 'Documents/Legal Aspects of Corporate Management and Finance.pdf'\n",
      "Error generating for chunk: 'Documents/Legal Aspects of Corporate Management and Finance.pdf'\n",
      "Error generating for chunk: 'Documents/Legal Aspects of Corporate Management and Finance.pdf'\n",
      "Error generating for chunk: 'Documents/Legal Aspects of Corporate Management and Finance.pdf'\n",
      "Error generating for chunk: 'Documents/Legal Aspects of Corporate Management and Finance.pdf'\n",
      "Error generating for chunk: 'Documents/Legal Aspects of Corporate Management and Finance.pdf'\n",
      "Error generating for chunk: 'Documents/Legal Aspects of Corporate Management and Finance.pdf'\n",
      "Error generating for chunk: 'Documents/Legal Aspects of Corporate Management and Finance.pdf'\n",
      "Error generating for chunk: 'Documents/Legal Aspects of Corporate Management and Finance.pdf'\n",
      "Error generating for chunk: 'Documents/Legal Aspects of Corporate Management and Finance.pdf'\n",
      "Error generating for chunk: 'Documents/Legal Aspects of Corporate Management and Finance.pdf'\n",
      "Error generating for chunk: 'Documents/Legal Aspects of Corporate Management and Finance.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "Error generating for chunk: 'Documents/PrinciplesofFinance-WEB.pdf'\n",
      "No batches to save.\n",
      "All runs completed.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# delete existing folder if you want to rerun the script from scratch uncomment the following lines\n",
    "\n",
    "if os.path.exists(f\"Generated_Results/LLAMA3_1/{run_id}\"):\n",
    "    shutil.rmtree(f\"Generated_Results/LLAMA3_1/{run_id}\")\n",
    "\n",
    "# deleting existing checkpoint file if you want to rerun the script from scratch uncomment the following lines\n",
    "if os.path.exists(checkpoint_path):\n",
    "    os.remove(checkpoint_path)\n",
    "    \n",
    "for chunk_size in chunk_sizes:\n",
    "    json_file_path = f\"../Yaman/Generate_Paragraphs/Results/extracted_chunks_{chunk_size}_overlap.json\"\n",
    "\n",
    "    if not os.path.exists(json_file_path):\n",
    "        print(f\"Missing input file: {json_file_path}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        chunk_data = json.load(file)\n",
    "\n",
    "\n",
    "    for max_tokens in max_token_list:\n",
    "        output_file_path = f\"Generated_Results/LLAMA3_1/{run_id}/generation_log_{chunk_size}_Token_{max_tokens}_Q{questions_num}.json\"\n",
    "        if os.path.exists(output_file_path):\n",
    "            print(f\"Output already exists: {output_file_path}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Starting new run: Chunk={chunk_size}, Max Tokens={max_tokens}\")\n",
    "\n",
    "        qa_results = {}\n",
    "        total_chunks = 0\n",
    "        success_count = 0\n",
    "        fail_count = 0\n",
    "        total_questions = 0\n",
    "        qa_count_mismatch = 0\n",
    "        batch_checkpoints = []\n",
    "        batch_qa_results = {}\n",
    "        start_time = time.time()\n",
    "\n",
    "        for doc_name, chunks in chunk_data.items():\n",
    "            if doc_name not in qa_results:\n",
    "                qa_results[doc_name] = []\n",
    "            if doc_name not in batch_qa_results:\n",
    "                batch_qa_results[doc_name] = []\n",
    "\n",
    "            for chunk_index, chunk in enumerate(chunks[:100]):  # Limit to 1000 chunks for testing\n",
    "                run_key = (chunk_size, max_tokens, doc_name, chunk_index)\n",
    "                if run_key in completed_set:\n",
    "                    print(f\"Skipping completed sample: {run_key}\")\n",
    "                    continue\n",
    "\n",
    "                total_chunks += 1\n",
    "\n",
    "                prompt = f\"\"\"\n",
    "Generate {questions_num} question-answer pairs based on the following text segment. \n",
    "Return the result in valid JSON format as a list of objects.\n",
    "\n",
    "Text Segment:\n",
    "\n",
    "{chunk}\n",
    "\n",
    "Response Format:\n",
    "[\n",
    "    {{\"question\": \"generated question\", \"answer\": \"generated Answer\"}},\n",
    "]\n",
    "\n",
    "Question answers should be at least 250 words long.\n",
    "\n",
    "Do NOT include any explanation or preamble before or after the JSON output.\n",
    "Return ONLY valid JSON output.\n",
    "\n",
    "Answer:\n",
    "                \"\"\"\n",
    "\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "                try:\n",
    "                    max_context = model.config.max_position_embeddings\n",
    "                    input_len = inputs['input_ids'].shape[1]\n",
    "                    if input_len + max_tokens > max_context:\n",
    "                        print(f\"Skipping chunk (too long): input_len={input_len}\")\n",
    "                        continue\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        output_tokens = model.generate(**inputs, max_new_tokens=max_tokens,pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "                    generated_tokens = output_tokens[0][len(inputs[\"input_ids\"][0]):]\n",
    "                    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "                    try:\n",
    "                        qa_pairs = json.loads(generated_text)\n",
    "                        if isinstance(qa_pairs, list):\n",
    "                            sample_result = {\n",
    "                                \"chunk\": chunk,\n",
    "                                \"qa_pairs\": qa_pairs\n",
    "                            }\n",
    "                            qa_results[doc_name].append(sample_result)\n",
    "                            batch_qa_results[doc_name].append(sample_result)\n",
    "                            success_count += 1\n",
    "                            total_questions += len(qa_pairs)\n",
    "\n",
    "                            if len(qa_pairs) != questions_num:\n",
    "                                qa_count_mismatch += 1\n",
    "                        else:\n",
    "                            fail_count += 1\n",
    "                    except json.JSONDecodeError:\n",
    "                        fail_count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating for chunk: {e}\")\n",
    "                    fail_count += 1\n",
    "                    \n",
    "                completed_set.add(run_key)\n",
    "                batch_checkpoints.append(run_key)\n",
    "\n",
    "                if len(batch_checkpoints) >= BATCH_SAVE_INTERVAL:\n",
    "                    output_file_path = f\"Generated_Results/LLAMA3_1/{run_id}/generation_log_{chunk_size}_Token_{max_tokens}_Q{questions_num}.json\"\n",
    "                    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "\n",
    "                    if os.path.exists(output_file_path):\n",
    "                        with open(output_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                            existing_results = json.load(f)\n",
    "                    else:\n",
    "                        existing_results = {}\n",
    "\n",
    "                    # Merge existing with new\n",
    "                    for k, v in batch_qa_results.items():\n",
    "                        if k in existing_results:\n",
    "                            existing_results[k].extend(v)\n",
    "                        else:\n",
    "                            existing_results[k] = v\n",
    "\n",
    "                    with open(output_file_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "                        json.dump(existing_results, out_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "                    # Save checkpoint\n",
    "                    pd.DataFrame(batch_checkpoints, columns=[\"chunk_size\", \"max_tokens\", \"doc_name\", \"chunk_index\"]) \\\n",
    "                        .to_csv(checkpoint_path, mode='a', header=False, index=False)\n",
    "\n",
    "                    # Save results log incrementally\n",
    "                    elapsed_time = timedelta(seconds=time.time() - start_time)\n",
    "                    results_row = [\n",
    "                        chunk_size, questions_num, qa_count_mismatch, total_questions,\n",
    "                        max_tokens, total_chunks, success_count, fail_count, str(elapsed_time)\n",
    "                    ]\n",
    "                    csv_output_path = f\"Generated_Results/LLAMA3_1/{run_id}/results_log.csv\"\n",
    "                    if not os.path.exists(csv_output_path):\n",
    "                        results_df.columns = [\n",
    "                            \"chunk_size\", \"questions_num\", \"qa_count_mismatch\", \"total_questions\",\n",
    "                            \"max_tokens\", \"total_chunks\", \"success_count\", \"fail_count\", \"elapsed_time\"\n",
    "                        ]\n",
    "                        results_df.loc[0] = results_row\n",
    "                        results_df.to_csv(csv_output_path, index=False)\n",
    "                    else:\n",
    "                        pd.DataFrame([results_row], columns=results_df.columns).to_csv(csv_output_path, mode='a', header=False, index=False)\n",
    "\n",
    "                    # Clear batch\n",
    "                    batch_checkpoints = []\n",
    "                    batch_qa_results = {}\n",
    "\n",
    "        # Save QA Output\n",
    "        if batch_checkpoints:\n",
    "            if os.path.exists(output_file_path):\n",
    "                with open(output_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    existing_results = json.load(f)\n",
    "            else:\n",
    "                existing_results = {}\n",
    "\n",
    "            for k, v in batch_qa_results.items():\n",
    "                if k in existing_results:\n",
    "                    existing_results[k].extend(v)\n",
    "                else:\n",
    "                    existing_results[k] = v\n",
    "\n",
    "            with open(output_file_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "                json.dump(existing_results, out_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "            pd.DataFrame(batch_checkpoints, columns=[\"chunk_size\", \"max_tokens\", \"doc_name\", \"chunk_index\"]) \\\n",
    "                .to_csv(checkpoint_path, mode='a', header=False, index=False)\n",
    "\n",
    "            elapsed_time = timedelta(seconds=time.time() - start_time)\n",
    "            results_row = [\n",
    "                chunk_size, questions_num, qa_count_mismatch, total_questions,\n",
    "                max_tokens, total_chunks, success_count, fail_count, str(elapsed_time)\n",
    "            ]\n",
    "            results_columns = [\n",
    "                \"chunk_size\", \"questions_num\", \"qa_count_mismatch\", \"total_questions\",\n",
    "                \"max_tokens\", \"total_chunks\", \"success_count\", \"fail_count\", \"elapsed_time\"\n",
    "            ]\n",
    "            results_csv_path = f\"Generated_Results/LLAMA3_1/{run_id}/results_log.csv\"\n",
    "            os.makedirs(os.path.dirname(results_csv_path), exist_ok=True)\n",
    "            if not os.path.exists(results_csv_path):\n",
    "                pd.DataFrame([results_row], columns=results_columns).to_csv(results_csv_path, index=False)\n",
    "            else:\n",
    "                pd.DataFrame([results_row], columns=results_columns).to_csv(results_csv_path, mode='a', header=False, index=False)\n",
    "\n",
    "            # Power analysis logging for final flush\n",
    "            scores_df = power_analysis(chunk_size, max_tokens, batch_qa_results, run_id, elapsed_time)\n",
    "            timestamp_dir = f\"Generated_Results/LLAMA3_1/{run_id}/scores\"\n",
    "            os.makedirs(timestamp_dir, exist_ok=True)\n",
    "            scores_path = f\"{timestamp_dir}/scores/scores.csv\"\n",
    "            os.makedirs(os.path.dirname(scores_path), exist_ok=True)\n",
    "            if os.path.exists(scores_path):\n",
    "                scores_df.to_csv(scores_path, mode='a', header=False, index=False)\n",
    "            else:\n",
    "                scores_df.to_csv(scores_path, index=False)\n",
    "        else:\n",
    "            print(\"No batches to save.\")\n",
    "        # Final power analysis\n",
    "    \n",
    "print(\"All runs completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# def convert_to_seconds(time_str):\n",
    "# \ttry:\n",
    "# \t\tminutes, seconds = map(float, time_str.split(\":\"))\n",
    "# \t\treturn minutes * 60 + seconds\n",
    "# \texcept ValueError:\n",
    "# \t\treturn None\n",
    "\n",
    "\n",
    "# df_scores = pd.read_csv(f\"elapsed_time.csv\")\n",
    "# df_scores[\"elapsed_time_seconds\"] = df_scores[\"elapsed_time\"].apply(convert_to_seconds)\n",
    "# df_scores[\"time_per_sample\"] = df_scores[\"elapsed_time_seconds\"] / df_scores[\"samples\"]\n",
    "\n",
    "# grouped_estimates = df_scores.groupby([\"chunk_size\", \"max_tokens\"])[\"time_per_sample\"].mean().reset_index()\n",
    "\n",
    "# grouped_estimates[\"time_for_100\"] = grouped_estimates[\"time_per_sample\"] * 100\n",
    "# grouped_estimates[\"time_for_1000\"] = grouped_estimates[\"time_per_sample\"] * 1000\n",
    "# grouped_estimates[\"time_for_5000\"] = grouped_estimates[\"time_per_sample\"] * 5000\n",
    "# grouped_estimates.to_csv(\"average_time_estimates.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-forge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
