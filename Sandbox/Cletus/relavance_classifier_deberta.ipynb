{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04c002d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to JSONL file.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "\n",
    "DB_FILE = \"chunks.db\"\n",
    "OUTPUT_FILE = \"exported_chunks.jsonl\"\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Query all data from chunks table\n",
    "cur.execute(\"SELECT text, label FROM chunks\")\n",
    "rows = cur.fetchall()\n",
    "\n",
    "# Write to JSONL\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for text, label in rows:\n",
    "        obj = {\"text\": text}\n",
    "        if label is not None:\n",
    "            obj[\"label\"] = label\n",
    "        f.write(json.dumps(obj) + \"\\n\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"Data exported to JSONL file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9369a9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.29.2\n",
      "<class 'transformers.training_args.TrainingArguments'>\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "print(transformers.TrainingArguments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47d48089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled chunks after removing label 11: {1: 8199, 0: 800}\n",
      "Final labeled chunks: {1: 5384, 0: 800}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the labeled chunks\n",
    "with open(\"exported_chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    labeled_chunks = [json.loads(line) for line in f]\n",
    "\n",
    "data = pd.DataFrame(labeled_chunks)\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "\n",
    "# Get the first 9000 rows\n",
    "data = data.head(9000)\n",
    "\n",
    "# Remove rows with label == 11\n",
    "data = data[data['label'] != 11]\n",
    "\n",
    "# Print labeled count after removing label 11\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "print(f\"Labeled chunks after removing label 11: {labeled_count}\")\n",
    "\n",
    "# Remove rows where label == 1 and text length < 100\n",
    "data = data[~((data['label'] == 1) & (data['text'].str.len() < 100))]\n",
    "\n",
    "# Print final labeled count\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "print(f\"Final labeled chunks: {labeled_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a629272d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([3.8648, 0.5743], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bcbe650ff704057977212cae444faf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92aa6ff066dd41d9859ea3fa0f659530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda410e25ffe49e98fbf96f8585f6856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/930 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6752, 'learning_rate': 4.9462365591397855e-05, 'epoch': 0.03}\n",
      "{'loss': 0.6024, 'learning_rate': 4.89247311827957e-05, 'epoch': 0.06}\n",
      "{'loss': 0.4507, 'learning_rate': 4.8387096774193554e-05, 'epoch': 0.1}\n",
      "{'loss': 0.3033, 'learning_rate': 4.78494623655914e-05, 'epoch': 0.13}\n",
      "{'loss': 0.314, 'learning_rate': 4.731182795698925e-05, 'epoch': 0.16}\n",
      "{'loss': 0.2666, 'learning_rate': 4.67741935483871e-05, 'epoch': 0.19}\n",
      "{'loss': 0.963, 'learning_rate': 4.6236559139784944e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0811, 'learning_rate': 4.56989247311828e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3581, 'learning_rate': 4.516129032258064e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3529, 'learning_rate': 4.4623655913978496e-05, 'epoch': 0.32}\n",
      "{'loss': 0.2746, 'learning_rate': 4.408602150537635e-05, 'epoch': 0.35}\n",
      "{'loss': 0.5411, 'learning_rate': 4.3548387096774194e-05, 'epoch': 0.39}\n",
      "{'loss': 0.2955, 'learning_rate': 4.301075268817205e-05, 'epoch': 0.42}\n",
      "{'loss': 0.4125, 'learning_rate': 4.247311827956989e-05, 'epoch': 0.45}\n",
      "{'loss': 0.4073, 'learning_rate': 4.1935483870967746e-05, 'epoch': 0.48}\n",
      "{'loss': 0.1477, 'learning_rate': 4.13978494623656e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3125, 'learning_rate': 4.0860215053763444e-05, 'epoch': 0.55}\n",
      "{'loss': 0.1687, 'learning_rate': 4.032258064516129e-05, 'epoch': 0.58}\n",
      "{'loss': 0.3091, 'learning_rate': 3.978494623655914e-05, 'epoch': 0.61}\n",
      "{'loss': 0.4998, 'learning_rate': 3.924731182795699e-05, 'epoch': 0.65}\n",
      "{'loss': 0.2771, 'learning_rate': 3.870967741935484e-05, 'epoch': 0.68}\n",
      "{'loss': 0.1358, 'learning_rate': 3.817204301075269e-05, 'epoch': 0.71}\n",
      "{'loss': 0.2028, 'learning_rate': 3.763440860215054e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0208, 'learning_rate': 3.7096774193548386e-05, 'epoch': 0.77}\n",
      "{'loss': 0.412, 'learning_rate': 3.655913978494624e-05, 'epoch': 0.81}\n",
      "{'loss': 0.4683, 'learning_rate': 3.602150537634409e-05, 'epoch': 0.84}\n",
      "{'loss': 0.2086, 'learning_rate': 3.548387096774194e-05, 'epoch': 0.87}\n",
      "{'loss': 0.3701, 'learning_rate': 3.494623655913979e-05, 'epoch': 0.9}\n",
      "{'loss': 0.1362, 'learning_rate': 3.4408602150537636e-05, 'epoch': 0.94}\n",
      "{'loss': 0.1878, 'learning_rate': 3.387096774193548e-05, 'epoch': 0.97}\n",
      "{'loss': 0.2663, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56457b1cb1443cd9374544bb9740d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27823910117149353, 'eval_runtime': 8.8158, 'eval_samples_per_second': 140.316, 'eval_steps_per_second': 8.848, 'epoch': 1.0}\n",
      "{'loss': 0.0085, 'learning_rate': 3.279569892473118e-05, 'epoch': 1.03}\n",
      "{'loss': 0.1222, 'learning_rate': 3.2258064516129034e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0107, 'learning_rate': 3.172043010752688e-05, 'epoch': 1.1}\n",
      "{'loss': 0.2221, 'learning_rate': 3.118279569892473e-05, 'epoch': 1.13}\n",
      "{'loss': 0.1973, 'learning_rate': 3.0645161290322585e-05, 'epoch': 1.16}\n",
      "{'loss': 0.2167, 'learning_rate': 3.010752688172043e-05, 'epoch': 1.19}\n",
      "{'loss': 0.3238, 'learning_rate': 2.9569892473118284e-05, 'epoch': 1.23}\n",
      "{'loss': 0.1264, 'learning_rate': 2.9032258064516133e-05, 'epoch': 1.26}\n",
      "{'loss': 0.1911, 'learning_rate': 2.8494623655913982e-05, 'epoch': 1.29}\n",
      "{'loss': 0.3467, 'learning_rate': 2.7956989247311828e-05, 'epoch': 1.32}\n",
      "{'loss': 0.2304, 'learning_rate': 2.7419354838709678e-05, 'epoch': 1.35}\n",
      "{'loss': 0.1451, 'learning_rate': 2.6881720430107527e-05, 'epoch': 1.39}\n",
      "{'loss': 0.1372, 'learning_rate': 2.6344086021505376e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0358, 'learning_rate': 2.5806451612903226e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0044, 'learning_rate': 2.5268817204301075e-05, 'epoch': 1.48}\n",
      "{'loss': 0.2679, 'learning_rate': 2.4731182795698928e-05, 'epoch': 1.52}\n",
      "{'loss': 0.1406, 'learning_rate': 2.4193548387096777e-05, 'epoch': 1.55}\n",
      "{'loss': 0.2224, 'learning_rate': 2.3655913978494626e-05, 'epoch': 1.58}\n",
      "{'loss': 0.2253, 'learning_rate': 2.3118279569892472e-05, 'epoch': 1.61}\n",
      "{'loss': 0.1497, 'learning_rate': 2.258064516129032e-05, 'epoch': 1.65}\n",
      "{'loss': 0.31, 'learning_rate': 2.2043010752688174e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0055, 'learning_rate': 2.1505376344086024e-05, 'epoch': 1.71}\n",
      "{'loss': 0.3283, 'learning_rate': 2.0967741935483873e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0871, 'learning_rate': 2.0430107526881722e-05, 'epoch': 1.77}\n",
      "{'loss': 0.1436, 'learning_rate': 1.989247311827957e-05, 'epoch': 1.81}\n",
      "{'loss': 0.1421, 'learning_rate': 1.935483870967742e-05, 'epoch': 1.84}\n",
      "{'loss': 0.1578, 'learning_rate': 1.881720430107527e-05, 'epoch': 1.87}\n",
      "{'loss': 0.0899, 'learning_rate': 1.827956989247312e-05, 'epoch': 1.9}\n",
      "{'loss': 0.0685, 'learning_rate': 1.774193548387097e-05, 'epoch': 1.94}\n",
      "{'loss': 0.048, 'learning_rate': 1.7204301075268818e-05, 'epoch': 1.97}\n",
      "{'loss': 0.144, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f0104a82db4dd4ae696333e40efa0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33145129680633545, 'eval_runtime': 8.859, 'eval_samples_per_second': 139.633, 'eval_steps_per_second': 8.805, 'epoch': 2.0}\n",
      "{'loss': 0.1581, 'learning_rate': 1.6129032258064517e-05, 'epoch': 2.03}\n",
      "{'loss': 0.0379, 'learning_rate': 1.5591397849462366e-05, 'epoch': 2.06}\n",
      "{'loss': 0.0026, 'learning_rate': 1.5053763440860215e-05, 'epoch': 2.1}\n",
      "{'loss': 0.0019, 'learning_rate': 1.4516129032258066e-05, 'epoch': 2.13}\n",
      "{'loss': 0.3645, 'learning_rate': 1.3978494623655914e-05, 'epoch': 2.16}\n",
      "{'loss': 0.1127, 'learning_rate': 1.3440860215053763e-05, 'epoch': 2.19}\n",
      "{'loss': 0.1684, 'learning_rate': 1.2903225806451613e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0613, 'learning_rate': 1.2365591397849464e-05, 'epoch': 2.26}\n",
      "{'loss': 0.293, 'learning_rate': 1.1827956989247313e-05, 'epoch': 2.29}\n",
      "{'loss': 0.0443, 'learning_rate': 1.129032258064516e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0037, 'learning_rate': 1.0752688172043012e-05, 'epoch': 2.35}\n",
      "{'loss': 0.2623, 'learning_rate': 1.0215053763440861e-05, 'epoch': 2.39}\n",
      "{'loss': 0.0134, 'learning_rate': 9.67741935483871e-06, 'epoch': 2.42}\n",
      "{'loss': 0.0071, 'learning_rate': 9.13978494623656e-06, 'epoch': 2.45}\n",
      "{'loss': 0.1084, 'learning_rate': 8.602150537634409e-06, 'epoch': 2.48}\n",
      "{'loss': 0.1281, 'learning_rate': 8.064516129032258e-06, 'epoch': 2.52}\n",
      "{'loss': 0.2061, 'learning_rate': 7.526881720430108e-06, 'epoch': 2.55}\n",
      "{'loss': 0.0079, 'learning_rate': 6.989247311827957e-06, 'epoch': 2.58}\n",
      "{'loss': 0.0867, 'learning_rate': 6.451612903225806e-06, 'epoch': 2.61}\n",
      "{'loss': 0.0628, 'learning_rate': 5.9139784946236566e-06, 'epoch': 2.65}\n",
      "{'loss': 0.0744, 'learning_rate': 5.376344086021506e-06, 'epoch': 2.68}\n",
      "{'loss': 0.0023, 'learning_rate': 4.838709677419355e-06, 'epoch': 2.71}\n",
      "{'loss': 0.1079, 'learning_rate': 4.3010752688172045e-06, 'epoch': 2.74}\n",
      "{'loss': 0.1459, 'learning_rate': 3.763440860215054e-06, 'epoch': 2.77}\n",
      "{'loss': 0.121, 'learning_rate': 3.225806451612903e-06, 'epoch': 2.81}\n",
      "{'loss': 0.0467, 'learning_rate': 2.688172043010753e-06, 'epoch': 2.84}\n",
      "{'loss': 0.0333, 'learning_rate': 2.1505376344086023e-06, 'epoch': 2.87}\n",
      "{'loss': 0.0396, 'learning_rate': 1.6129032258064516e-06, 'epoch': 2.9}\n",
      "{'loss': 0.0706, 'learning_rate': 1.0752688172043011e-06, 'epoch': 2.94}\n",
      "{'loss': 0.0024, 'learning_rate': 5.376344086021506e-07, 'epoch': 2.97}\n",
      "{'loss': 0.0022, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40bd89d0ea4e4c0bb1e916cd0aaac92f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.234201118350029, 'eval_runtime': 8.8628, 'eval_samples_per_second': 139.573, 'eval_steps_per_second': 8.801, 'epoch': 3.0}\n",
      "{'train_runtime': 329.1049, 'train_samples_per_second': 45.095, 'train_steps_per_second': 2.826, 'train_loss': 0.19406638859060182, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b262415600b144ccaaf1b11574daf83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9470    0.8938    0.9196       160\n",
      "           1     0.9843    0.9926    0.9884      1077\n",
      "\n",
      "    accuracy                         0.9798      1237\n",
      "   macro avg     0.9657    0.9432    0.9540      1237\n",
      "weighted avg     0.9795    0.9798    0.9795      1237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DebertaV2Tokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Loading the data\n",
    "data['label'] = data['label'].astype(int)\n",
    "\n",
    "# Train-Test Split using stratified sampling\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, stratify=data['label'], random_state=42)\n",
    "\n",
    "# since there is a class imbalance, we will compute class weights\n",
    "# to handle this in the loss function\n",
    "labels = train_df[\"label\"].values\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# Convert ing the DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "# Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-small\", num_labels=len(data['label'].unique()))\n",
    "model.to(device)\n",
    "# Tokenization\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v3-small\")\n",
    "\n",
    "def tokenize(input_data):\n",
    "    return tokenizer(input_data[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Creating a custom Trainer to handle weighted loss\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Creating the Trainer\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluating the model\n",
    "preds = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(preds.predictions, axis=1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_dataset[\"labels\"], pred_labels, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eaa1d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Text 1:\n",
      "Text: The federal government alerts its contractors to CI threats\n",
      "and subjects them to \"awareness programs\" under the\n",
      "DOD's Defense Information Counter Espionage (DICE)\n",
      "program. The Defense Investigative Service (DIS)\n",
      "maintains a host of useful databases such as the Foreign\n",
      "Ownership, Control, or Influenc...\n",
      "Actual label   : 1\n",
      "Predicted label: 1\n",
      "\n",
      " Text 2:\n",
      "Text: 9. The prosperity of nations rests very largely on the\n",
      "      six inches of soil between the surface and the subsoil\n",
      "      of the territory.\n",
      "Actual label   : 1\n",
      "Predicted label: 1\n",
      "\n",
      " Text 3:\n",
      "Text: _Accept_ means _to receive_. _Except_ as a verb means _to exclude_; as a\n",
      "preposition it means _with the exception of_. Insert the correct form in\n",
      "the following:\n",
      "Actual label   : 1\n",
      "Predicted label: 1\n",
      "\n",
      " Text 4:\n",
      "Text: How the West lost the East. The economics, the politics, the geopolitics, the\n",
      "conspiracies, the corruption, the old and the new, the plough and the internet - it is all\n",
      "here, in colourful and provocative prose.\n",
      "Actual label   : 0\n",
      "Predicted label: 1\n",
      "\n",
      " Text 5:\n",
      "Text: In all my years in the Balkans, I have yet to come across a\n",
      "voluntary act - a single voluntary act - by an academic.\n",
      "Actual label   : 1\n",
      "Predicted label: 1\n",
      "\n",
      " Text 6:\n",
      "Text: Moreover, The moral authority of those who preach\n",
      "against corruption in poor countries - the officials of the\n",
      "IMF, the World Bank, the EU, the OECD - is strained by\n",
      "their ostentatious lifestyle, conspicuous consumption, and\n",
      "\"pragmatic\" morality.\n",
      "Actual label   : 1\n",
      "Predicted label: 1\n",
      "\n",
      " Text 7:\n",
      "Text: AGENCY DEFINED.--Merely for purposes of convenience, it may be best to\n",
      "divide the whole subject of agency into three branches: Principal and\n",
      "agent; master and servant; employer and independent contractor. The term\n",
      "\"agency,\" when used in the broad sense, indicates a relation which\n",
      "exists where one pe...\n",
      "Actual label   : 1\n",
      "Predicted label: 1\n",
      "\n",
      " Text 8:\n",
      "Text: .com for businesses\n",
      "          .org for non-profit organizations\n",
      "          .gov and .mil for government and military agencies\n",
      "          .net for companies or organizations that run large networks.\n",
      "Actual label   : 1\n",
      "Predicted label: 1\n",
      "\n",
      " Text 9:\n",
      "Text: Yours truly,\n",
      "                                              S. D. Jensen\n",
      "Actual label   : 0\n",
      "Predicted label: 0\n",
      "\n",
      " Text 10:\n",
      "Text: THE NATURE OF A CORPORATION.--The nature of a corporation is perhaps\n",
      "best understood by an illustration. In the case of People's Pleasure\n",
      "Park Co. v. Rohleder, 109 Va. 439, the facts were as follows: There was\n",
      "a large tract of land divided up into a number of lots, and in each\n",
      "deed, when a lot was s...\n",
      "Actual label   : 1\n",
      "Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "# Running test on 10 random samples\n",
    "sample_df = test_df.sample(10, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Tokenize the samples\n",
    "encodings = tokenizer(\n",
    "    sample_df[\"text\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encodings)\n",
    "    probs = torch.softmax(outputs.logits, dim=1)\n",
    "    predictions = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "\n",
    "# Display actual vs predicted\n",
    "for i in range(10):\n",
    "    print(f\"\\n Text {i+1}:\")\n",
    "    print(f\"Text: {sample_df.loc[i, 'text'][:300]}{'...' if len(sample_df.loc[i, 'text']) > 300 else ''}\")\n",
    "    print(f\"Actual label   : {sample_df.loc[i, 'label']}\")\n",
    "    print(f\"Predicted label: {predictions[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4788fa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text 1:\n",
      "Text: Photosynthesis is the process by which green plants convert sunlight into energy.\n",
      "Predicted label: 1 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 2:\n",
      "Text: Bibliography\n",
      "Chapter 3\n",
      "Index\n",
      "Acknowledgments\n",
      "Predicted label: 0 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 3:\n",
      "Text: The mitochondria is often called the powerhouse of the cell.\n",
      "Predicted label: 1 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 4:\n",
      "Text: References\n",
      "[1] Smith et al. 2022\n",
      "Predicted label: 0 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 5:\n",
      "Text: This text explains Newton’s laws of motion in detail.\n",
      "Predicted label: 0 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 6:\n",
      "Text: Appendix A: Glossary of Terms\n",
      "Predicted label: 0 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 7:\n",
      "Text: How does the water cycle work in nature?\n",
      "Predicted label: 0 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 8:\n",
      "Text: The syllabus is subject to change without notice.\n",
      "Predicted label: 0 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 9:\n",
      "Text: Cell division is crucial for reproduction in organisms.\n",
      "Predicted label: 0 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 10:\n",
      "Text: Table of contents\n",
      "1. Preface\n",
      "2. Introduction\n",
      "Predicted label: 0 (1 = Relevant, 0 = Irrelevant)\n"
     ]
    }
   ],
   "source": [
    "# Testing the models generalization on new unseen data\n",
    "\n",
    "# Random new text samples\n",
    "new_texts = [\n",
    "    \"Photosynthesis is the process by which green plants convert sunlight into energy.\",\n",
    "    \"Bibliography\\nChapter 3\\nIndex\\nAcknowledgments\",\n",
    "    \"The mitochondria is often called the powerhouse of the cell.\",\n",
    "    \"References\\n[1] Smith et al. 2022\",\n",
    "    \"This text explains Newton’s laws of motion in detail.\",\n",
    "    \"Appendix A: Glossary of Terms\",\n",
    "    \"How does the water cycle work in nature?\",\n",
    "    \"The syllabus is subject to change without notice.\",\n",
    "    \"Cell division is crucial for reproduction in organisms.\",\n",
    "    \"Table of contents\\n1. Preface\\n2. Introduction\"\n",
    "]\n",
    "\n",
    "# Tokenize the new texts\n",
    "encodings = tokenizer(\n",
    "    new_texts,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "# RUn the model on new texts\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encodings)\n",
    "    probs = torch.softmax(outputs.logits, dim=1)\n",
    "    predictions = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "\n",
    "# Results \n",
    "for i, text in enumerate(new_texts):\n",
    "    print(f\"\\nText {i+1}:\")\n",
    "    print(f\"Text: {text[:300]}{'...' if len(text) > 300 else ''}\")\n",
    "    print(f\"Predicted label: {predictions[i]} (1 = Relevant, 0 = Irrelevant)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42958493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b938391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f33febd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe84e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeafbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, HfFolder, Repository, create_repo\n",
    "\n",
    "repo_id = \"LMForge/text_relevance_classfier\"\n",
    "\n",
    "# Create repo if it doesn’t exist\n",
    "create_repo(repo_id, exist_ok=True)\n",
    "\n",
    "# Push your saved model folder\n",
    "from huggingface_hub import upload_folder\n",
    "upload_folder(\n",
    "    repo_id=repo_id,\n",
    "    folder_path=\"classifier_model\",  # your saved folder\n",
    "    commit_message=\"Initial model upload\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44bb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-forge-Copy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
