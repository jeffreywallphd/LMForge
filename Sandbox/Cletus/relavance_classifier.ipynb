{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de54e6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to JSONL file.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "\n",
    "DB_FILE = \"chunks.db\"\n",
    "OUTPUT_FILE = \"exported_chunks.jsonl\"\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Query all data from chunks table\n",
    "cur.execute(\"SELECT text, label FROM chunks\")\n",
    "rows = cur.fetchall()\n",
    "\n",
    "# Write to JSONL\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for text, label in rows:\n",
    "        obj = {\"text\": text}\n",
    "        if label is not None:\n",
    "            obj[\"label\"] = label\n",
    "        f.write(json.dumps(obj) + \"\\n\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"Data exported to JSONL file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d06ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled chunks after removing label 11: {1: 8199, 0: 800}\n",
      "Final labeled chunks: {1: 5384, 0: 800}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the labeled chunks\n",
    "with open(\"exported_chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    labeled_chunks = [json.loads(line) for line in f]\n",
    "\n",
    "data = pd.DataFrame(labeled_chunks)\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "\n",
    "# Get the first 9000 rows\n",
    "data = data.head(9000)\n",
    "\n",
    "# Remove rows with label == 11\n",
    "data = data[data['label'] != 11]\n",
    "\n",
    "# Print labeled count after removing label 11\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "print(f\"Labeled chunks after removing label 11: {labeled_count}\")\n",
    "\n",
    "# Remove rows where label == 1 and text length < 100\n",
    "data = data[~((data['label'] == 1) & (data['text'].str.len() < 100))]\n",
    "\n",
    "# Print final labeled count\n",
    "labeled_count = data['label'].value_counts().to_dict()\n",
    "print(f\"Final labeled chunks: {labeled_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e10292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([3.8648, 0.5743], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15cadcf1069946f18b120d5d43cb0de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d4d6e9f3494c9d9b14377e43f7507f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\ctngweru\\AppData\\Local\\anaconda3\\envs\\llm-forge-Copy\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf9559da7fa47e5b4457e329e09de7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/930 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6134, 'learning_rate': 4.9462365591397855e-05, 'epoch': 0.03}\n",
      "{'loss': 0.5157, 'learning_rate': 4.89247311827957e-05, 'epoch': 0.06}\n",
      "{'loss': 0.3569, 'learning_rate': 4.8387096774193554e-05, 'epoch': 0.1}\n",
      "{'loss': 0.1959, 'learning_rate': 4.78494623655914e-05, 'epoch': 0.13}\n",
      "{'loss': 0.3643, 'learning_rate': 4.731182795698925e-05, 'epoch': 0.16}\n",
      "{'loss': 0.3029, 'learning_rate': 4.67741935483871e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3797, 'learning_rate': 4.6236559139784944e-05, 'epoch': 0.23}\n",
      "{'loss': 0.1163, 'learning_rate': 4.56989247311828e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3242, 'learning_rate': 4.516129032258064e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3069, 'learning_rate': 4.4623655913978496e-05, 'epoch': 0.32}\n",
      "{'loss': 0.2747, 'learning_rate': 4.408602150537635e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3707, 'learning_rate': 4.3548387096774194e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3371, 'learning_rate': 4.301075268817205e-05, 'epoch': 0.42}\n",
      "{'loss': 0.493, 'learning_rate': 4.247311827956989e-05, 'epoch': 0.45}\n",
      "{'loss': 0.2898, 'learning_rate': 4.1935483870967746e-05, 'epoch': 0.48}\n",
      "{'loss': 0.1496, 'learning_rate': 4.13978494623656e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3901, 'learning_rate': 4.0860215053763444e-05, 'epoch': 0.55}\n",
      "{'loss': 0.2173, 'learning_rate': 4.032258064516129e-05, 'epoch': 0.58}\n",
      "{'loss': 0.2835, 'learning_rate': 3.978494623655914e-05, 'epoch': 0.61}\n",
      "{'loss': 0.3885, 'learning_rate': 3.924731182795699e-05, 'epoch': 0.65}\n",
      "{'loss': 0.1819, 'learning_rate': 3.870967741935484e-05, 'epoch': 0.68}\n",
      "{'loss': 0.1327, 'learning_rate': 3.817204301075269e-05, 'epoch': 0.71}\n",
      "{'loss': 0.1055, 'learning_rate': 3.763440860215054e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0276, 'learning_rate': 3.7096774193548386e-05, 'epoch': 0.77}\n",
      "{'loss': 0.4666, 'learning_rate': 3.655913978494624e-05, 'epoch': 0.81}\n",
      "{'loss': 0.554, 'learning_rate': 3.602150537634409e-05, 'epoch': 0.84}\n",
      "{'loss': 0.1807, 'learning_rate': 3.548387096774194e-05, 'epoch': 0.87}\n",
      "{'loss': 0.3078, 'learning_rate': 3.494623655913979e-05, 'epoch': 0.9}\n",
      "{'loss': 0.073, 'learning_rate': 3.4408602150537636e-05, 'epoch': 0.94}\n",
      "{'loss': 0.2166, 'learning_rate': 3.387096774193548e-05, 'epoch': 0.97}\n",
      "{'loss': 0.2773, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73abad8a619d44299082b3fd7a0302dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2780472934246063, 'eval_runtime': 4.2225, 'eval_samples_per_second': 292.953, 'eval_steps_per_second': 18.472, 'epoch': 1.0}\n",
      "{'loss': 0.0078, 'learning_rate': 3.279569892473118e-05, 'epoch': 1.03}\n",
      "{'loss': 0.1706, 'learning_rate': 3.2258064516129034e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0075, 'learning_rate': 3.172043010752688e-05, 'epoch': 1.1}\n",
      "{'loss': 0.2529, 'learning_rate': 3.118279569892473e-05, 'epoch': 1.13}\n",
      "{'loss': 0.2801, 'learning_rate': 3.0645161290322585e-05, 'epoch': 1.16}\n",
      "{'loss': 0.1653, 'learning_rate': 3.010752688172043e-05, 'epoch': 1.19}\n",
      "{'loss': 0.3258, 'learning_rate': 2.9569892473118284e-05, 'epoch': 1.23}\n",
      "{'loss': 0.0556, 'learning_rate': 2.9032258064516133e-05, 'epoch': 1.26}\n",
      "{'loss': 0.159, 'learning_rate': 2.8494623655913982e-05, 'epoch': 1.29}\n",
      "{'loss': 0.3066, 'learning_rate': 2.7956989247311828e-05, 'epoch': 1.32}\n",
      "{'loss': 0.1938, 'learning_rate': 2.7419354838709678e-05, 'epoch': 1.35}\n",
      "{'loss': 0.1355, 'learning_rate': 2.6881720430107527e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0656, 'learning_rate': 2.6344086021505376e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0402, 'learning_rate': 2.5806451612903226e-05, 'epoch': 1.45}\n",
      "{'loss': 0.003, 'learning_rate': 2.5268817204301075e-05, 'epoch': 1.48}\n",
      "{'loss': 0.2121, 'learning_rate': 2.4731182795698928e-05, 'epoch': 1.52}\n",
      "{'loss': 0.2769, 'learning_rate': 2.4193548387096777e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0966, 'learning_rate': 2.3655913978494626e-05, 'epoch': 1.58}\n",
      "{'loss': 0.2773, 'learning_rate': 2.3118279569892472e-05, 'epoch': 1.61}\n",
      "{'loss': 0.1145, 'learning_rate': 2.258064516129032e-05, 'epoch': 1.65}\n",
      "{'loss': 0.2279, 'learning_rate': 2.2043010752688174e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0217, 'learning_rate': 2.1505376344086024e-05, 'epoch': 1.71}\n",
      "{'loss': 0.2458, 'learning_rate': 2.0967741935483873e-05, 'epoch': 1.74}\n",
      "{'loss': 0.1791, 'learning_rate': 2.0430107526881722e-05, 'epoch': 1.77}\n",
      "{'loss': 0.1603, 'learning_rate': 1.989247311827957e-05, 'epoch': 1.81}\n",
      "{'loss': 0.2215, 'learning_rate': 1.935483870967742e-05, 'epoch': 1.84}\n",
      "{'loss': 0.2604, 'learning_rate': 1.881720430107527e-05, 'epoch': 1.87}\n",
      "{'loss': 0.0709, 'learning_rate': 1.827956989247312e-05, 'epoch': 1.9}\n",
      "{'loss': 0.0097, 'learning_rate': 1.774193548387097e-05, 'epoch': 1.94}\n",
      "{'loss': 0.0027, 'learning_rate': 1.7204301075268818e-05, 'epoch': 1.97}\n",
      "{'loss': 0.0348, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f303d316287a454eb5ff221a3e4c7972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31458428502082825, 'eval_runtime': 4.317, 'eval_samples_per_second': 286.539, 'eval_steps_per_second': 18.068, 'epoch': 2.0}\n",
      "{'loss': 0.144, 'learning_rate': 1.6129032258064517e-05, 'epoch': 2.03}\n",
      "{'loss': 0.0032, 'learning_rate': 1.5591397849462366e-05, 'epoch': 2.06}\n",
      "{'loss': 0.0022, 'learning_rate': 1.5053763440860215e-05, 'epoch': 2.1}\n",
      "{'loss': 0.0023, 'learning_rate': 1.4516129032258066e-05, 'epoch': 2.13}\n",
      "{'loss': 0.1042, 'learning_rate': 1.3978494623655914e-05, 'epoch': 2.16}\n",
      "{'loss': 0.1075, 'learning_rate': 1.3440860215053763e-05, 'epoch': 2.19}\n",
      "{'loss': 0.1433, 'learning_rate': 1.2903225806451613e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0425, 'learning_rate': 1.2365591397849464e-05, 'epoch': 2.26}\n",
      "{'loss': 0.0042, 'learning_rate': 1.1827956989247313e-05, 'epoch': 2.29}\n",
      "{'loss': 0.0051, 'learning_rate': 1.129032258064516e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0344, 'learning_rate': 1.0752688172043012e-05, 'epoch': 2.35}\n",
      "{'loss': 0.1863, 'learning_rate': 1.0215053763440861e-05, 'epoch': 2.39}\n",
      "{'loss': 0.0264, 'learning_rate': 9.67741935483871e-06, 'epoch': 2.42}\n",
      "{'loss': 0.0926, 'learning_rate': 9.13978494623656e-06, 'epoch': 2.45}\n",
      "{'loss': 0.0757, 'learning_rate': 8.602150537634409e-06, 'epoch': 2.48}\n",
      "{'loss': 0.1717, 'learning_rate': 8.064516129032258e-06, 'epoch': 2.52}\n",
      "{'loss': 0.0018, 'learning_rate': 7.526881720430108e-06, 'epoch': 2.55}\n",
      "{'loss': 0.0034, 'learning_rate': 6.989247311827957e-06, 'epoch': 2.58}\n",
      "{'loss': 0.0379, 'learning_rate': 6.451612903225806e-06, 'epoch': 2.61}\n",
      "{'loss': 0.0782, 'learning_rate': 5.9139784946236566e-06, 'epoch': 2.65}\n",
      "{'loss': 0.0759, 'learning_rate': 5.376344086021506e-06, 'epoch': 2.68}\n",
      "{'loss': 0.052, 'learning_rate': 4.838709677419355e-06, 'epoch': 2.71}\n",
      "{'loss': 0.004, 'learning_rate': 4.3010752688172045e-06, 'epoch': 2.74}\n",
      "{'loss': 0.1614, 'learning_rate': 3.763440860215054e-06, 'epoch': 2.77}\n",
      "{'loss': 0.1307, 'learning_rate': 3.225806451612903e-06, 'epoch': 2.81}\n",
      "{'loss': 0.0965, 'learning_rate': 2.688172043010753e-06, 'epoch': 2.84}\n",
      "{'loss': 0.0145, 'learning_rate': 2.1505376344086023e-06, 'epoch': 2.87}\n",
      "{'loss': 0.002, 'learning_rate': 1.6129032258064516e-06, 'epoch': 2.9}\n",
      "{'loss': 0.106, 'learning_rate': 1.0752688172043011e-06, 'epoch': 2.94}\n",
      "{'loss': 0.0031, 'learning_rate': 5.376344086021506e-07, 'epoch': 2.97}\n",
      "{'loss': 0.0013, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0dc461da8444f959ac7b132e2e31d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23098085820674896, 'eval_runtime': 4.3805, 'eval_samples_per_second': 282.389, 'eval_steps_per_second': 17.806, 'epoch': 3.0}\n",
      "{'train_runtime': 176.4869, 'train_samples_per_second': 84.091, 'train_steps_per_second': 5.27, 'train_loss': 0.1687075160493854, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0582c2d0185c4138a7ce30bbadedcb6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9172    0.9000    0.9085       160\n",
      "           1     0.9852    0.9879    0.9866      1077\n",
      "\n",
      "    accuracy                         0.9766      1237\n",
      "   macro avg     0.9512    0.9440    0.9475      1237\n",
      "weighted avg     0.9764    0.9766    0.9765      1237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Loading the data\n",
    "data['label'] = data['label'].astype(int)\n",
    "\n",
    "# Train-Test Split using stratified sampling\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, stratify=data['label'], random_state=42)\n",
    "\n",
    "# since there is a class imbalance, we will compute class weights\n",
    "# to handle this in the loss function\n",
    "labels = train_df[\"label\"].values\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# Convert ing the DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(input_data):\n",
    "    return tokenizer(input_data[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Loaading the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2).to(device)\n",
    "\n",
    "# Creating a custom Trainer to handle weighted loss\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Creating the Trainer\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluating the model\n",
    "preds = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(preds.predictions, axis=1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_dataset[\"labels\"], pred_labels, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2d637f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Text 1:\n",
      "Text: The federal government alerts its contractors to CI threats\n",
      "and subjects them to \"awareness programs\" under the\n",
      "DOD's Defense Information Counter Espionage (DICE)\n",
      "program. The Defense Investigative Service (DIS)\n",
      "maintains a host of useful databases such as the Foreign\n",
      "Ownership, Control, or Influenc...\n",
      "Actual label   : 1\n",
      "Predicted label: 1\n",
      "\n",
      " Text 2:\n",
      "Text: 9. The prosperity of nations rests very largely on the\n",
      "      six inches of soil between the surface and the subsoil\n",
      "      of the territory.\n",
      "Actual label   : 1\n",
      "Predicted label: 1\n",
      "\n",
      " Text 3:\n",
      "Text: _Accept_ means _to receive_. _Except_ as a verb means _to exclude_; as a\n",
      "preposition it means _with the exception of_. Insert the correct form in\n",
      "the following:\n",
      "Actual label   : 1\n",
      "Predicted label: 1\n",
      "\n",
      " Text 4:\n",
      "Text: How the West lost the East. The economics, the politics, the geopolitics, the\n",
      "conspiracies, the corruption, the old and the new, the plough and the internet - it is all\n",
      "here, in colourful and provocative prose.\n",
      "Actual label   : 0\n",
      "Predicted label: 1\n",
      "\n",
      " Text 5:\n",
      "Text: In all my years in the Balkans, I have yet to come across a\n",
      "voluntary act - a single voluntary act - by an academic.\n",
      "Actual label   : 1\n",
      "Predicted label: 1\n",
      "\n",
      " Text 6:\n",
      "Text: Moreover, The moral authority of those who preach\n",
      "against corruption in poor countries - the officials of the\n",
      "IMF, the World Bank, the EU, the OECD - is strained by\n",
      "their ostentatious lifestyle, conspicuous consumption, and\n",
      "\"pragmatic\" morality.\n",
      "Actual label   : 1\n",
      "Predicted label: 1\n",
      "\n",
      " Text 7:\n",
      "Text: AGENCY DEFINED.--Merely for purposes of convenience, it may be best to\n",
      "divide the whole subject of agency into three branches: Principal and\n",
      "agent; master and servant; employer and independent contractor. The term\n",
      "\"agency,\" when used in the broad sense, indicates a relation which\n",
      "exists where one pe...\n",
      "Actual label   : 1\n",
      "Predicted label: 1\n",
      "\n",
      " Text 8:\n",
      "Text: .com for businesses\n",
      "          .org for non-profit organizations\n",
      "          .gov and .mil for government and military agencies\n",
      "          .net for companies or organizations that run large networks.\n",
      "Actual label   : 1\n",
      "Predicted label: 0\n",
      "\n",
      " Text 9:\n",
      "Text: Yours truly,\n",
      "                                              S. D. Jensen\n",
      "Actual label   : 0\n",
      "Predicted label: 0\n",
      "\n",
      " Text 10:\n",
      "Text: THE NATURE OF A CORPORATION.--The nature of a corporation is perhaps\n",
      "best understood by an illustration. In the case of People's Pleasure\n",
      "Park Co. v. Rohleder, 109 Va. 439, the facts were as follows: There was\n",
      "a large tract of land divided up into a number of lots, and in each\n",
      "deed, when a lot was s...\n",
      "Actual label   : 1\n",
      "Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "# Running test on 10 random samples\n",
    "sample_df = test_df.sample(10, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Tokenize the samples\n",
    "encodings = tokenizer(\n",
    "    sample_df[\"text\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encodings)\n",
    "    probs = torch.softmax(outputs.logits, dim=1)\n",
    "    predictions = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "\n",
    "# Display actual vs predicted\n",
    "for i in range(10):\n",
    "    print(f\"\\n Text {i+1}:\")\n",
    "    print(f\"Text: {sample_df.loc[i, 'text'][:300]}{'...' if len(sample_df.loc[i, 'text']) > 300 else ''}\")\n",
    "    print(f\"Actual label   : {sample_df.loc[i, 'label']}\")\n",
    "    print(f\"Predicted label: {predictions[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4695652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text 1:\n",
      "Text: Photosynthesis is the process by which green plants convert sunlight into energy.\n",
      "Predicted label: 1 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 2:\n",
      "Text: Bibliography\n",
      "Chapter 3\n",
      "Index\n",
      "Acknowledgments\n",
      "Predicted label: 0 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 3:\n",
      "Text: The mitochondria is often called the powerhouse of the cell.\n",
      "Predicted label: 1 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 4:\n",
      "Text: References\n",
      "[1] Smith et al. 2022\n",
      "Predicted label: 0 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 5:\n",
      "Text: This text explains Newton’s laws of motion in detail.\n",
      "Predicted label: 0 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 6:\n",
      "Text: Appendix A: Glossary of Terms\n",
      "Predicted label: 0 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 7:\n",
      "Text: How does the water cycle work in nature?\n",
      "Predicted label: 0 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 8:\n",
      "Text: The syllabus is subject to change without notice.\n",
      "Predicted label: 0 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 9:\n",
      "Text: Cell division is crucial for reproduction in organisms.\n",
      "Predicted label: 1 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 10:\n",
      "Text: Table of contents\n",
      "1. Preface\n",
      "2. Introduction\n",
      "Predicted label: 0 (1 = Relevant, 0 = Irrelevant)\n"
     ]
    }
   ],
   "source": [
    "# Testing the models generalization on new unseen data\n",
    "\n",
    "# Random new text samples\n",
    "new_texts = [\n",
    "    \"Photosynthesis is the process by which green plants convert sunlight into energy.\",\n",
    "    \"Bibliography\\nChapter 3\\nIndex\\nAcknowledgments\",\n",
    "    \"The mitochondria is often called the powerhouse of the cell.\",\n",
    "    \"References\\n[1] Smith et al. 2022\",\n",
    "    \"This text explains Newton’s laws of motion in detail.\",\n",
    "    \"Appendix A: Glossary of Terms\",\n",
    "    \"How does the water cycle work in nature?\",\n",
    "    \"The syllabus is subject to change without notice.\",\n",
    "    \"Cell division is crucial for reproduction in organisms.\",\n",
    "    \"Table of contents\\n1. Preface\\n2. Introduction\"\n",
    "]\n",
    "\n",
    "# Tokenize the new texts\n",
    "encodings = tokenizer(\n",
    "    new_texts,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "# RUn the model on new texts\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encodings)\n",
    "    probs = torch.softmax(outputs.logits, dim=1)\n",
    "    predictions = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "\n",
    "# Results \n",
    "for i, text in enumerate(new_texts):\n",
    "    print(f\"\\nText {i+1}:\")\n",
    "    print(f\"Text: {text[:300]}{'...' if len(text) > 300 else ''}\")\n",
    "    print(f\"Predicted label: {predictions[i]} (1 = Relevant, 0 = Irrelevant)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268dd075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text 1:\n",
      "Text: In this chapter, we explore the foundational principles of quantum mechanics, including the concept of wave-particle duality, uncertainty, and quantum entanglement. These phenomena have been experimentally verified and form the basis for quantum computing and quantum encryption. Students will learn ...\n",
      "Predicted label: 0 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 2:\n",
      "Text: Table of Contents\n",
      "Preface\n",
      "Chapter 1: Introduction to Biology\n",
      "Chapter 2: The Cell\n",
      "Chapter 3: Genetics\n",
      "Chapter 4: Evolution\n",
      "Chapter 5: Ecology\n",
      "Chapter 6: Human Anatomy\n",
      "Chapter 7: The Immune System\n",
      "Glossary\n",
      "Index\n",
      "Acknowledgements\n",
      "Author Biography\n",
      "Disclaimer: This textbook is provided as-is without warr...\n",
      "Predicted label: 0 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 3:\n",
      "Text: Photosynthesis is a biochemical process that converts light energy into chemical energy in plants, algae, and some bacteria. The process involves two major stages: the light-dependent reactions and the Calvin cycle. In the light-dependent reactions, sunlight is absorbed by chlorophyll and used to sp...\n",
      "Predicted label: 1 (1 = Relevant, 0 = Irrelevant)\n",
      "\n",
      "Text 4:\n",
      "Text: Acknowledgements\n",
      "This work would not have been possible without the support of our research assistants, editorial team, and the generous funding provided by the National Science Foundation and other partners. We also thank the numerous reviewers and contributors who helped shape the final version of...\n",
      "Predicted label: 0 (1 = Relevant, 0 = Irrelevant)\n"
     ]
    }
   ],
   "source": [
    "#save the model\n",
    "model_save_path = \"classifier_model\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)  \n",
    "\n",
    "# using the saved model\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_save_path)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_save_path).to(device)\n",
    "\n",
    "# Testing the saved model on long texts\n",
    "long_texts = [\n",
    "\n",
    "    \"In this chapter, we explore the foundational principles of quantum mechanics, including the concept of wave-particle duality, uncertainty, and quantum entanglement. These phenomena have been experimentally verified and form the basis for quantum computing and quantum encryption. Students will learn how to apply Schrödinger’s equation and analyze quantum states in one-dimensional potential wells, along with real-world implications.\",\n",
    "    \n",
    "    \"Table of Contents\\nPreface\\nChapter 1: Introduction to Biology\\nChapter 2: The Cell\\nChapter 3: Genetics\\nChapter 4: Evolution\\nChapter 5: Ecology\\nChapter 6: Human Anatomy\\nChapter 7: The Immune System\\nGlossary\\nIndex\\nAcknowledgements\\nAuthor Biography\\nDisclaimer: This textbook is provided as-is without warranty.\",\n",
    "    \n",
    "\n",
    "    \"Photosynthesis is a biochemical process that converts light energy into chemical energy in plants, algae, and some bacteria. The process involves two major stages: the light-dependent reactions and the Calvin cycle. In the light-dependent reactions, sunlight is absorbed by chlorophyll and used to split water molecules, releasing oxygen and generating ATP and NADPH. The Calvin cycle then uses these energy molecules to fix carbon dioxide into glucose, which serves as an energy source for the plant.\",\n",
    "    \n",
    "    \"Acknowledgements\\nThis work would not have been possible without the support of our research assistants, editorial team, and the generous funding provided by the National Science Foundation and other partners. We also thank the numerous reviewers and contributors who helped shape the final version of this manuscript. All errors, however, remain the responsibility of the authors.\"\n",
    "]\n",
    "# Tokenize the texts\n",
    "encodings = tokenizer(\n",
    "    long_texts,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "# Predicting usijng the saved model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encodings)\n",
    "    probs = torch.softmax(outputs.logits, dim=1)\n",
    "    predictions = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "\n",
    "for i, text in enumerate(long_texts):\n",
    "    print(f\"\\nText {i+1}:\")\n",
    "    print(f\"Text: {text[:300]}{'...' if len(text) > 300 else ''}\")\n",
    "    print(f\"Predicted label: {predictions[i]} (1 = Relevant, 0 = Irrelevant)\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-forge-Copy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
